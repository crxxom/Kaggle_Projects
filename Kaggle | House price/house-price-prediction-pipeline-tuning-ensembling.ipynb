{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing data + EDA","metadata":{}},{"cell_type":"markdown","source":"Importing necessary libaries and dataset for the section","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\nimport math\n\n#Create a scrollable window so dataframe will not overflow the screen\n#Taken from https://www.youtube.com/watch?v=NQQ3DRdXAXE&list=PL1CS4FDP5q9n5UM0qH_2XgsjH-8a6TK06&index=66\ndef scrollable_table(df, table_id, title):\n    html = f'<h3>{title}</h3>'\n    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n    html += df.to_html()\n    html += '</div>'\n    return html","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-26T14:17:33.165674Z","iopub.execute_input":"2023-06-26T14:17:33.166164Z","iopub.status.idle":"2023-06-26T14:17:33.173414Z","shell.execute_reply.started":"2023-06-26T14:17:33.166131Z","shell.execute_reply":"2023-06-26T14:17:33.171755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-26T14:17:33.972401Z","iopub.execute_input":"2023-06-26T14:17:33.972843Z","iopub.status.idle":"2023-06-26T14:17:34.018655Z","shell.execute_reply.started":"2023-06-26T14:17:33.972801Z","shell.execute_reply":"2023-06-26T14:17:34.017227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"html_train = scrollable_table(df_train, \"training_df\", \"Train\")\nHTML(html_train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-26T14:17:34.862634Z","iopub.execute_input":"2023-06-26T14:17:34.863974Z","iopub.status.idle":"2023-06-26T14:17:36.127201Z","shell.execute_reply.started":"2023-06-26T14:17:34.863934Z","shell.execute_reply":"2023-06-26T14:17:36.126331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train dataset has {df_train.shape[1]} columns and {df_train.shape[0]} rows.\")\nprint(f\"Test dataset has {df_test.shape[1]} columns and {df_test.shape[0]} rows.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-26T14:17:40.069906Z","iopub.execute_input":"2023-06-26T14:17:40.070323Z","iopub.status.idle":"2023-06-26T14:17:40.076562Z","shell.execute_reply.started":"2023-06-26T14:17:40.070293Z","shell.execute_reply":"2023-06-26T14:17:40.075211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"There are {len(set(df_train.dtypes))} unique data type in the df which are {set(df_train.dtypes)}.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-26T14:17:40.400481Z","iopub.execute_input":"2023-06-26T14:17:40.401175Z","iopub.status.idle":"2023-06-26T14:17:40.408253Z","shell.execute_reply.started":"2023-06-26T14:17:40.401144Z","shell.execute_reply":"2023-06-26T14:17:40.406464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features = df_train.select_dtypes(include=['int64', 'float64'])\ncategorical_features = df_train.select_dtypes(include=['O'])","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:17:40.823504Z","iopub.execute_input":"2023-06-26T14:17:40.823888Z","iopub.status.idle":"2023-06-26T14:17:40.830830Z","shell.execute_reply.started":"2023-06-26T14:17:40.823861Z","shell.execute_reply":"2023-06-26T14:17:40.829316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is one way that you can make quick visualization with seaborn, in which you can roughly understand the general distribution of all features to make further and deeper analysis. I found it as a very useful step to do before any deeper analysis especially when you got a huge amount of features like this one.\n\nI basically did regplot for numerical features instead of scatterplot for some additional information (correlation with saleprice) and boxplot for categorical features, which you can change based on your preferences.","metadata":{}},{"cell_type":"code","source":"#cluster same datatype together so it will be visually better\ntrain_dtcluster = numerical_features.join(categorical_features) \n\n#The function take in 3 arguments, your dataset, the target column (SalePrice in this case)\n#And the number of columns it takes which has a default value of 4\ndef plot_all(df, target_col, num_cols=4):\n    num_rows = math.ceil(len(df.columns)/num_cols)\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(16, 4*num_rows))\n    sns.set(font_scale=1.2, style='whitegrid')\n    for i, col_name in enumerate(df.columns):\n        if col_name != target_col.name:\n            ax = axes[i // num_cols, i % num_cols]\n            if df[col_name].dtype == \"O\":\n                sns.boxplot(x=df[col_name], y=target_col, ax=ax) #Categorical plot\n            else:\n                sns.regplot(x=df[col_name], y=target_col, ax=ax) #Numerical plot\n            ax.set_xlabel(col_name, fontsize=14)\n            ax.set_ylabel(target_col.name, fontsize=14)\n            ax.tick_params(axis='both', which='major', labelsize=12)\n    plt.tight_layout()\n    plt.show()\n\nplot_all(train_dtcluster.drop('Id',axis=1), df_train.SalePrice, num_cols=5)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-26T14:17:42.797986Z","iopub.execute_input":"2023-06-26T14:17:42.798482Z","iopub.status.idle":"2023-06-26T14:18:04.494107Z","shell.execute_reply.started":"2023-06-26T14:17:42.798444Z","shell.execute_reply":"2023-06-26T14:18:04.492828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plots effectively gave us some insights to the correlation between different features and the target sale price. For example, we can potentially drop some seemingly useless features like Utilities, PoolArea and MiscVal by eyes for better training efficiency and outcome. \n\nAlso, based on the regplots, we can clearly identify some of the 'numerical_features' should be labelled as categorical features which is what we will be doing next.","metadata":{}},{"cell_type":"code","source":"df_train[['MSSubClass', 'OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', \n          'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', \n          'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']]= df_train[['MSSubClass', 'OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', \n          'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', \n          'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']].astype(str)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:18:04.496710Z","iopub.execute_input":"2023-06-26T14:18:04.497111Z","iopub.status.idle":"2023-06-26T14:18:04.524489Z","shell.execute_reply.started":"2023-06-26T14:18:04.497075Z","shell.execute_reply":"2023-06-26T14:18:04.523378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df_train.corr())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-26T14:18:04.526398Z","iopub.execute_input":"2023-06-26T14:18:04.527198Z","iopub.status.idle":"2023-06-26T14:18:05.159176Z","shell.execute_reply.started":"2023-06-26T14:18:04.527160Z","shell.execute_reply":"2023-06-26T14:18:05.157738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(df_train.SalePrice, kde=True)\nplt.title(\"Distribution of SalePrice\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-26T14:18:05.162163Z","iopub.execute_input":"2023-06-26T14:18:05.162587Z","iopub.status.idle":"2023-06-26T14:18:05.732287Z","shell.execute_reply.started":"2023-06-26T14:18:05.162553Z","shell.execute_reply":"2023-06-26T14:18:05.731120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The histplot suggest a positive skew of our target distribution which suggests that we could apply log transformation to our target variable. By applying log transformation we can reduce the skewness and make the distribution more normal which can significantly improve the performances of many ML algorithms that suggest a normal distribution. But remember to take exp on the final prediction to get the correct results at the end.","metadata":{}},{"cell_type":"code","source":"df_train['SalePrice'] = np.log(df_train.SalePrice)\ny_train = df_train['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:18:05.733881Z","iopub.execute_input":"2023-06-26T14:18:05.734355Z","iopub.status.idle":"2023-06-26T14:18:05.741834Z","shell.execute_reply.started":"2023-06-26T14:18:05.734314Z","shell.execute_reply":"2023-06-26T14:18:05.740660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(df_train.SalePrice, kde=True)\nplt.title(\"Normalized SalePrice Distribution\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-26T14:18:05.743380Z","iopub.execute_input":"2023-06-26T14:18:05.743870Z","iopub.status.idle":"2023-06-26T14:18:06.290044Z","shell.execute_reply.started":"2023-06-26T14:18:05.743838Z","shell.execute_reply":"2023-06-26T14:18:06.288685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nan_df = pd.DataFrame({\"missing_count\": df_train.isna().sum().values, \n                       \"dtype\": df_train.dtypes.values,\n                       \"Percentage of missing values\": (df_train.isna().sum().values / df_train.shape[0]) * 100},\n              index=df_train.columns)\nhtml_nandf = scrollable_table(nan_df.sort_values(by=\"missing_count\", ascending=False), \"nan\",\n                             \"Count of missing values in each features\")\nHTML(html_nandf)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-26T14:18:06.291554Z","iopub.execute_input":"2023-06-26T14:18:06.292003Z","iopub.status.idle":"2023-06-26T14:18:06.341813Z","shell.execute_reply.started":"2023-06-26T14:18:06.291964Z","shell.execute_reply":"2023-06-26T14:18:06.340483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, PoolQC, MiscFeature, Alley and Fence all have a high percentage of missing values which I will be dropping them in this case.","metadata":{}},{"cell_type":"code","source":"df_train = df_train.drop([\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\"],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:18:06.343768Z","iopub.execute_input":"2023-06-26T14:18:06.344417Z","iopub.status.idle":"2023-06-26T14:18:06.355220Z","shell.execute_reply.started":"2023-06-26T14:18:06.344373Z","shell.execute_reply":"2023-06-26T14:18:06.353871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the remaining columns that have missing values, I will be using SimpleImputer from sklearn to impute mean for numerical features and constant 'missing' value for categorical variables which make more sense this case since missing values for categorical features may be a sign that that particular house does not have that particular feature, eg. the house does not have a garage.","metadata":{}},{"cell_type":"markdown","source":"One other observation we got from the plot_all function and just based on the feature descriptions themselves is that there are quite a number of similar features in the dataset which could most likely result in **multicollinearity** (high correlation between two or more predictor variables). This will cause huge problem especailly for algorithms such as linear regression. \n\nThere are multiple appraoch in reducing the problem of multicollinearity, such as manually dropping features, L1L2 Regularization and PCA which is what I will be doing in this notebook as part of the pipeline.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing\n\nNow we will preprocess our data, in this section I will be:\n\n1. Feature Selection\n2. Removing Outliners\n3. Numerical to categorical transformation\n4. Imputing\n5. Normalizing and scaling numerical features\n6. One Hot Encoding\n7. Pass everything through a pipeline","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import skew\n\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntrain = train.drop('Id',axis=1)\ntrain = train[train.GrLivArea < 4000] #Remove outliners\ntest_id = test.Id\ntest = test.drop('Id',axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:18:06.356588Z","iopub.execute_input":"2023-06-26T14:18:06.356942Z","iopub.status.idle":"2023-06-26T14:18:06.415637Z","shell.execute_reply.started":"2023-06-26T14:18:06.356913Z","shell.execute_reply":"2023-06-26T14:18:06.414473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transfer the columns to categorical data\ntransfer_col = ['MSSubClass', 'OverallQual', 'OverallCond', 'BsmtFullBath', \n                'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n                'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', \n                'MoSold', 'YrSold']\nfor i in transfer_col:\n    train[i] = train[i].astype(str)\n\n#remove outliners\ntrain = train[train.GrLivArea < 4000] \n\n#Drop unnecessary features\nX = train.drop([\"SalePrice\", \"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"Utilities\"] ,axis=1)\n\n#Log y\ny = np.log(train.SalePrice)\n\n#Imputing both categorical and numerical features\nnum_col = X.select_dtypes(include=['int64', 'float64']).columns\ncat_col = X.select_dtypes(include=['object']).columns\n\nimputer_num = SimpleImputer(strategy='mean')\nimputer_cat = SimpleImputer(strategy='constant', fill_value='missing')\n\nfor col in X.columns:\n    if X[col].dtype == 'O':\n        X[col] = imputer_cat.fit_transform(X[col].values.reshape(-1,1))[:,0] \n    else:\n        X[col] = imputer_num.fit_transform(X[col].values.reshape(-1,1))[:,0] \n\n#normalize features\nskewness = X[num_col].apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\nskewed_features = skewness.index\nX[skewed_features] = np.log1p(X[skewed_features])\n\n#StandScaler\nX[num_col] = StandardScaler().fit_transform(X[num_col])\n\n#One hot encoding\nfor col in cat_col:\n    X[col] = X[col].apply(lambda x: str(x) + str(col))\n    \ndummies_train = pd.get_dummies(X[cat_col])\nX = X.drop(cat_col, axis=1)\nX = X.join(dummies_train)\n\nX_preprocessed_train = X","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:18:06.419767Z","iopub.execute_input":"2023-06-26T14:18:06.420385Z","iopub.status.idle":"2023-06-26T14:18:06.698312Z","shell.execute_reply.started":"2023-06-26T14:18:06.420294Z","shell.execute_reply":"2023-06-26T14:18:06.697102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_preprocessed_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:18:06.699944Z","iopub.execute_input":"2023-06-26T14:18:06.700323Z","iopub.status.idle":"2023-06-26T14:18:06.728372Z","shell.execute_reply.started":"2023-06-26T14:18:06.700294Z","shell.execute_reply":"2023-06-26T14:18:06.727075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we basically do the same for test set, except for one hot encoding where we would create a framework and use **.align** function to encode the dataset.","metadata":{}},{"cell_type":"code","source":"#Transfer the columns to categorical data\ntransfer_col = ['MSSubClass', 'OverallQual', 'OverallCond', 'BsmtFullBath', \n                'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n                'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', \n                'MoSold', 'YrSold']\nfor i in transfer_col:\n    test[i] = test[i].astype(str)\n\nX = test.drop([\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"Utilities\"] ,axis=1)\n\nnum_col = X.select_dtypes(include=['int64', 'float64']).columns\ncat_col = X.select_dtypes(include=['object']).columns\n\nimputer_num = SimpleImputer(strategy='mean')\nimputer_cat = SimpleImputer(strategy='constant', fill_value='missing')\n\nfor col in X.columns:\n    if X[col].dtype == 'O':\n        X[col] = imputer_cat.fit_transform(X[col].values.reshape(-1,1))[:,0] \n    else:\n        X[col] = imputer_num.fit_transform(X[col].values.reshape(-1,1))[:,0] \n\nskewness = X[num_col].apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\nskewed_features = skewness.index\nX[skewed_features] = np.log1p(X[skewed_features])\n\nX[num_col] = StandardScaler().fit_transform(X[num_col])\n\n#One hot encode with align\nfor col in cat_col:\n    X[col] = X[col].apply(lambda x: str(x) + str(col))\ndummies_test = pd.get_dummies(X[cat_col])\nX = X.drop(cat_col, axis=1)\nfinal_train, final_test = dummies_train.align(dummies_test, join='left', axis=1)\nX = X.join(final_test)\nX = X.fillna(0)\n\nX_preprocessed_test = X","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:18:06.729812Z","iopub.execute_input":"2023-06-26T14:18:06.730173Z","iopub.status.idle":"2023-06-26T14:18:07.004928Z","shell.execute_reply.started":"2023-06-26T14:18:06.730145Z","shell.execute_reply":"2023-06-26T14:18:07.003274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_preprocessed_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:18:07.007655Z","iopub.execute_input":"2023-06-26T14:18:07.008043Z","iopub.status.idle":"2023-06-26T14:18:07.034493Z","shell.execute_reply.started":"2023-06-26T14:18:07.008000Z","shell.execute_reply":"2023-06-26T14:18:07.033549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline","metadata":{}},{"cell_type":"markdown","source":"Now we will be passing everything to a pipeline. What I am doing down here is creating estimators, you can image it like a function where you can pass a dataset through the function and return the output. \n\n(Note: All the above codes do not have any affect on what we will be doing starting from now, it'sjust for demonstration purposes)","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import skew\n\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntrain = train[train.GrLivArea < 4000] #Remove outliners\ny = np.log(train.SalePrice)\ntrain = train.drop(['Id','SalePrice'],axis=1)\ntest_id = test.Id\ntest = test.drop('Id',axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:10:44.174274Z","iopub.execute_input":"2023-06-26T14:10:44.175045Z","iopub.status.idle":"2023-06-26T14:10:44.227462Z","shell.execute_reply.started":"2023-06-26T14:10:44.174998Z","shell.execute_reply":"2023-06-26T14:10:44.226385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureDropper(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X.drop([\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"Utilities\"], axis=1)\n\nclass categorical_transformation(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        transfer_col = ['MSSubClass', 'OverallQual', 'OverallCond', 'BsmtFullBath', \n                        'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n                        'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', \n                        'MoSold', 'YrSold']\n        for i in transfer_col:\n            X[i] = X[i].astype(str)\n        return X\n\nclass Imputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        imputer_num = SimpleImputer(strategy='mean')\n        imputer_cat = SimpleImputer(strategy='constant', fill_value='missing')\n        for col in X.columns:\n            if X[col].dtype == 'O':\n                X[col] = imputer_cat.fit_transform(X[col].values.reshape(-1,1))[:,0] \n            else:\n                X[col] = imputer_num.fit_transform(X[col].values.reshape(-1,1))[:,0] \n        return X\n\nclass normalize(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        num_col = X.select_dtypes(exclude=\"O\").columns\n        X[num_col] = StandardScaler().fit_transform(X[num_col])\n        return X\n\nclass log_skewed(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        num_col = X.select_dtypes(exclude='O').columns\n        skewness = X[num_col].apply(lambda x: skew(x))\n        skewness = skewness[abs(skewness) > 0.5]\n        skewed_features = skewness.index\n        X[skewed_features] = np.log1p(X[skewed_features])\n        return X\n\nclass FeatureEncode(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        \n        #Create a framework based on train dataset\n        cat_col = framework.select_dtypes(include='O').columns\n        framework_dummies = pd.get_dummies(framework[cat_col])\n        \n        #One Hot encoding for passed in dataset\n        for col in cat_col:\n            X[col] = X[col].apply(lambda x: str(x) + str(col))\n        X_dummies = pd.get_dummies(X[cat_col])\n        X = X.drop(cat_col,axis=1)\n        _, final = framework_dummies.align(X_dummies, join='left', axis=1)\n        X = X.join(final)\n        X = X.fillna(0)\n        return X\n\n\nclass PCA_transformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        pca = PCA()\n        pca = PCA(n_components=107) #retain 95% variance\n        X_pca = pca.fit_transform(X)\n        return X_pca","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:10:45.655851Z","iopub.execute_input":"2023-06-26T14:10:45.656521Z","iopub.status.idle":"2023-06-26T14:10:45.672957Z","shell.execute_reply.started":"2023-06-26T14:10:45.656474Z","shell.execute_reply":"2023-06-26T14:10:45.671743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pipeline_framework is used to create a framework prototype for one hot encoding since train set and test set have differ categorical values, if we don't standardize the two dataset and just pass it through One Hot Encoding directly, it will create different datasets with different columns which we will not be able to pass it through training our models.","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline([\n        ('dropfeatures', FeatureDropper()),\n        ('cat_trans', categorical_transformation()),\n        ('impute', Imputer()),\n        ('log_feature', log_skewed()),\n        ('scaler', normalize()),\n        ('Encoding', FeatureEncode()),\n])\npipeline_framework = Pipeline([\n        ('dropfeatures', FeatureDropper()),\n        ('cat_trans', categorical_transformation()),\n        ('impute', Imputer()),\n])\n\nframework = pipeline_framework.fit_transform(train)\ncat_col = framework.select_dtypes(include='O')\nfor col in cat_col:\n    framework[col] = framework[col].apply(lambda x: str(x) + str(col))\n    \nX_preprocessed_train = pipeline.fit_transform(train)\nX_preprocessed_test = pipeline.fit_transform(test)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:10:47.040617Z","iopub.execute_input":"2023-06-26T14:10:47.041467Z","iopub.status.idle":"2023-06-26T14:10:47.788091Z","shell.execute_reply.started":"2023-06-26T14:10:47.041428Z","shell.execute_reply":"2023-06-26T14:10:47.787214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_preprocessed_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:10:51.233096Z","iopub.execute_input":"2023-06-26T14:10:51.233487Z","iopub.status.idle":"2023-06-26T14:10:51.256161Z","shell.execute_reply.started":"2023-06-26T14:10:51.233454Z","shell.execute_reply":"2023-06-26T14:10:51.255109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_preprocessed_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:10:52.030418Z","iopub.execute_input":"2023-06-26T14:10:52.030793Z","iopub.status.idle":"2023-06-26T14:10:52.053879Z","shell.execute_reply.started":"2023-06-26T14:10:52.030766Z","shell.execute_reply":"2023-06-26T14:10:52.052733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed_train, y, \n                                                    test_size=0.2, random_state=0)\n\ndef rmse_cv(model):\n    cv_score = cross_val_score(model, X_preprocessed_train, y, cv=5, \n                               scoring='neg_mean_squared_error')\n    rmse = np.sqrt(-cv_score).mean()\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2023-06-26T13:30:29.712340Z","iopub.execute_input":"2023-06-26T13:30:29.712785Z","iopub.status.idle":"2023-06-26T13:30:29.727507Z","shell.execute_reply.started":"2023-06-26T13:30:29.712746Z","shell.execute_reply":"2023-06-26T13:30:29.726284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First start off with models with no parameters at all, we will be doing hyperparameter tuning with GridSearchCV in next section","metadata":{}},{"cell_type":"code","source":"models_raw = {\n    'Ridge': RidgeCV(),\n    'Lasso': LassoCV(alphas=np.logspace(-4,4,9)),\n    'ElasticNet': ElasticNetCV(),\n    'XGBoost': XGBRegressor(),\n    'SVR': SVR()\n}\n\n\nfor name, model in models_raw.items():\n    print(f\"{name} RMSE: {rmse_cv(model)}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-26T13:30:31.544418Z","iopub.execute_input":"2023-06-26T13:30:31.544829Z","iopub.status.idle":"2023-06-26T13:30:46.183397Z","shell.execute_reply.started":"2023-06-26T13:30:31.544798Z","shell.execute_reply":"2023-06-26T13:30:46.182178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Linear regressors appear to have the best performances at this point, but this may be largely due to the fact that other more complex models like XGBoost usually will give a much better performance after hyperparameter tuning.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning\n\nNoted that I comment the whole script out since it will take a very long time to train the models which I would include the results as comment at the bottom if you are interested.","metadata":{}},{"cell_type":"code","source":"param_grids = {\n    'Ridge': {\n        'fit_intercept': [True, False],\n        'alphas': [[0.1, 1.0, 10.0], [0.01, 0.1, 1.0], [0.001, 0.01, 0.1]]\n    },\n    'Lasso': {\n        'fit_intercept': [True, False],\n    },\n    'ElasticNet': {\n        'fit_intercept': [True, False],\n        'l1_ratio': [0.1, 0.5, 0.9],\n        'alphas': [[0.1, 1.0, 10.0], [0.01, 0.1, 1.0], [0.001, 0.01, 0.1]]\n    },\n    'XGBoost': {\n        'n_estimators': [500, 1000],\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 8],\n        'reg_alpha': [0.0, 0.2],\n        'reg_lambda': [0.0, 0.2],\n        'min_child_weight': [1, 3, 5],\n    },\n    'SVR': {\n        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n        'C': [0.1, 1, 10],\n        'epsilon': [0.01, 0.1, 1],\n        'gamma': ['scale', 'auto'],\n    }\n    \n}","metadata":{"execution":{"iopub.status.busy":"2023-06-26T12:47:17.076865Z","iopub.execute_input":"2023-06-26T12:47:17.077306Z","iopub.status.idle":"2023-06-26T12:47:17.086379Z","shell.execute_reply.started":"2023-06-26T12:47:17.077267Z","shell.execute_reply":"2023-06-26T12:47:17.085279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrids = {}\ncv = 3\n\nfor model_name, model in models_raw.items():\n    \n    grids[model_name] = GridSearchCV(estimator=model, \n                                     param_grid=param_grids[model_name], \n                                     cv=cv, \n                                     scoring='neg_mean_squared_error', \n                                     n_jobs=-1)\n    grids[model_name].fit(X_preprocessed_train, y)\n    best_params = grids[model_name].best_params_\n    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n    \n    print(f'Best parameters for {model_name}: {best_params}')\n    print(f'Best RMSE for {model_name}: {best_score}\\n')\n    \n\"\"\"\nBest parameters for Ridge: {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': True}\nBest RMSE for Ridge: 0.11655349784666286\n\nBest parameters for Lasso: {'fit_intercept': True}\nBest RMSE for Lasso: 0.1171914969370188\n\nBest parameters for ElasticNet: {'alphas': [0.001, 0.01, 0.1], 'fit_intercept': True, 'l1_ratio': 0.5}\nBest RMSE for ElasticNet: 0.11489623589933039\n\nBest parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 1000, 'reg_alpha': 0.0, 'reg_lambda': 0.2}\nBest RMSE for XGBoost: 0.12194649951014798\n\nBest parameters for SVR: {'C': 10, 'epsilon': 0.01, 'gamma': 'auto', 'kernel': 'rbf'}\nBest RMSE for SVR: 0.11591388647499179\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-06-26T12:47:17.914148Z","iopub.execute_input":"2023-06-26T12:47:17.914553Z","iopub.status.idle":"2023-06-26T13:23:54.065696Z","shell.execute_reply.started":"2023-06-26T12:47:17.914522Z","shell.execute_reply":"2023-06-26T13:23:54.064577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on these results, we will run a 2nd round of hyperparameter tuning to deal with edge cases, it's basically the same script with different param_grids.","metadata":{}},{"cell_type":"code","source":"param_grids = {\n    'Ridge': {\n        'fit_intercept': [True],\n        'alphas': [[0.1, 1.0, 10.0]],\n    },\n    'Lasso': {\n        'fit_intercept': [True],\n        'max_iter': [500, 800, 1000],\n\n    },\n    'ElasticNet': {\n        'fit_intercept': [True],\n        'l1_ratio': [0.01, 0.05, 0.1],\n        'alphas': [[0.001, 0.01, 0.1]],\n\n    },\n    'XGBoost': {\n        'n_estimators': [1000],\n        'learning_rate': [0.05, 0.1, 0.2],\n        'max_depth': [3, 4, 5],\n        'reg_alpha': [0.0],\n        'reg_lambda': [0.1, 0.2, 0.5],\n        'min_child_weight': [3],\n    },\n    'SVR': {\n        'kernel': ['rbf'],\n        'C': [8, 10, 15, 20],\n        'epsilon': [0.01, 0.05],\n        'gamma': ['auto']\n    }\n    \n}\n\ngrids = {}\ncv = 3\n\nfor model_name, model in models_raw.items():\n    \n    grids[model_name] = GridSearchCV(estimator=model, \n                                     param_grid=param_grids[model_name], \n                                     cv=cv, \n                                     scoring='neg_mean_squared_error', \n                                     n_jobs=-1)\n    grids[model_name].fit(X_preprocessed_train, y)\n    best_params = grids[model_name].best_params_\n    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n    \n    print(f'Best parameters for {model_name}: {best_params}')\n    print(f'Best RMSE for {model_name}: {best_score}\\n')\n    \n\"\"\"\nBest parameters for Ridge: {'alphas': [0.1, 1.0, 10.0], 'fit_intercept': True}\nBest RMSE for Ridge: 0.11655349784666286\n\nBest parameters for Lasso: {'fit_intercept': True, 'max_iter': 500}\nBest RMSE for Lasso: 0.1171914969370188\n\nBest parameters for ElasticNet: {'alphas': [0.001, 0.01, 0.1], 'fit_intercept': True, 'l1_ratio': 0.1}\nBest RMSE for ElasticNet: 0.11558577029260873\n\nBest parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 1000, 'reg_alpha': 0.0, 'reg_lambda': 0.2}\nBest RMSE for XGBoost: 0.12194649951014798\n\nBest parameters for SVR: {'C': 8, 'epsilon': 0.01, 'gamma': 'auto', 'kernel': 'rbf'}\nBest RMSE for SVR: 0.11511200323011732\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-06-26T13:35:26.325735Z","iopub.execute_input":"2023-06-26T13:35:26.326317Z","iopub.status.idle":"2023-06-26T13:43:06.104303Z","shell.execute_reply.started":"2023-06-26T13:35:26.326277Z","shell.execute_reply":"2023-06-26T13:43:06.102807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LinearRegression = LinearRegression()\nRidge = RidgeCV(alphas=[0.1, 1.0, 10.0], fit_intercept=True)\nLasso = LassoCV(alphas=np.logspace(-4,4,9), fit_intercept=True, max_iter=500)\nElasticNet = ElasticNetCV(alphas=[0.001, 0.01, 0.1], fit_intercept=True, l1_ratio=0.1)\nXGBoost = XGBRegressor(learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=1000, reg_lambda=0.2)\nSVR = SVR(C=8, epsilon=0.01, gamma='auto', kernel='rbf')","metadata":{"execution":{"iopub.status.busy":"2023-06-26T13:45:28.281618Z","iopub.execute_input":"2023-06-26T13:45:28.282098Z","iopub.status.idle":"2023-06-26T13:45:28.289522Z","shell.execute_reply.started":"2023-06-26T13:45:28.282059Z","shell.execute_reply":"2023-06-26T13:45:28.288479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_models = {\n    'Ridge': Ridge,\n    'Lasso': Lasso,\n    'ElasticNet': ElasticNet,\n    'XGBoost': XGBoost,\n    'SVR': SVR,\n}\n\nfor model_name, model in base_models.items():\n    model.fit(X_train, y_train)\n    model_cvscore = np.sqrt(-cross_val_score(model, X_preprocessed_train, y, cv=5, scoring='neg_mean_squared_error')).mean()\n    print(f\"{model_name} score: {model_cvscore}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-26T13:45:43.410768Z","iopub.execute_input":"2023-06-26T13:45:43.411199Z","iopub.status.idle":"2023-06-26T13:46:35.926807Z","shell.execute_reply.started":"2023-06-26T13:45:43.411166Z","shell.execute_reply":"2023-06-26T13:46:35.925590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Ensembling\n\nThis section will be ensembling the above models together, using stacking and voting.\n\nEnsembling is a very powerful technique in machine learning in which we use it to combine the performance of different models together to get a more robust outcome which usually give a better performance in general.","metadata":{}},{"cell_type":"markdown","source":"## Voting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\n\n# Create a voting classifier\nvoting_model = VotingRegressor(estimators=[('L1', Ridge), \n                                          ('L2', Lasso),\n                                          ('EN', ElasticNet),\n                                          ('xgb', XGBoost),\n                                          ('svr', SVR)])\n\nvoting_model.fit(X_preprocessed_train, y)\nrmse_voting = np.sqrt(-cross_val_score(voting_model, X_preprocessed_train, y, cv=5, scoring='neg_mean_squared_error')).mean()\nprint(f\"VotingRegressor RMSE: {rmse_voting}\")\n#RMSE: 0.10977104827444395","metadata":{"execution":{"iopub.status.busy":"2023-06-26T13:48:04.580558Z","iopub.execute_input":"2023-06-26T13:48:04.580996Z","iopub.status.idle":"2023-06-26T13:49:07.066255Z","shell.execute_reply.started":"2023-06-26T13:48:04.580967Z","shell.execute_reply":"2023-06-26T13:49:07.064879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_model2 = VotingRegressor(estimators=[('L1', Ridge), \n                                          ('L2', Lasso),\n                                          ('EN', ElasticNet),])\n\nvoting_model2.fit(X_preprocessed_train, y)\nrmse_voting2 = np.sqrt(-cross_val_score(voting_model2, X_preprocessed_train, y, cv=5, scoring='neg_mean_squared_error')).mean()\nprint(f\"VotingRegressor2 RMSE: {rmse_voting2}\")\n#RMSE: 0.11280943325732835","metadata":{"execution":{"iopub.status.busy":"2023-06-26T13:56:01.231444Z","iopub.execute_input":"2023-06-26T13:56:01.231904Z","iopub.status.idle":"2023-06-26T13:56:06.602647Z","shell.execute_reply.started":"2023-06-26T13:56:01.231875Z","shell.execute_reply":"2023-06-26T13:56:06.599368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\n\nbase_regressors = [('L1', Ridge), \n                   ('L2', Lasso),\n                   ('EN', ElasticNet),\n                   ('xgb', XGBoost),\n                   ('svr', SVR)]\n\n\nensemble = StackingRegressor(\n    estimators=base_regressors,\n    final_estimator=XGBoost\n)\nensemble.fit(X_preprocessed_train,y)\n\nscores = cross_val_score(\n    ensemble, X_preprocessed_train, y, cv=5, scoring='neg_mean_squared_error'\n)\n\nprint(\"Mean score:\", np.sqrt(-scores).mean())\nprint(\"Standard deviation:\", np.sqrt(-scores).std())\n\n# Mean score: 0.13003926654506529\n# Standard deviation: 0.006159681441211842","metadata":{"execution":{"iopub.status.busy":"2023-06-26T13:57:35.676458Z","iopub.execute_input":"2023-06-26T13:57:35.676892Z","iopub.status.idle":"2023-06-26T14:02:28.549539Z","shell.execute_reply.started":"2023-06-26T13:57:35.676863Z","shell.execute_reply":"2023-06-26T14:02:28.548422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"markdown","source":"Now we are going to make our submissions, I will be submitting all the baseline models with the three ensembled model to see what's the difference between them. Now, we will pass our test dataframe through our pipeline and make our predictions which you can see one of the reasons why we use pipeline is that we can easily reuse it in the future when we want to pass it through test set, or make modifications to the pipeline as well, making everything more convenient.","metadata":{}},{"cell_type":"code","source":"final_models = {\n    'Ridge': Ridge,\n    'Lasso': Lasso,\n    'ElasticNet': ElasticNet,\n    'XGBoost': XGBoost,\n    'SVR': SVR,\n    'Voting1': voting_model,\n    'Voting2': voting_model2,\n    'Stack': ensemble\n}\nfor model_name, model in final_models.items():\n    submission_df = pd.DataFrame({'Id': test_id, 'SalePrice': np.exp(model.predict(X_preprocessed_test))})\n    submission_name = 'final_submission_' + model_name + '.csv'\n    submission_df.to_csv(submission_name, index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:12:06.327329Z","iopub.execute_input":"2023-06-26T14:12:06.328410Z","iopub.status.idle":"2023-06-26T14:12:08.346596Z","shell.execute_reply.started":"2023-06-26T14:12:06.328373Z","shell.execute_reply":"2023-06-26T14:12:08.345408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The score of each submission is as followed:\n\nVoting1: 0.13708 \n\nVoting2: 0.13798\n\nRidge: 0.13957\n\nLasso: 0.14043\n\nSVR: 0.14772\n\nElasticNet: 0.14782\n\nXGBoost: 0.14825\n\nStack: 0.15377","metadata":{}},{"cell_type":"markdown","source":"The overall result is fairly alright standing at 30% on leaderboard, you can furthur improve the model performances by training additional models and most importantly, adding useful features based on domain knowledge which is something I didn't do much in this notebook. \n\nIf you are still here, feel free to give me some comments on how to improve the model or whether there are anything I did wrong in my notebook, your help and support is very much appreciated.","metadata":{}}]}