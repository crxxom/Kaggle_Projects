{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f90c7e",
   "metadata": {
    "papermill": {
     "duration": 0.018758,
     "end_time": "2023-08-10T13:42:40.692458",
     "exception": false,
     "start_time": "2023-08-10T13:42:40.673700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">‚ùó Important ‚ùó </span></div>\n",
    "\n",
    "Hey there, whether you come from the ‚úÖ Comprehensive Overview on NLP for Beginners ü•≥  notebook or you click on this notebook directly, I just want to let you know that this notebook is part 1 to the NLP beginner series. Here are the links to the whole series:\n",
    "    \n",
    "**‚úÖ Comprehensive Overview on NLP for Beginners ü•≥ (collection of all series)** <br>\n",
    " https://www.kaggle.com/code/crxxom/comprehensive-overview-on-nlp-for-beginners\n",
    " \n",
    "    \n",
    "**üî¥ NLP Beginner Series Part 1: NLP Preprocessing** <br>\n",
    "https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-1-nlp-preprocessing\n",
    "\n",
    "**üü° NLP Beginner Series Part 2.1: Word Embeddings** <br>\n",
    "https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-2-1-word-embeddings\n",
    "\n",
    "**üü¢ NLP Beginner Series Part 2.2: Embedding Models** <br>\n",
    "https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-2-2-embedding-models\n",
    "\n",
    "**üü£ NLP Beginner Series Part 3: Case Study** <br>\n",
    " https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-3-case-study\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e875b",
   "metadata": {
    "papermill": {
     "duration": 0.017723,
     "end_time": "2023-08-10T13:42:40.728413",
     "exception": false,
     "start_time": "2023-08-10T13:42:40.710690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üî•  Overview of the Notebook  üî•</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dbb17",
   "metadata": {
    "papermill": {
     "duration": 0.017589,
     "end_time": "2023-08-10T13:42:40.763824",
     "exception": false,
     "start_time": "2023-08-10T13:42:40.746235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## >> Important concepts and techniques you need to know to get you started with NLP explained in a **‚û° beginner friendly way ‚¨Ö**\n",
    "\n",
    "## >> **ONLY** pre-requisite you need: Python, Pandas and Numpy\n",
    "\n",
    "## >> üïë Take your Time! (~ 20-30 minutes)\n",
    "\n",
    "## >> Is this notebook for me ü§î\n",
    "\n",
    "This notebook **covers the absolute fundamentals of NLP to let you have an overview with what exactly is NLP and how it work.** \n",
    "\n",
    "### üåü Skim through the notebook\n",
    "\n",
    "Quickly scroll through the whole notebook to see if the style suit your learning needs!\n",
    "\n",
    "### üåü This notebook is beginner friendly\n",
    "This notebook is very much a **beginner friendly notebook**, the only pre-requisite you will need is Python and some common data science frameworks such as Pandas and Numpy. All the concepts covered in this notebook will be clearly explained but may not be deeply explained, you will be encouraged to do additional research on your own if you would like to understand more about a particular topic. The goal of this notebook is to give you an overview to NLP and introduce you with key vocabularies and concepts you will need to know in NLP.\n",
    "\n",
    "\n",
    "### üåü Seperate sections to suit your needs\n",
    "**Each section of the notebook is seperated**, ie. you do NOT need to run the code on section 1 to be able to run the code in section 2, libraries and framework will be introduced and imported in each individual section, dataset are not interrelatble across sections. If you are only interested in any particular section, **you can skip through the sections to run the code in that particular section only**. In fact, I try to make each codeblock as seperated as possible so that you do not need to run codeblock A to be able to run B.\n",
    "\n",
    "\n",
    "### üåü Images to help you learn\n",
    "This notebook contains **a lot of images** to demonstrate the concepts and principles as I find it to be the best way to understand complicated concepts. These images are mostly images from google and screenshots from youtube tutorials, if any parties find it inappropriate to use their images/screenshots, please let me know I will delete is as soon as possible.\n",
    "\n",
    "### üåü I am a beginner also\n",
    "This notebook is **written from the perspective of a beginner** to the world of machine learning. With that being said, there may be some concepts that isn't covered clearly/deeply, but on the other hand, I feel like it will be easier to catch up if you are a beginner as well, as I will not be throwing some complicated jargons and overload you too much since I am a beginner as well. Additionally I have also included a lot of resources if you want to have a deeper insight for each topics.\n",
    "\n",
    "\n",
    "### üåü Notebook designed in a way to not overwhelm you\n",
    "**Take your time!** Small and lengthy words are hard and dull to read, that's why this notebook trys to **minimize the amount of text and use bold text, images and emojis** so that you can navigate the notebook faster. The notebook also attempts to **minimize the amount of code** as much as possible, to save you from being too overwhelmed. \n",
    "\n",
    "**Try to run the codeblocks youself**, running and changing codes in the codeblocks can give you a more immersive learning experience. \n",
    "\n",
    "If you appreciate my work or would like to express your views/make suggestions on some of my code and content, feel free to comment and correct me, I will make sure to keep updating and make improvements to this notebook! I hope you will find this notebook useful and an enjoyable journey.\n",
    "\n",
    "## üîé Concepts and techniques you can take away with you from this notebook\n",
    "\n",
    "#### üü¢ [Overview of NLP Pipeline](#nlp-pipeline)\n",
    "\n",
    "#### üü¢ [Tokenization and spaCy pipeline](#tokenization)\n",
    "\n",
    "#### üü¢ [Stop Word](#stop-word)\n",
    "\n",
    "#### üü¢ [Stemming and Lemmatization](#stemming-and-lemmatization)\n",
    "\n",
    "#### üü¢ [Out of Vocabulary (OOV)](#out-of-vocabulary)\n",
    "\n",
    "#### üü¢ [POS (Part of Speech) tagging](#part-of-speech-tagging)\n",
    "\n",
    "#### üü¢ [Name Entity Recognition (NER)](#named-entity-recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d05c5",
   "metadata": {
    "papermill": {
     "duration": 0.017875,
     "end_time": "2023-08-10T13:42:40.799974",
     "exception": false,
     "start_time": "2023-08-10T13:42:40.782099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/YmCSFRs.png\" style=\"height:500px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92d9a4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-07T07:39:21.604021Z",
     "iopub.status.busy": "2023-08-07T07:39:21.603523Z",
     "iopub.status.idle": "2023-08-07T07:39:21.612542Z",
     "shell.execute_reply": "2023-08-07T07:39:21.610742Z",
     "shell.execute_reply.started": "2023-08-07T07:39:21.603985Z"
    },
    "papermill": {
     "duration": 0.017214,
     "end_time": "2023-08-10T13:42:40.834915",
     "exception": false,
     "start_time": "2023-08-10T13:42:40.817701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/buymiQ8.png\" style=\"height:500px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87b03e",
   "metadata": {
    "papermill": {
     "duration": 0.017526,
     "end_time": "2023-08-10T13:42:40.870523",
     "exception": false,
     "start_time": "2023-08-10T13:42:40.852997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"nlp-pipeline\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Overview of NLP Pipeline</span></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00ba59",
   "metadata": {
    "papermill": {
     "duration": 0.017788,
     "end_time": "2023-08-10T13:42:40.905705",
     "exception": false,
     "start_time": "2023-08-10T13:42:40.887917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/Jm6yryM.png\" style=\"height:250px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c230d1",
   "metadata": {
    "papermill": {
     "duration": 0.017197,
     "end_time": "2023-08-10T13:42:40.940782",
     "exception": false,
     "start_time": "2023-08-10T13:42:40.923585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1Ô∏è‚É£ Data Collection\n",
    "* Collection of raw data to train the model\n",
    "* Typical NLP data examples are text data such as reviews from amazon, tweets on twitter or news articles from google news\n",
    " \n",
    "### 2Ô∏è‚É£ Text Cleaning\n",
    "* Common text cleaning tasks are removing special characters, removing HTML tags, removing extra lines and space etc.\n",
    "* Often involve the use of **regular expression** (it's basically a technique to input a sequence to extract a pattern in a sentence, it is not a machine learning thing but more like a python thing)\n",
    "\n",
    "### 3Ô∏è‚É£ Preprocessing\n",
    "* The point of preprocessing is to transform our dataset in a way that allow our model to better grab the key information and value of our input data\n",
    "* A lot of techniques and concepts will be covered in this section including,\n",
    "* **Tokenization**\n",
    "* **Stop word**\n",
    "* **Stemming and Lemmatization**\n",
    "* **POS tagging**\n",
    "* **Name Entity Recognition (NER)**\n",
    "\n",
    "\n",
    "### 4Ô∏è‚É£ Feature Engineering\n",
    "* In NLP, feature engineering process mostly involve **vectorizing** words (converting strings into meaningful numbers), techniques and concept covered in this section including,\n",
    "* **Bag of Word (BOW)**\n",
    "* **Bag of n-grams**\n",
    "* **Term Frequency - Inverse Document Frequency (TF-IDF)**\n",
    "* **Word embedding**\n",
    "* **Cosine Similarity**\n",
    "* **CBOW and SkipGram**\n",
    "* **Word2Vec**\n",
    "* **GloVe**\n",
    "* **fastText**\n",
    "* **Bidirectional Encoder Representations from Transformers (BERT)**\n",
    "\n",
    "### 5Ô∏è‚É£ Modelling\n",
    "* Examples of models commonly used for NLP tasks are Naive Bayes and deep learning algorithms like LSTM\n",
    "\n",
    "### 6Ô∏è‚É£-8Ô∏è‚É£ Evaluation, Deployment and Monitoring and model updating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c7139",
   "metadata": {
    "papermill": {
     "duration": 0.017233,
     "end_time": "2023-08-10T13:42:40.975549",
     "exception": false,
     "start_time": "2023-08-10T13:42:40.958316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"tokenization\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üåü Tokenization with spaCy üåü</span></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59a93c",
   "metadata": {
    "papermill": {
     "duration": 0.017549,
     "end_time": "2023-08-10T13:42:41.011032",
     "exception": false,
     "start_time": "2023-08-10T13:42:40.993483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/ia7NSZy.png\" style=\"height:300px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e7d3e",
   "metadata": {
    "papermill": {
     "duration": 0.018194,
     "end_time": "2023-08-10T13:42:41.047117",
     "exception": false,
     "start_time": "2023-08-10T13:42:41.028923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "‚ùó *Tokenization is used in natural language processing to **split paragraphs and sentences into smaller units** that can be more easily assigned meaning. The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words).*\n",
    "\n",
    "In NLP, you will most commonly come across with the term 'token'. A lot of time a token is the same as a word, especially in english language, but for different languages, it may be different due to the difference in grammar and structure.\n",
    "\n",
    "In this section, we will be exploring the power of tokenization with **spaCy**, a very popular NLP framework. We will be looking at what is tokenization and some useful methods and attributes built in spaCy libaray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f9abd1",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-10T13:42:41.085038Z",
     "iopub.status.busy": "2023-08-10T13:42:41.084436Z",
     "iopub.status.idle": "2023-08-10T13:42:41.088616Z",
     "shell.execute_reply": "2023-08-10T13:42:41.087887Z"
    },
    "papermill": {
     "duration": 0.026847,
     "end_time": "2023-08-10T13:42:41.092496",
     "exception": false,
     "start_time": "2023-08-10T13:42:41.065649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if you never download spacy before:\n",
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a81938d4",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-08-10T13:42:41.130875Z",
     "iopub.status.busy": "2023-08-10T13:42:41.130198Z",
     "iopub.status.idle": "2023-08-10T13:42:41.140017Z",
     "shell.execute_reply": "2023-08-10T13:42:41.138927Z"
    },
    "papermill": {
     "duration": 0.031726,
     "end_time": "2023-08-10T13:42:41.142365",
     "exception": false,
     "start_time": "2023-08-10T13:42:41.110639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter out the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af4782",
   "metadata": {
    "papermill": {
     "duration": 0.017617,
     "end_time": "2023-08-10T13:42:41.178009",
     "exception": false,
     "start_time": "2023-08-10T13:42:41.160392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before we start talking about tokenization, I would like to briefy introduce you with the pipelines that spacy offer. spaCy offer users with trained pipelines that you can use by using the line `spacy.load(\"en_core_web_sm\")` where en stands for english, and sm stands for small. \n",
    "\n",
    "There are different pipelines that spacy offer other than \"en_core_web_sm\" (12 MB), such as \"en_core_web_lg\" (560MB), \"zh_core_web_md\", (zh -> chinese, md -> medium, 74MB) and more. You can see **all the available pipeline on https://spacy.io/usage/models** and click into each 'pipeline' for more information. \n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/eAqN9F9.png\" style=\"height:400px;\"> </div>\n",
    "\n",
    "<br>\n",
    "\n",
    "**You need to download the pipeline before using it by typing `python -m spacy download en_core_web_sm` in your terminal**, which is what we will be using in this section.\n",
    "\n",
    "\n",
    "For more information: \n",
    "\n",
    "spaCy pipeline: https://spacy.io/usage/processing-pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f027c42",
   "metadata": {
    "papermill": {
     "duration": 0.017758,
     "end_time": "2023-08-10T13:42:41.213637",
     "exception": false,
     "start_time": "2023-08-10T13:42:41.195879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/sEZ1yu7.png\" style=\"height:200px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0f9c6",
   "metadata": {
    "papermill": {
     "duration": 0.017292,
     "end_time": "2023-08-10T13:42:41.248687",
     "exception": false,
     "start_time": "2023-08-10T13:42:41.231395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### üßë‚Äçüíª Let's code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a760cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:42:41.285788Z",
     "iopub.status.busy": "2023-08-10T13:42:41.285395Z",
     "iopub.status.idle": "2023-08-10T13:42:56.302000Z",
     "shell.execute_reply": "2023-08-10T13:42:56.300877Z"
    },
    "papermill": {
     "duration": 15.039221,
     "end_time": "2023-08-10T13:42:56.305124",
     "exception": false,
     "start_time": "2023-08-10T13:42:41.265903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components in en_core_web_sm pipeline:  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Components in blank pipeline:  []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_sm = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp_blank = spacy.blank(\"en\")\n",
    "\n",
    "print(\"Components in en_core_web_sm pipeline: \", nlp_sm.pipe_names)\n",
    "print(\"Components in blank pipeline: \", nlp_blank.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe66a0",
   "metadata": {
    "papermill": {
     "duration": 0.018084,
     "end_time": "2023-08-10T13:42:56.341779",
     "exception": false,
     "start_time": "2023-08-10T13:42:56.323695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can add elements manually to your nlp pipeline in your own need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c82440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:42:56.378976Z",
     "iopub.status.busy": "2023-08-10T13:42:56.378193Z",
     "iopub.status.idle": "2023-08-10T13:42:56.396984Z",
     "shell.execute_reply": "2023-08-10T13:42:56.396011Z"
    },
    "papermill": {
     "duration": 0.040251,
     "end_time": "2023-08-10T13:42:56.399539",
     "exception": false,
     "start_time": "2023-08-10T13:42:56.359288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components in sm pipeline:  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'sentencizer']\n"
     ]
    }
   ],
   "source": [
    "nlp_sm.add_pipe('sentencizer')\n",
    "print(\"Components in sm pipeline: \", nlp_sm.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc287b6",
   "metadata": {
    "papermill": {
     "duration": 0.017854,
     "end_time": "2023-08-10T13:42:56.435710",
     "exception": false,
     "start_time": "2023-08-10T13:42:56.417856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Each spaCy pipeline contains a tokenizer, even for spacy.blank() pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34faa76b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:42:56.474475Z",
     "iopub.status.busy": "2023-08-10T13:42:56.473283Z",
     "iopub.status.idle": "2023-08-10T13:42:56.505506Z",
     "shell.execute_reply": "2023-08-10T13:42:56.504109Z"
    },
    "papermill": {
     "duration": 0.054389,
     "end_time": "2023-08-10T13:42:56.507967",
     "exception": false,
     "start_time": "2023-08-10T13:42:56.453578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of doc1[0] for nlp_sm:  <class 'spacy.tokens.token.Token'>\n",
      "Type of doc2[0] for nlp_blank:  <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp_sm(\"Natural Language Processing\")\n",
    "token1_0 = doc1[0]\n",
    "print(\"Type of doc1[0] for nlp_sm: \", type(token1_0))\n",
    "doc2 = nlp_blank(\"Natural Language Processing\")\n",
    "token2_0 = doc2[0]\n",
    "print(\"Type of doc2[0] for nlp_blank: \", type(token2_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec11777",
   "metadata": {
    "papermill": {
     "duration": 0.018113,
     "end_time": "2023-08-10T13:42:56.544578",
     "exception": false,
     "start_time": "2023-08-10T13:42:56.526465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tokenization is often the very first step of preprocessing, it is important to break down a sentence to individual 'token' so that each token has like its own meaning. In fact, in some cases, you may want to assign token manually especially when it comes to slang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95cc76f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:42:56.583540Z",
     "iopub.status.busy": "2023-08-10T13:42:56.582744Z",
     "iopub.status.idle": "2023-08-10T13:42:57.733228Z",
     "shell.execute_reply": "2023-08-10T13:42:57.732061Z"
    },
    "papermill": {
     "duration": 1.172707,
     "end_time": "2023-08-10T13:42:57.735464",
     "exception": false,
     "start_time": "2023-08-10T13:42:56.562757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yessir\n"
     ]
    }
   ],
   "source": [
    "# Before manually assigning token\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"yessir\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124d4f04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:42:57.777429Z",
     "iopub.status.busy": "2023-08-10T13:42:57.776996Z",
     "iopub.status.idle": "2023-08-10T13:42:58.954586Z",
     "shell.execute_reply": "2023-08-10T13:42:58.953453Z"
    },
    "papermill": {
     "duration": 1.200625,
     "end_time": "2023-08-10T13:42:58.957080",
     "exception": false,
     "start_time": "2023-08-10T13:42:57.756455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "sir\n"
     ]
    }
   ],
   "source": [
    "# After manually assigning token\n",
    "\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"yessir\", [\n",
    "    {ORTH: \"yes\"},\n",
    "    {ORTH: \"sir\"},\n",
    "])\n",
    "\n",
    "doc = nlp(\"yessir\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c382cc",
   "metadata": {
    "papermill": {
     "duration": 0.018308,
     "end_time": "2023-08-10T13:42:58.994045",
     "exception": false,
     "start_time": "2023-08-10T13:42:58.975737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tokenization is essential on its own, but spaCy offers a lot of **built-in attributes and methods** you can use to achieve cool things. Here are some few examples.\n",
    "\n",
    "For more information: https://spacy.io/api/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e9ce6f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:42:59.033387Z",
     "iopub.status.busy": "2023-08-10T13:42:59.032998Z",
     "iopub.status.idle": "2023-08-10T13:43:00.188001Z",
     "shell.execute_reply": "2023-08-10T13:43:00.186765Z"
    },
    "papermill": {
     "duration": 1.177833,
     "end_time": "2023-08-10T13:43:00.190422",
     "exception": false,
     "start_time": "2023-08-10T13:42:59.012589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The [alpha?: True] [currency?: False] [punctuation? False] [email? False]\n",
      "total [alpha?: True] [currency?: False] [punctuation? False] [email? False]\n",
      "is [alpha?: True] [currency?: False] [punctuation? False] [email? False]\n",
      "$ [alpha?: False] [currency?: True] [punctuation? False] [email? False]\n",
      "50 [alpha?: False] [currency?: False] [punctuation? False] [email? False]\n",
      ", [alpha?: False] [currency?: False] [punctuation? True] [email? False]\n",
      "pay [alpha?: True] [currency?: False] [punctuation? False] [email? False]\n",
      "me [alpha?: True] [currency?: False] [punctuation? False] [email? False]\n",
      "through [alpha?: True] [currency?: False] [punctuation? False] [email? False]\n",
      "email [alpha?: True] [currency?: False] [punctuation? False] [email? False]\n",
      "at [alpha?: True] [currency?: False] [punctuation? False] [email? False]\n",
      "abc@gmail.com [alpha?: False] [currency?: False] [punctuation? False] [email? True]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"The total is $50, pay me through email at abc@gmail.com\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token} [alpha?: {token.is_alpha}] [currency?: {token.is_currency}] [punctuation? {token.is_punct}] [email? {token.like_email}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94adfc75",
   "metadata": {
    "papermill": {
     "duration": 0.018604,
     "end_time": "2023-08-10T13:43:00.228129",
     "exception": false,
     "start_time": "2023-08-10T13:43:00.209525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"stop-word\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Stop Word</span></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42235f5b",
   "metadata": {
    "papermill": {
     "duration": 0.018678,
     "end_time": "2023-08-10T13:43:00.265680",
     "exception": false,
     "start_time": "2023-08-10T13:43:00.247002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img src=\"https://i.imgur.com/owv8iQX.jpg\" style=\"height:300px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2439e",
   "metadata": {
    "papermill": {
     "duration": 0.018774,
     "end_time": "2023-08-10T13:43:00.303583",
     "exception": false,
     "start_time": "2023-08-10T13:43:00.284809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "‚ùó *Stop words are a set of commonly used words in a language. Examples of stop words in English are ‚Äúa,‚Äù ‚Äúthe,‚Äù ‚Äúis,‚Äù ‚Äúare,‚Äù etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information.*\n",
    "\n",
    "There are several reasons why we want to remove stop words, with the most important one being to  **reduce the number of irrelevant words** in text analysis process so that words that contribute more to the context of the text will be given a relatively heavier weight. Other reasons would be to reduce the size of input to **enhance training efficiency**.\n",
    "\n",
    "Of course, we do not always want to remove these stop words for various reasons such as loss of information etc.\n",
    "\n",
    "\n",
    "For more information: <br>\n",
    "Why shouldn't we remove stop word:  <br>\n",
    "https://opensourceconnections.com/blog/2023/01/24/10-reasons-why-you-shouldnt-remove-stop-words/#:~:text=Removing%20stop%20words%20may%20now,retrieved%20in%20the%20first%20step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f3bd3",
   "metadata": {
    "papermill": {
     "duration": 0.018668,
     "end_time": "2023-08-10T13:43:00.341452",
     "exception": false,
     "start_time": "2023-08-10T13:43:00.322784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### üßë‚Äçüíª Let's code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66d830c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:00.381689Z",
     "iopub.status.busy": "2023-08-10T13:43:00.380673Z",
     "iopub.status.idle": "2023-08-10T13:43:00.388023Z",
     "shell.execute_reply": "2023-08-10T13:43:00.386720Z"
    },
    "papermill": {
     "duration": 0.030019,
     "end_time": "2023-08-10T13:43:00.390451",
     "exception": false,
     "start_time": "2023-08-10T13:43:00.360432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words in spacy: 326\n",
      "Some example of stop words: ['may', 'becomes', 'back', 'formerly', 'empty']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(\"Number of stop words in spacy:\", len(STOP_WORDS))\n",
    "\n",
    "print(\"Some example of stop words:\", list(STOP_WORDS)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ed9f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:00.430645Z",
     "iopub.status.busy": "2023-08-10T13:43:00.430245Z",
     "iopub.status.idle": "2023-08-10T13:43:01.612702Z",
     "shell.execute_reply": "2023-08-10T13:43:01.611268Z"
    },
    "papermill": {
     "duration": 1.205691,
     "end_time": "2023-08-10T13:43:01.615360",
     "exception": false,
     "start_time": "2023-08-10T13:43:00.409669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Apple and Google lead initiative for an industry specification to address unwanted tracking\n",
      "After filtering:  Apple Google lead initiative industry specification address unwanted tracking\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Apple and Google lead initiative for an industry specification to address unwanted tracking\")\n",
    "\n",
    "filtered_stop_words = []\n",
    "\n",
    "for token in doc:\n",
    "    if not token.is_stop: # we use token attribute .is_stop\n",
    "        filtered_stop_words.append(token.text)\n",
    "print(\"Original: \", doc)\n",
    "print(\"After filtering: \", \" \".join(filtered_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba154d8e",
   "metadata": {
    "papermill": {
     "duration": 0.01907,
     "end_time": "2023-08-10T13:43:01.653858",
     "exception": false,
     "start_time": "2023-08-10T13:43:01.634788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"stemming-and-lemmatization\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üåü Stemming and Lemmatization üåü</span></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8f246",
   "metadata": {
    "papermill": {
     "duration": 0.019199,
     "end_time": "2023-08-10T13:43:01.692557",
     "exception": false,
     "start_time": "2023-08-10T13:43:01.673358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/WbJvTWD.png\" style=\"height:300px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555b7af",
   "metadata": {
    "papermill": {
     "duration": 0.019812,
     "end_time": "2023-08-10T13:43:01.731696",
     "exception": false,
     "start_time": "2023-08-10T13:43:01.711884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "‚ùó *Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.*\n",
    "\n",
    "Stemming and Lemmatization are very important and fundamental concept in NLP. They both aim to achieve the same goal - **convert words to their 'base form/root meaning'**. In fact, we usually only use either one of stemming and lemmatization, but not both in our NLP task. \n",
    "\n",
    "The difference between Stemming and Lemmatization lies in their working principle. \n",
    "\n",
    "In stemming, it attempts to achieve the goal by setting a set of rules to alter the words to change them to their base form. Let's see a raw example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df9899cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:01.772618Z",
     "iopub.status.busy": "2023-08-10T13:43:01.772188Z",
     "iopub.status.idle": "2023-08-10T13:43:01.779992Z",
     "shell.execute_reply": "2023-08-10T13:43:01.778871Z"
    },
    "papermill": {
     "duration": 0.031923,
     "end_time": "2023-08-10T13:43:01.783069",
     "exception": false,
     "start_time": "2023-08-10T13:43:01.751146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing -> play\n",
      "happy -> happy\n",
      "studies -> stud\n",
      "plays -> play\n",
      "play -> play\n"
     ]
    }
   ],
   "source": [
    "input_words = ['playing', 'happy', 'studies', 'plays', 'play']\n",
    "\n",
    "def imaginary_stemming_function(token):\n",
    "    if token[-3:] == 'ies' or token[-3:] == 'ing':\n",
    "        token = token[:-3]\n",
    "    if token[-1] == 's':\n",
    "        token = token[:-1]\n",
    "    return token\n",
    "\n",
    "for token in input_words:\n",
    "    print(token, '->', imaginary_stemming_function(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c4d8f",
   "metadata": {
    "papermill": {
     "duration": 0.019079,
     "end_time": "2023-08-10T13:43:01.821567",
     "exception": false,
     "start_time": "2023-08-10T13:43:01.802488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I hope this show a bit of how stemming work. As you can see, stemming **does not have linguistic knowledge**, so although it may be faster compare to lemmatization, there are several problems associated with stemming such as stemming cannot covert 'gave' into 'give'. \n",
    "\n",
    "In fact, spaCy does not support stemming, so for illustration purpose, we will be using another popular NLP framework - **NLTK** to illustrate the concept of stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a8dff",
   "metadata": {
    "papermill": {
     "duration": 0.019225,
     "end_time": "2023-08-10T13:43:01.860231",
     "exception": false,
     "start_time": "2023-08-10T13:43:01.841006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### üßë‚Äçüíª Let's code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c067572d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:01.900791Z",
     "iopub.status.busy": "2023-08-10T13:43:01.900301Z",
     "iopub.status.idle": "2023-08-10T13:43:01.905529Z",
     "shell.execute_reply": "2023-08-10T13:43:01.904269Z"
    },
    "papermill": {
     "duration": 0.028309,
     "end_time": "2023-08-10T13:43:01.907889",
     "exception": false,
     "start_time": "2023-08-10T13:43:01.879580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you never install nltk before\n",
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "878af51e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:01.949041Z",
     "iopub.status.busy": "2023-08-10T13:43:01.948153Z",
     "iopub.status.idle": "2023-08-10T13:43:03.349272Z",
     "shell.execute_reply": "2023-08-10T13:43:03.347985Z"
    },
    "papermill": {
     "duration": 1.424993,
     "end_time": "2023-08-10T13:43:03.352405",
     "exception": false,
     "start_time": "2023-08-10T13:43:01.927412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give -> give\n",
      "gave -> gave\n",
      "giving -> give\n",
      "swim -> swim\n",
      "swims -> swim\n",
      "swimming -> swim\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "raw_words = ['give', 'gave', 'giving', 'swim', 'swims', 'swimming']\n",
    "\n",
    "for word in raw_words:\n",
    "    print(word, \"->\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08f77b",
   "metadata": {
    "papermill": {
     "duration": 0.019111,
     "end_time": "2023-08-10T13:43:03.391400",
     "exception": false,
     "start_time": "2023-08-10T13:43:03.372289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Lemmatization** on the other hand **have linguistic knowledge**, it basically works like a dictionary where each word is somehow linked by some algorithm/model to the base word. Let's illustrate the idea of lemmatization with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfe2b359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:03.431143Z",
     "iopub.status.busy": "2023-08-10T13:43:03.430790Z",
     "iopub.status.idle": "2023-08-10T13:43:04.595044Z",
     "shell.execute_reply": "2023-08-10T13:43:04.594266Z"
    },
    "papermill": {
     "duration": 1.186813,
     "end_time": "2023-08-10T13:43:04.597629",
     "exception": false,
     "start_time": "2023-08-10T13:43:03.410816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give -> give / 11640825575873464194\n",
      "gave -> give / 11640825575873464194\n",
      "giving -> give / 11640825575873464194\n",
      "swim -> swim / 13054409096476681252\n",
      "swims -> swim / 13054409096476681252\n",
      "swimming -> swimming / 12526975369366237900\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"give gave giving swim swims swimming\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"->\", token.lemma_, \"/\", token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47052b",
   "metadata": {
    "papermill": {
     "duration": 0.018037,
     "end_time": "2023-08-10T13:43:04.634655",
     "exception": false,
     "start_time": "2023-08-10T13:43:04.616618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can also add the .lemma_ yourself for special characters such as slang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "215fa0b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:04.673681Z",
     "iopub.status.busy": "2023-08-10T13:43:04.673267Z",
     "iopub.status.idle": "2023-08-10T13:43:04.687666Z",
     "shell.execute_reply": "2023-08-10T13:43:04.686714Z"
    },
    "papermill": {
     "duration": 0.036502,
     "end_time": "2023-08-10T13:43:04.689768",
     "exception": false,
     "start_time": "2023-08-10T13:43:04.653266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nah -> nah\n",
      "no -> no\n",
      "nope -> nope\n"
     ]
    }
   ],
   "source": [
    "# Before adding attribute_ruler\n",
    "\n",
    "doc = nlp(\"nah no nope\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cbc1476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:04.729223Z",
     "iopub.status.busy": "2023-08-10T13:43:04.728835Z",
     "iopub.status.idle": "2023-08-10T13:43:04.744347Z",
     "shell.execute_reply": "2023-08-10T13:43:04.743162Z"
    },
    "papermill": {
     "duration": 0.038284,
     "end_time": "2023-08-10T13:43:04.746715",
     "exception": false,
     "start_time": "2023-08-10T13:43:04.708431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nah -> no\n",
      "no -> no\n",
      "nope -> no\n"
     ]
    }
   ],
   "source": [
    "# After adding attribute_ruler\n",
    "\n",
    "ar = nlp.get_pipe(\"attribute_ruler\")\n",
    "\n",
    "ar.add([[{\"TEXT\": \"nah\"}],[{\"TEXT\": \"nope\",}]], {\"LEMMA\": \"no\"})\n",
    "\n",
    "doc = nlp(\"nah no nope\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df23564",
   "metadata": {
    "papermill": {
     "duration": 0.020158,
     "end_time": "2023-08-10T13:43:04.785898",
     "exception": false,
     "start_time": "2023-08-10T13:43:04.765740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So, the ultimate question, **stemming or lemmatization**. It really depends on how you take your tradeoffs, but in most of the cases, lemmatization is recommended for obvious reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2488a",
   "metadata": {
    "papermill": {
     "duration": 0.018494,
     "end_time": "2023-08-10T13:43:04.823707",
     "exception": false,
     "start_time": "2023-08-10T13:43:04.805213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/v0pQ56g.png\" style=\"height:400px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cab5dd",
   "metadata": {
    "papermill": {
     "duration": 0.018644,
     "end_time": "2023-08-10T13:43:04.861573",
     "exception": false,
     "start_time": "2023-08-10T13:43:04.842929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"out-of-vocabulary\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Out of Vocabulary (OOV)</span></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009df164",
   "metadata": {
    "papermill": {
     "duration": 0.018911,
     "end_time": "2023-08-10T13:43:04.899910",
     "exception": false,
     "start_time": "2023-08-10T13:43:04.880999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The concept of OOV is actually being illustrated in the above examples. OOV is basically what it suggests, words that are out of our domain/model, they can be slangs like 'lol', 'nah' etc. Or if you are training your own model, it can be all the words that didn't appear in your training process. This is an issue that you will face later on when discussing on topics such as Bag of Words and Word Embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c75ca40d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:04.941009Z",
     "iopub.status.busy": "2023-08-10T13:43:04.940593Z",
     "iopub.status.idle": "2023-08-10T13:43:10.183739Z",
     "shell.execute_reply": "2023-08-10T13:43:10.182628Z"
    },
    "papermill": {
     "duration": 5.266341,
     "end_time": "2023-08-10T13:43:10.186232",
     "exception": false,
     "start_time": "2023-08-10T13:43:04.919891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nah is OOV False\n",
      "no is OOV False\n",
      "nope is OOV False\n",
      "lol is OOV False\n",
      "lmao is OOV False\n",
      "yesir is OOV True\n",
      "swim is OOV False\n",
      "swimming is OOV False\n",
      "yes is OOV False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Noted that we are using lg model, because sm model itself does not contain a lot of vocab,\n",
    "# so most of the words when using sm model will be OOV\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "doc = nlp('nah no nope lol lmao yesir swim swimming yes')\n",
    "\n",
    "for token in doc:\n",
    "    print(token, 'is OOV', token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e411dd",
   "metadata": {
    "papermill": {
     "duration": 0.019131,
     "end_time": "2023-08-10T13:43:10.224630",
     "exception": false,
     "start_time": "2023-08-10T13:43:10.205499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Result may be surprising, the large model even contain words like 'lol' and 'lmao' but 'yesir' demonstrate the idea of OOV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0025b3",
   "metadata": {
    "papermill": {
     "duration": 0.019594,
     "end_time": "2023-08-10T13:43:10.263551",
     "exception": false,
     "start_time": "2023-08-10T13:43:10.243957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"part-of-speech-tagging\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Part of Speech (PoS) Tagging</span></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c1744",
   "metadata": {
    "papermill": {
     "duration": 0.01888,
     "end_time": "2023-08-10T13:43:10.301904",
     "exception": false,
     "start_time": "2023-08-10T13:43:10.283024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/A4jz5TR.png\" style=\"height:300px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f5217",
   "metadata": {
    "papermill": {
     "duration": 0.019083,
     "end_time": "2023-08-10T13:43:10.340547",
     "exception": false,
     "start_time": "2023-08-10T13:43:10.321464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Part of Speech is probably something you learnt back in elementary school, it is very important part of a language. You might wonder, why do we bother to do part of speech tagging? Here is an example:\n",
    "\n",
    "Together, they **plant** (verb) a **plant** (noun).\n",
    "\n",
    "Now you may start to appreciate the importance of part of speech tagging in NLP. A lot of times, especially in English Language, the same word will have very different meaning depending on the PoS, by doing PoS tagging, we aim to allow our model to have a **better understanding of the grammar and to be able to distinguish words that have multiple meaning**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d25ce5",
   "metadata": {
    "papermill": {
     "duration": 0.019102,
     "end_time": "2023-08-10T13:43:10.378946",
     "exception": false,
     "start_time": "2023-08-10T13:43:10.359844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### üßë‚Äçüíª Let's code \n",
    "\n",
    "\n",
    "Let's see PoS tagging in action with spaCy and NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4069db8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:10.419413Z",
     "iopub.status.busy": "2023-08-10T13:43:10.419013Z",
     "iopub.status.idle": "2023-08-10T13:43:11.561059Z",
     "shell.execute_reply": "2023-08-10T13:43:11.559374Z"
    },
    "papermill": {
     "duration": 1.1655,
     "end_time": "2023-08-10T13:43:11.563559",
     "exception": false,
     "start_time": "2023-08-10T13:43:10.398059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing: VERB (verb) -> VBG (verb, gerund or present participle)\n",
      "Part: PROPN (proper noun) -> NNP (noun, proper singular)\n",
      "-: PUNCT (punctuation) -> HYPH (punctuation mark, hyphen)\n",
      "of: ADP (adposition) -> IN (conjunction, subordinating or preposition)\n",
      "-: PUNCT (punctuation) -> HYPH (punctuation mark, hyphen)\n",
      "Speech: PROPN (proper noun) -> NNP (noun, proper singular)\n",
      "Tags: PROPN (proper noun) -> NNP (noun, proper singular)\n",
      "with: ADP (adposition) -> IN (conjunction, subordinating or preposition)\n",
      "NLTK: PROPN (proper noun) -> NNP (noun, proper singular)\n",
      "and: CCONJ (coordinating conjunction) -> CC (conjunction, coordinating)\n",
      "Spacy: PROPN (proper noun) -> NNP (noun, proper singular)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"Visualizing Part-of-Speech Tags with NLTK and Spacy\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token}: {token.pos_} ({spacy.explain(token.pos_)}) -> {token.tag_} ({spacy.explain(token.tag_)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f593b581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:11.605289Z",
     "iopub.status.busy": "2023-08-10T13:43:11.604097Z",
     "iopub.status.idle": "2023-08-10T13:43:12.747607Z",
     "shell.execute_reply": "2023-08-10T13:43:12.746570Z"
    },
    "papermill": {
     "duration": 1.16688,
     "end_time": "2023-08-10T13:43:12.749856",
     "exception": false,
     "start_time": "2023-08-10T13:43:11.582976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f9d956e1d146459eb87ff99938999624-0\" class=\"displacy\" width=\"950\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Visualizing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">Part-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">of-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">Speech</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">Tags</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">NLTK</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">Spacy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f9d956e1d146459eb87ff99938999624-0-0\" stroke-width=\"2px\" d=\"M70,152.0 C70,102.0 140.0,102.0 140.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f9d956e1d146459eb87ff99938999624-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M140.0,154.0 L148.0,142.0 132.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f9d956e1d146459eb87ff99938999624-0-1\" stroke-width=\"2px\" d=\"M170,152.0 C170,102.0 240.0,102.0 240.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f9d956e1d146459eb87ff99938999624-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M240.0,154.0 L248.0,142.0 232.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f9d956e1d146459eb87ff99938999624-0-2\" stroke-width=\"2px\" d=\"M370,152.0 C370,102.0 440.0,102.0 440.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f9d956e1d146459eb87ff99938999624-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M370,154.0 L362,142.0 378,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f9d956e1d146459eb87ff99938999624-0-3\" stroke-width=\"2px\" d=\"M270,152.0 C270,52.0 445.0,52.0 445.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f9d956e1d146459eb87ff99938999624-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M445.0,154.0 L453.0,142.0 437.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f9d956e1d146459eb87ff99938999624-0-4\" stroke-width=\"2px\" d=\"M170,152.0 C170,2.0 550.0,2.0 550.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f9d956e1d146459eb87ff99938999624-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550.0,154.0 L558.0,142.0 542.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f9d956e1d146459eb87ff99938999624-0-5\" stroke-width=\"2px\" d=\"M570,152.0 C570,102.0 640.0,102.0 640.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f9d956e1d146459eb87ff99938999624-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M640.0,154.0 L648.0,142.0 632.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f9d956e1d146459eb87ff99938999624-0-6\" stroke-width=\"2px\" d=\"M670,152.0 C670,102.0 740.0,102.0 740.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f9d956e1d146459eb87ff99938999624-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,154.0 L748.0,142.0 732.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f9d956e1d146459eb87ff99938999624-0-7\" stroke-width=\"2px\" d=\"M670,152.0 C670,52.0 845.0,52.0 845.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f9d956e1d146459eb87ff99938999624-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M845.0,154.0 L853.0,142.0 837.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization with displacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Visualizing Part-of-Speech Tags with NLTK and Spacy\")\n",
    "displacy.render(doc, style = \"dep\", options={'distance': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebf2ef70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:12.792482Z",
     "iopub.status.busy": "2023-08-10T13:43:12.792026Z",
     "iopub.status.idle": "2023-08-10T13:43:13.421131Z",
     "shell.execute_reply": "2023-08-10T13:43:13.420037Z"
    },
    "papermill": {
     "duration": 0.653124,
     "end_time": "2023-08-10T13:43:13.423432",
     "exception": false,
     "start_time": "2023-08-10T13:43:12.770308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Visualizing', 'VBG'),\n",
       " ('Part-of-Speech', 'NNP'),\n",
       " ('Tags', 'NNP'),\n",
       " ('with', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Spacy', 'NNP')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PoS Tagging with NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = word_tokenize(\"Visualizing Part-of-Speech Tags with NLTK and Spacy\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a89e984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:13.465958Z",
     "iopub.status.busy": "2023-08-10T13:43:13.465572Z",
     "iopub.status.idle": "2023-08-10T13:43:13.472873Z",
     "shell.execute_reply": "2023-08-10T13:43:13.471896Z"
    },
    "papermill": {
     "duration": 0.031146,
     "end_time": "2023-08-10T13:43:13.474872",
     "exception": false,
     "start_time": "2023-08-10T13:43:13.443726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'NN'), ('Orange', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually assign tagging with NLTK\n",
    "# Tuseful when you want to assign a default or fallback tag to all tokens in a text, \n",
    "# regardless of their actual part-of-speech.\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    " \n",
    "tagging = DefaultTagger('NN')\n",
    "\n",
    "tagging.tag(['Apple', 'Orange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41c18145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:13.517628Z",
     "iopub.status.busy": "2023-08-10T13:43:13.517210Z",
     "iopub.status.idle": "2023-08-10T13:43:13.526790Z",
     "shell.execute_reply": "2023-08-10T13:43:13.525684Z"
    },
    "papermill": {
     "duration": 0.033762,
     "end_time": "2023-08-10T13:43:13.528901",
     "exception": false,
     "start_time": "2023-08-10T13:43:13.495139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Visualizing', 'Part-of-Speech', 'Tags', 'with', 'NLTK', 'and', 'Spacy']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# untagging with NLTK\n",
    "# useful when you want to remove the PoS tags from the tagged tokens \n",
    "# and work with only the plain word-tokens.\n",
    "from nltk.tag import untag\n",
    "text = word_tokenize(\"Visualizing Part-of-Speech Tags with NLTK and Spacy\")\n",
    "tagged_token = nltk.pos_tag(text)\n",
    "untag(tagged_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35e88e",
   "metadata": {
    "papermill": {
     "duration": 0.019943,
     "end_time": "2023-08-10T13:43:13.569553",
     "exception": false,
     "start_time": "2023-08-10T13:43:13.549610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"named-entity-recognition\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üåü Named Entity Recognition (NER) üåü </span></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7ff34",
   "metadata": {
    "papermill": {
     "duration": 0.020079,
     "end_time": "2023-08-10T13:43:13.610073",
     "exception": false,
     "start_time": "2023-08-10T13:43:13.589994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/edHXTu9.png\" style=\"height:300px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72684b",
   "metadata": {
    "papermill": {
     "duration": 0.019763,
     "end_time": "2023-08-10T13:43:13.650535",
     "exception": false,
     "start_time": "2023-08-10T13:43:13.630772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "‚ùó *Named Entity Recognition (NER) is a task of Natural Language Processing (NLP) that involves **identifying and classifying named entities in a text** into predefined categories such as person names, organizations, locations, and others.*\n",
    "\n",
    "NER is particularly useful if you would like to locate specific information from a corpus (text), particularly to **find out 'where', 'what', 'who', 'when'** from a paragraph. \n",
    "\n",
    "NER is being used for wide range of purposes, from recommendation system to search engines and customer support services, it is very useful if you want a quick summary of over a large amount of text. \n",
    "\n",
    "There are 4 method of NER, including \n",
    "1. **dictionary-based** (a large dictionary containing vocabularies eg. {'Apple', 'Tesla'...} to match each token from the text)\n",
    "2. **rule-based** (detect named entities by patterns using techniques such as regular expression)\n",
    "3. **machine learning** (train ML models with text that are being labelled)\n",
    "4. **deep learning based** (train model to detect the entities by understanding the semantic and syntactic relationship between various words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440a134",
   "metadata": {
    "papermill": {
     "duration": 0.020554,
     "end_time": "2023-08-10T13:43:13.691588",
     "exception": false,
     "start_time": "2023-08-10T13:43:13.671034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### üßë‚Äçüíª Let's code \n",
    "\n",
    "Let's see NER in action with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdf3d5d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:13.734386Z",
     "iopub.status.busy": "2023-08-10T13:43:13.733943Z",
     "iopub.status.idle": "2023-08-10T13:43:16.575203Z",
     "shell.execute_reply": "2023-08-10T13:43:16.574078Z"
    },
    "papermill": {
     "duration": 2.865231,
     "end_time": "2023-08-10T13:43:16.577314",
     "exception": false,
     "start_time": "2023-08-10T13:43:13.712083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liverpool | GPE | Countries, cities, states\n",
      "Alexis Mac Allister | PERSON | People, including fictional\n",
      "¬£35m | MONEY | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\"Liverpool confirm the signing of Alexis Mac Allister on cut-price ¬£35m deal\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9922250c",
   "metadata": {
    "papermill": {
     "duration": 0.021042,
     "end_time": "2023-08-10T13:43:16.619139",
     "exception": false,
     "start_time": "2023-08-10T13:43:16.598097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Below is the extracted article, let us see the power of NER with spacy visualization tool displacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab38a39b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T13:43:16.662970Z",
     "iopub.status.busy": "2023-08-10T13:43:16.662576Z",
     "iopub.status.idle": "2023-08-10T13:43:16.731992Z",
     "shell.execute_reply": "2023-08-10T13:43:16.730908Z"
    },
    "papermill": {
     "duration": 0.094197,
     "end_time": "2023-08-10T13:43:16.734416",
     "exception": false,
     "start_time": "2023-08-10T13:43:16.640219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Liverpool\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " have ticked the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " box on their \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    summer\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " rebuilding programme by completing the signing of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Alexis Mac Allister\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".</br></br>\n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Argentine World Cup\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " winner, who passed a medical in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Merseyside\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tuesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", has agreed terms on a deal that will run to \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    June 2028\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " and an idea of the importance that will be placed on his arrival is the fact he will be given \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Liverpool\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "'s vacant No \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    10\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " shirt.</br></br>It is a significant number in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Liverpool\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "'s history, as it was worn by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John Barnes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and, more recently, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sadio Mane\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " but it is also significant for \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mac Allister\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to, given the symbolism of the number in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Argentina\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " with \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Diego Maradona\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lionel Messi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".</br></br>\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Liverpool\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " have been tracking \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mac Allister\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " since before \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    last winter's\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Cup\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       ". The club have offered no guidance on how much they have paid for the \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    24-year-old\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " nor are \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brighton\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " but \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mail Sport\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " understands the fee is in the region of ¬£\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    35million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</br></br>It had initially been expected that it would take \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    more than ¬£60million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Liverpool\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " to secure \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mac Allister's\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " release, so this investment ‚Äì on face value ‚Äì makes huge economic sense: it is up to the player to see if the figure become a bargain.</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\"\"\"\n",
    "Liverpool have ticked the first box on their summer rebuilding programme by completing the signing of Alexis Mac Allister.\n",
    "\n",
    "The Argentine World Cup winner, who passed a medical in Merseyside on Tuesday, has agreed terms on a deal that will run to June 2028 and an idea of the importance that will be placed on his arrival is the fact he will be given Liverpool's vacant No 10 shirt.\n",
    "\n",
    "It is a significant number in Liverpool's history, as it was worn by John Barnes and, more recently, Sadio Mane but it is also significant for Mac Allister to, given the symbolism of the number in Argentina with Diego Maradona and Lionel Messi.\n",
    "\n",
    "Liverpool have been tracking Mac Allister since before last winter's World Cup. The club have offered no guidance on how much they have paid for the 24-year-old nor are Brighton but Mail Sport understands the fee is in the region of ¬£35million.\n",
    "\n",
    "It had initially been expected that it would take more than ¬£60million for Liverpool to secure Mac Allister's release, so this investment ‚Äì on face value ‚Äì makes huge economic sense: it is up to the player to see if the figure become a bargain.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eee6228",
   "metadata": {
    "papermill": {
     "duration": 0.020932,
     "end_time": "2023-08-10T13:43:16.776056",
     "exception": false,
     "start_time": "2023-08-10T13:43:16.755124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see, NER allow you to have a quick understanding of large amount of text by labelling named entities. In fact, named entities usually carry most of the information from a text as compared to stop words like 'is', 'the', 'and'. But of course, the use of NER is not limited to just a quick summary as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618421a1",
   "metadata": {
    "papermill": {
     "duration": 0.021062,
     "end_time": "2023-08-10T13:43:16.817912",
     "exception": false,
     "start_time": "2023-08-10T13:43:16.796850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"summary-1\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üìù Short Summary (Part 1: Preprocessing) üìù</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359f0d7",
   "metadata": {
    "papermill": {
     "duration": 0.020482,
     "end_time": "2023-08-10T13:43:16.861431",
     "exception": false,
     "start_time": "2023-08-10T13:43:16.840949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's review what we have learnt so far. \n",
    "\n",
    "### 1. [Tokenization](#tokenization)\n",
    "\n",
    "Process of converting sentences into 'tokens' to assign meanings\n",
    "\n",
    "### 2. [spaCy pipeline](#tokenization)\n",
    "\n",
    "Introduced spaCy trained pipeline and architecture\n",
    "\n",
    "### 3. [Stop word](#stop-word)\n",
    "\n",
    "Set of commonly used words, sometimes we would like to remove them to reduce noise\n",
    "\n",
    "\n",
    "### 4. [Stemming and Lemmatization](#stemming-and-lemmatization)\n",
    "\n",
    "Process of breaking down tokens into their root/base form\n",
    "\n",
    "### 5. [Out of Vocabulary (OOV)](#out-of-vocabulary)\n",
    "\n",
    "Vocabularies that is not covered/trained by the NLP model\n",
    "\n",
    "### 6. [Part of Speech (PoS) Tagging](#part-of-speech-tagging)\n",
    "\n",
    "Assigning part of speech to tokens to allow model to classify words of multiple meaning\n",
    "\n",
    "### 7. [Named Entity Recognition (NER)](#named-entity-recognition)\n",
    "\n",
    "Technique to identify named entity in a corpus (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb057c0",
   "metadata": {
    "papermill": {
     "duration": 0.020188,
     "end_time": "2023-08-10T13:43:16.902710",
     "exception": false,
     "start_time": "2023-08-10T13:43:16.882522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"practice-1\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">‚úç Do Some Practice and Take a Break ‚úç</span></div>\n",
    "\n",
    "Before we move to the next section (feature engineering), it is important that you feel comfortable with the concepts we have mentioned so far. Preprocessing is a prior step to feature engineering, these concepts will be brough up in later sections when necessary. \n",
    "\n",
    "The best way to achieve this is through some practice, try to preprocess the competition dataset with the techniques that you have just learnt! \n",
    "\n",
    "Once you are ready, let's move on to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f3d3d",
   "metadata": {
    "papermill": {
     "duration": 0.020691,
     "end_time": "2023-08-10T13:43:16.944737",
     "exception": false,
     "start_time": "2023-08-10T13:43:16.924046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Continue on: **üü° NLP Beginner Series Part 2.1:  Word Embeddings** \n",
    "https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-2-1-word-embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e62cf",
   "metadata": {
    "papermill": {
     "duration": 0.020144,
     "end_time": "2023-08-10T13:43:16.985852",
     "exception": false,
     "start_time": "2023-08-10T13:43:16.965708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"hey-there\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üëã Hey there üëã</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a1cf7b",
   "metadata": {
    "papermill": {
     "duration": 0.02032,
     "end_time": "2023-08-10T13:43:17.027231",
     "exception": false,
     "start_time": "2023-08-10T13:43:17.006911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### üåü About the Author\n",
    "\n",
    "Hey there, I am an undergraduate majoring in quantitative finance at the Univeristy of Hong Kong. I am a highly motivated person with a huge passion in mathematics and computer science related topics. \n",
    "\n",
    "When I said I am a beginner in the start of the notebook, I am not lying! In fact, I am a self learner in the field of machine learning and natural language processing, and as the moment I am writing this, I have only started my machine learning journey for a couple of months. \n",
    "\n",
    "Although I do not have a lot of experience and knowlegde in the field, I am more than happy to connect and learn/work together in some projects and stuff like that.\n",
    "\n",
    "Do you know Kaggle got discord server? https://discord.gg/kaggle (crxxom) <br>\n",
    "My Linkedin: https://www.linkedin.com/in/jadon-ng-848a48263/\n",
    "\n",
    "### üåü Upvote the notebook!\n",
    "\n",
    "If you find the notebook useful, it will be great if you can show some support by upvoting the notebook, it means a lot to me! \n",
    "\n",
    "Also, if you want to make suggestions/corrections to the content of this notebook, don't hesitate to comment your thoughts, I will reply you as soon as possible.\n",
    "\n",
    "One more thing, if you want to share some of your resources/notebooks, comment down your links in the comment section and let us learn! \n",
    "\n",
    "\n",
    "### üåü Will there be updates?\n",
    "\n",
    "As in for now, I will only be updating the notebook if there are some incorrect information or if I discover some useful resources to share. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 50.69866,
   "end_time": "2023-08-10T13:43:20.620566",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-10T13:42:29.921906",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
