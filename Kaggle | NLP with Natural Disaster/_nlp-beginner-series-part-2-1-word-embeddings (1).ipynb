{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db81b941",
   "metadata": {
    "papermill": {
     "duration": 0.016715,
     "end_time": "2023-08-11T09:38:08.659343",
     "exception": false,
     "start_time": "2023-08-11T09:38:08.642628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">❗ Important ❗ </span></div>\n",
    "\n",
    "Hey there, whether you come from the ✅ Comprehensive Overview on NLP for Beginners 🥳  notebook or you click on this notebook directly, I just want to let you know that this notebook is part 2.1 to the NLP beginner series. Here are the links to the whole series:\n",
    "    \n",
    "**✅ Comprehensive Overview on NLP for Beginners 🥳 (collection of all series)** <br>\n",
    " https://www.kaggle.com/code/crxxom/comprehensive-overview-on-nlp-for-beginners\n",
    " \n",
    "    \n",
    "**🔴 NLP Beginner Series Part 1: NLP Preprocessing** <br>\n",
    "https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-1-nlp-preprocessing\n",
    "\n",
    "**🟡 NLP Beginner Series Part 2.1: Word Embeddings** <br>\n",
    "https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-2-1-word-embeddings\n",
    "\n",
    "**🟢 NLP Beginner Series Part 2.2: Embedding Models** <br>\n",
    "https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-2-2-embedding-models\n",
    "\n",
    "**🟣 NLP Beginner Series Part 3: Case Study** <br>\n",
    " https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-3-case-study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887fd0c",
   "metadata": {
    "papermill": {
     "duration": 0.01615,
     "end_time": "2023-08-11T09:38:08.691766",
     "exception": false,
     "start_time": "2023-08-11T09:38:08.675616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">🔥  Overview of the Notebook  🔥</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba9882",
   "metadata": {
    "papermill": {
     "duration": 0.015775,
     "end_time": "2023-08-11T09:38:08.723765",
     "exception": false,
     "start_time": "2023-08-11T09:38:08.707990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## >> Important concepts and techniques you need to know to get you started with NLP explained in a **➡ beginner friendly way ⬅**\n",
    "\n",
    "## >> **ONLY** pre-requisite you need: Python, Pandas and Numpy\n",
    "\n",
    "## >> 🕑 Take your Time! (~ 1 hour)\n",
    "\n",
    "## >> Is this notebook for me 🤔\n",
    "\n",
    "This notebook **covers the absolute fundamentals of NLP to let you have an overview with what exactly is NLP and how it work.** \n",
    "\n",
    "### 🌟 Skim through the notebook\n",
    "\n",
    "Quickly scroll through the whole notebook to see if the style suit your learning needs!\n",
    "\n",
    "### 🌟 This notebook is beginner friendly\n",
    "This notebook is very much a **beginner friendly notebook**, the only pre-requisite you will need is Python and some common data science frameworks such as Pandas and Numpy. All the concepts covered in this notebook will be clearly explained but may not be deeply explained, you will be encouraged to do additional research on your own if you would like to understand more about a particular topic. The goal of this notebook is to give you an overview to NLP and introduce you with key vocabularies and concepts you will need to know in NLP.\n",
    "\n",
    "\n",
    "### 🌟 Seperate sections to suit your needs\n",
    "**Each section of the notebook is seperated**, ie. you do NOT need to run the code on section 1 to be able to run the code in section 2, libraries and framework will be introduced and imported in each individual section, dataset are not interrelatble across sections. If you are only interested in any particular section, **you can skip through the sections to run the code in that particular section only**. In fact, I try to make each codeblock as seperated as possible so that you do not need to run codeblock A to be able to run B.\n",
    "\n",
    "\n",
    "### 🌟 Images to help you learn\n",
    "This notebook contains **a lot of images** to demonstrate the concepts and principles as I find it to be the best way to understand complicated concepts. These images are mostly images from google and screenshots from youtube tutorials, if any parties find it inappropriate to use their images/screenshots, please let me know I will delete is as soon as possible.\n",
    "\n",
    "### 🌟 I am a beginner also\n",
    "This notebook is **written from the perspective of a beginner** to the world of machine learning. With that being said, there may be some concepts that isn't covered clearly/deeply, but on the other hand, I feel like it will be easier to catch up if you are a beginner as well, as I will not be throwing some complicated jargons and overload you too much since I am a beginner as well. Additionally I have also included a lot of resources if you want to have a deeper insight for each topics.\n",
    "\n",
    "\n",
    "### 🌟 Notebook designed in a way to not overwhelm you\n",
    "**Take your time!** Small and lengthy words are hard and dull to read, that's why this notebook trys to **minimize the amount of text and use bold text, images and emojis** so that you can navigate the notebook faster. The notebook also attempts to **minimize the amount of code** as much as possible, to save you from being too overwhelmed. \n",
    "\n",
    "**Try to run the codeblocks youself**, running and changing codes in the codeblocks can give you a more immersive learning experience. \n",
    "\n",
    "If you appreciate my work or would like to express your views/make suggestions on some of my code and content, feel free to comment and correct me, I will make sure to keep updating and make improvements to this notebook! I hope you will find this notebook useful and an enjoyable journey.\n",
    "\n",
    "## 🔎 Concepts and techniques you can take away with you from this notebook\n",
    "\n",
    "#### 🟢 [Bag of Words (BOW)](#bag-of-words)\n",
    "\n",
    "#### 🟢 [Bag of n-grams](#bag-of-n-grams)\n",
    "\n",
    "#### 🟢 [Term Frequency - Inverse Document Frequency (TF-IDF)](#tf-idf)\n",
    "\n",
    "#### 🟢 [Word Embeddings](#word-embeddings)\n",
    "\n",
    "#### 🟢 [Cosine Similarity](#cosine-similarity)\n",
    "\n",
    "#### 🟢 [Arithmetic of Word Embeddings](#arithmetic-of-word-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfa47d",
   "metadata": {
    "papermill": {
     "duration": 0.015776,
     "end_time": "2023-08-11T09:38:08.755579",
     "exception": false,
     "start_time": "2023-08-11T09:38:08.739803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"bag-of-words\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Bag of Words (BOW)</span></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611670ff",
   "metadata": {
    "papermill": {
     "duration": 0.015647,
     "end_time": "2023-08-11T09:38:08.787238",
     "exception": false,
     "start_time": "2023-08-11T09:38:08.771591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/iiIOKHx.png\" style=\"height:200px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2055523",
   "metadata": {
    "papermill": {
     "duration": 0.016117,
     "end_time": "2023-08-11T09:38:08.819181",
     "exception": false,
     "start_time": "2023-08-11T09:38:08.803064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "❗ *The bag-of-words model is a simplifying representation used in natural language processing and information retrieval. In this model, **a text is represented as the bag of its words**, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.*\n",
    "\n",
    "### Computer doesn't understandand string/words,\n",
    "that's why we need certain algorithm and method to convert words into numbers that computer understand. One of these techniques is Bag of Word (BOW). \n",
    "\n",
    "The idea behind Bag of Word is fairly simple. Basically we would like to **get all unique tokens in all the data you feed in as a column**. For each of the sentence, we assign a counter for each word/column, just like what the above image illustrate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f730931",
   "metadata": {
    "papermill": {
     "duration": 0.015715,
     "end_time": "2023-08-11T09:38:08.850940",
     "exception": false,
     "start_time": "2023-08-11T09:38:08.835225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 🧑‍💻 Let's code \n",
    "\n",
    "There are certain limitations associated with BOW, but first let us see BOW in action with sklearn, a popular machine learning framework in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb977f1",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:08.884801Z",
     "iopub.status.busy": "2023-08-11T09:38:08.884052Z",
     "iopub.status.idle": "2023-08-11T09:38:08.895668Z",
     "shell.execute_reply": "2023-08-11T09:38:08.894705Z"
    },
    "papermill": {
     "duration": 0.03121,
     "end_time": "2023-08-11T09:38:08.898008",
     "exception": false,
     "start_time": "2023-08-11T09:38:08.866798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter out the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c57f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:08.934344Z",
     "iopub.status.busy": "2023-08-11T09:38:08.933565Z",
     "iopub.status.idle": "2023-08-11T09:38:08.958220Z",
     "shell.execute_reply": "2023-08-11T09:38:08.957405Z"
    },
    "papermill": {
     "duration": 0.046486,
     "end_time": "2023-08-11T09:38:08.960576",
     "exception": false,
     "start_time": "2023-08-11T09:38:08.914090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey there, how's the weather today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liverpool announced the signing of Szoboszlai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An apple a day keeps the doctor away.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Natural Language Processing is fun!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Text\n",
       "0            Hey there, how's the weather today.\n",
       "1  Liverpool announced the signing of Szoboszlai\n",
       "2          An apple a day keeps the doctor away.\n",
       "3            Natural Language Processing is fun!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare a very small dataset, you can change the text to test the difference!\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\"Hey there, how's the weather today.\",\n",
    "          \"Liverpool announced the signing of Szoboszlai\",\n",
    "          \"An apple a day keeps the doctor away.\",\n",
    "          \"Natural Language Processing is fun!\"]\n",
    "\n",
    "text_df = pd.DataFrame({\"Text\": corpus})\n",
    "\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e482ce25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:08.995411Z",
     "iopub.status.busy": "2023-08-11T09:38:08.994996Z",
     "iopub.status.idle": "2023-08-11T09:38:25.165384Z",
     "shell.execute_reply": "2023-08-11T09:38:25.164333Z"
    },
    "papermill": {
     "duration": 16.191353,
     "end_time": "2023-08-11T09:38:25.168098",
     "exception": false,
     "start_time": "2023-08-11T09:38:08.976745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text preprocessing, we will be \n",
    "# 1. removing stop words\n",
    "# 2. removing punctuations\n",
    "# 3. changing word to lower case\n",
    "# 4. lemmatization\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocessing(text):\n",
    "    doc = nlp(text.lower()) # lowercase\n",
    "    preprocessed_text = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct: # if the token is stop word/punctuation we do not add it to the list\n",
    "            continue\n",
    "        preprocessed_text.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb35eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:25.203747Z",
     "iopub.status.busy": "2023-08-11T09:38:25.202920Z",
     "iopub.status.idle": "2023-08-11T09:38:25.264606Z",
     "shell.execute_reply": "2023-08-11T09:38:25.263411Z"
    },
    "papermill": {
     "duration": 0.082057,
     "end_time": "2023-08-11T09:38:25.267101",
     "exception": false,
     "start_time": "2023-08-11T09:38:25.185044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hey weather today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>liverpool announce signing szoboszlai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple day keep doctor away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>natural language processing fun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Text\n",
       "0                      hey weather today\n",
       "1  liverpool announce signing szoboszlai\n",
       "2             apple day keep doctor away\n",
       "3        natural language processing fun"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform our dataset through applying our preprocessing function to the text_df\n",
    "\n",
    "text_df[\"Text\"] = text_df[\"Text\"].apply(preprocessing)\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a205e103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:25.302004Z",
     "iopub.status.busy": "2023-08-11T09:38:25.301599Z",
     "iopub.status.idle": "2023-08-11T09:38:26.127792Z",
     "shell.execute_reply": "2023-08-11T09:38:26.126085Z"
    },
    "papermill": {
     "duration": 0.847079,
     "end_time": "2023-08-11T09:38:26.130701",
     "exception": false,
     "start_time": "2023-08-11T09:38:25.283622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hey': 6,\n",
       " 'weather': 15,\n",
       " 'today': 14,\n",
       " 'liverpool': 9,\n",
       " 'announce': 0,\n",
       " 'signing': 12,\n",
       " 'szoboszlai': 13,\n",
       " 'apple': 1,\n",
       " 'day': 3,\n",
       " 'keep': 7,\n",
       " 'doctor': 4,\n",
       " 'away': 2,\n",
       " 'natural': 10,\n",
       " 'language': 8,\n",
       " 'processing': 11,\n",
       " 'fun': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's do our BOW \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(text_df['Text'])\n",
    "cv.vocabulary_ # Each unique token will be assigned as a dictionary with some kind of index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc6da6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:26.166007Z",
     "iopub.status.busy": "2023-08-11T09:38:26.165613Z",
     "iopub.status.idle": "2023-08-11T09:38:26.172723Z",
     "shell.execute_reply": "2023-08-11T09:38:26.171648Z"
    },
    "papermill": {
     "duration": 0.027508,
     "end_time": "2023-08-11T09:38:26.175083",
     "exception": false,
     "start_time": "2023-08-11T09:38:26.147575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['announce', 'apple', 'away', 'day', 'doctor', 'fun', 'hey', 'keep',\n",
       "       'language', 'liverpool', 'natural', 'processing', 'signing',\n",
       "       'szoboszlai', 'today', 'weather'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out() # we can also convert the vocabs to an array of the right order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fa7d24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:26.210814Z",
     "iopub.status.busy": "2023-08-11T09:38:26.210414Z",
     "iopub.status.idle": "2023-08-11T09:38:26.220642Z",
     "shell.execute_reply": "2023-08-11T09:38:26.219353Z"
    },
    "papermill": {
     "duration": 0.031239,
     "end_time": "2023-08-11T09:38:26.223258",
     "exception": false,
     "start_time": "2023-08-11T09:38:26.192019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the vector of a particular corpus, we use the .transform function and turn the result to an array\n",
    "\n",
    "cv.transform(['hey weather today']).toarray()\n",
    "\n",
    "# You can see from the results, there are three position that shows '1'\n",
    "# which is array[6],array[13],array[14] -> matching the position of the word in cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52fd9276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:26.260053Z",
     "iopub.status.busy": "2023-08-11T09:38:26.258929Z",
     "iopub.status.idle": "2023-08-11T09:38:26.279270Z",
     "shell.execute_reply": "2023-08-11T09:38:26.278200Z"
    },
    "papermill": {
     "duration": 0.041057,
     "end_time": "2023-08-11T09:38:26.281643",
     "exception": false,
     "start_time": "2023-08-11T09:38:26.240586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>announce</th>\n",
       "      <th>apple</th>\n",
       "      <th>away</th>\n",
       "      <th>day</th>\n",
       "      <th>doctor</th>\n",
       "      <th>fun</th>\n",
       "      <th>hey</th>\n",
       "      <th>keep</th>\n",
       "      <th>language</th>\n",
       "      <th>liverpool</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "      <th>signing</th>\n",
       "      <th>szoboszlai</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hey weather today</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liverpool announce signing szoboszlai</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple day keep doctor away</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natural language processing fun</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       announce  apple  away  day  doctor  \\\n",
       "Text                                                                        \n",
       "hey weather today                             0      0     0    0       0   \n",
       "liverpool announce signing szoboszlai         1      0     0    0       0   \n",
       "apple day keep doctor away                    0      1     1    1       1   \n",
       "natural language processing fun               0      0     0    0       0   \n",
       "\n",
       "                                       fun  hey  keep  language  liverpool  \\\n",
       "Text                                                                         \n",
       "hey weather today                        0    1     0         0          0   \n",
       "liverpool announce signing szoboszlai    0    0     0         0          1   \n",
       "apple day keep doctor away               0    0     1         0          0   \n",
       "natural language processing fun          1    0     0         1          0   \n",
       "\n",
       "                                       natural  processing  signing  \\\n",
       "Text                                                                  \n",
       "hey weather today                            0           0        0   \n",
       "liverpool announce signing szoboszlai        0           0        1   \n",
       "apple day keep doctor away                   0           0        0   \n",
       "natural language processing fun              1           1        0   \n",
       "\n",
       "                                       szoboszlai  today  weather  \n",
       "Text                                                               \n",
       "hey weather today                               0      1        1  \n",
       "liverpool announce signing szoboszlai           1      0        0  \n",
       "apple day keep doctor away                      0      0        0  \n",
       "natural language processing fun                 0      0        0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's visualize the result of countvectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(text_df['Text'])\n",
    "\n",
    "feature_names = cv.get_feature_names_out() # get all the unique tokens \n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=feature_names, index=text_df['Text']) # create a pandas dataframe\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2195532f",
   "metadata": {
    "papermill": {
     "duration": 0.017007,
     "end_time": "2023-08-11T09:38:26.316143",
     "exception": false,
     "start_time": "2023-08-11T09:38:26.299136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now I hope you grasp the general idea of how BOW work. Indeed is it pretty simply process and you can use this result dataframe to train your machine learning models for your NLP task. But in reality, there are several huge flaws in using BOW to vectorize the corpus. For instance, it faces the problem of\n",
    "\n",
    "### 1. **Sparsity** increase as the number of unique token (size of input) increases\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/EdPwBdK.png\" style=\"height:200px;\"> </div>\n",
    "<br>\n",
    "\n",
    "If you notice, in the example demonstrate above, there are a lot of zeros as the value of each corpus, **the percentage of useful information of each row will decrease as the size of input increase**. Why? Image rather than 4 corpus, we have a million sentences, just image how many zeros will there be in each row!\n",
    "\n",
    "### 2. **Dimension** increase as the number of unique token (size of input) increases\n",
    "\n",
    "Apart from the problem of sparsity, dimensionality is also a primary concern of BOW. Just like the example mentioned, if we have a million sentences, there will be millions of features/columns in the dataset which make our training process very inefficient.\n",
    "\n",
    "### 3. BOW cannot handle **Out of Vocabulary (OOV)** problem\n",
    "\n",
    "Imagine we use our example dataframe to train our model and now we want to classifer the sentence \"Machine Learning is awesome\". Our model simply cannot give us an accurate prediction because none of these tokens are in our feature!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d4c7fd",
   "metadata": {
    "papermill": {
     "duration": 0.016877,
     "end_time": "2023-08-11T09:38:26.350953",
     "exception": false,
     "start_time": "2023-08-11T09:38:26.334076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"bag-of-n-grams\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Bag of n-grams</span></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3b7d7",
   "metadata": {
    "papermill": {
     "duration": 0.016979,
     "end_time": "2023-08-11T09:38:26.385259",
     "exception": false,
     "start_time": "2023-08-11T09:38:26.368280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/QLE08hJ.png\" style=\"height:300px;\"> </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b4058",
   "metadata": {
    "papermill": {
     "duration": 0.017053,
     "end_time": "2023-08-11T09:38:26.419657",
     "exception": false,
     "start_time": "2023-08-11T09:38:26.402604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Just at looking at the imagine above, you may already have a good guess of what bag of n-grams do if you had go through the BOW section. Indeed, bag of n-grams simple increase the number of features/columns by altering between how to split the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13262d3",
   "metadata": {
    "papermill": {
     "duration": 0.017017,
     "end_time": "2023-08-11T09:38:26.453907",
     "exception": false,
     "start_time": "2023-08-11T09:38:26.436890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 🧑‍💻 Let's code \n",
    "\n",
    "Let's see bag of n-grams in action with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c9b87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:26.495543Z",
     "iopub.status.busy": "2023-08-11T09:38:26.495072Z",
     "iopub.status.idle": "2023-08-11T09:38:27.786498Z",
     "shell.execute_reply": "2023-08-11T09:38:27.785235Z"
    },
    "papermill": {
     "duration": 1.315807,
     "end_time": "2023-08-11T09:38:27.788957",
     "exception": false,
     "start_time": "2023-08-11T09:38:26.473150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hey weather today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>liverpool announce signing szoboszlai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple day keep doctor away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>natural language processing fun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Text\n",
       "0                      hey weather today\n",
       "1  liverpool announce signing szoboszlai\n",
       "2             apple day keep doctor away\n",
       "3        natural language processing fun"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will be all the dataset from the previous section, so these are just copy and paste\n",
    "\n",
    "# Dataset from BOW section\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\"Hey there, how's the weather today.\",\n",
    "          \"Liverpool announced the signing of Szoboszlai\",\n",
    "          \"An apple a day keeps the doctor away.\",\n",
    "          \"Natural Language Processing is fun!\"]\n",
    "\n",
    "text_df = pd.DataFrame({\"Text\": corpus})\n",
    "\n",
    "# Preprocess function from BOW section\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocessing(text):\n",
    "    doc = nlp(text.lower()) # lowercase\n",
    "    preprocessed_text = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct: # if the token is stop word/punctuation we do not add it to the list\n",
    "            continue\n",
    "        preprocessed_text.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(preprocessed_text)\n",
    "\n",
    "text_df[\"Text\"] = text_df[\"Text\"].apply(preprocessing)\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79422e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:27.826524Z",
     "iopub.status.busy": "2023-08-11T09:38:27.825800Z",
     "iopub.status.idle": "2023-08-11T09:38:27.846411Z",
     "shell.execute_reply": "2023-08-11T09:38:27.845233Z"
    },
    "papermill": {
     "duration": 0.04251,
     "end_time": "2023-08-11T09:38:27.849017",
     "exception": false,
     "start_time": "2023-08-11T09:38:27.806507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Bag of n-grams</h3><div id=\"Bag of n-grams\" style=\"height:200px; overflow:auto;\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>announce</th>\n",
       "      <th>announce signing</th>\n",
       "      <th>apple</th>\n",
       "      <th>apple day</th>\n",
       "      <th>away</th>\n",
       "      <th>day</th>\n",
       "      <th>day keep</th>\n",
       "      <th>doctor</th>\n",
       "      <th>doctor away</th>\n",
       "      <th>fun</th>\n",
       "      <th>hey</th>\n",
       "      <th>hey weather</th>\n",
       "      <th>keep</th>\n",
       "      <th>keep doctor</th>\n",
       "      <th>language</th>\n",
       "      <th>language processing</th>\n",
       "      <th>liverpool</th>\n",
       "      <th>liverpool announce</th>\n",
       "      <th>natural</th>\n",
       "      <th>natural language</th>\n",
       "      <th>processing</th>\n",
       "      <th>processing fun</th>\n",
       "      <th>signing</th>\n",
       "      <th>signing szoboszlai</th>\n",
       "      <th>szoboszlai</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "      <th>weather today</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hey weather today</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liverpool announce signing szoboszlai</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple day keep doctor away</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natural language processing fun</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,2)) \n",
    "# to declare bag of n_grams, we simply change the parameter\n",
    "# in this example we will be using bag of one word and bag of two word\n",
    "# noted that the parameter is in form of range, so (1,3) will be bag of 1,2,3 words\n",
    "\n",
    "\n",
    "# Same code from previous section\n",
    "X = cv.fit_transform(text_df['Text'])\n",
    "\n",
    "feature_names = cv.get_feature_names_out() # get all the unique tokens \n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=feature_names, index=text_df['Text']) # create a pandas dataframe\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# just a function that better display large pandas dataframe\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def create_scrollable_table(df, table_id, title):\n",
    "    html = f'<h3>{title}</h3>'\n",
    "    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n",
    "    html += df.to_html()\n",
    "    html += '</div>'\n",
    "    return html\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "html_df = create_scrollable_table(df, \n",
    "                            'Bag of n-grams', \n",
    "                            'Bag of n-grams')\n",
    "display(HTML(html_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf001d",
   "metadata": {
    "papermill": {
     "duration": 0.019159,
     "end_time": "2023-08-11T09:38:27.887363",
     "exception": false,
     "start_time": "2023-08-11T09:38:27.868204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The difference between Bag of n-grams and Bag of Words is that Bag of n-grams contains more information than BOW because **bag of n-grams capture more context around each word than BOW**. For example, instead of just capturing 'hey', 'weather' and 'today', bag of n-gram captures 'hey', 'hey weather', 'weather', 'today', 'weather today'.\n",
    "\n",
    "But also at the same time, the flaws of sparsity and dimensionality as mentioned in the BOW section is also being demonstrated here. While bag of n-grams also cannot deal with Out of Vocabulary (OOV) same as bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f3ff48",
   "metadata": {
    "papermill": {
     "duration": 0.018324,
     "end_time": "2023-08-11T09:38:27.925176",
     "exception": false,
     "start_time": "2023-08-11T09:38:27.906852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"tf-idf\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">🌟 Term Frequency - Inverse Document Frequency (TF-IDF)🌟 </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92b159",
   "metadata": {
    "papermill": {
     "duration": 0.01827,
     "end_time": "2023-08-11T09:38:27.961523",
     "exception": false,
     "start_time": "2023-08-11T09:38:27.943253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/4WcIN8I.png\" style=\"height:300px;\"> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cfbc8",
   "metadata": {
    "papermill": {
     "duration": 0.018142,
     "end_time": "2023-08-11T09:38:27.997769",
     "exception": false,
     "start_time": "2023-08-11T09:38:27.979627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When you are navigating through the sections of [Bag of Words](#bag-of-words) and [Bag of n-grams](#bag-of-n-grams), have you notice the fact that they both assign a weight of 1 to each appearance of word in the sentence? This might not make a big difference for our tiny dataset from our examples, but for larger dataset, it may be a problem because imagine words like 'is' will have a value of 100 and important words like 'Tesla' that contains the most information will have a value of 10. When we feed this dataset to our model to train, this will in turn **give words like 'is' a relatively high weighting and reduce the accuracy of the model** as you can imagine words like 'is' will have a very high value also for other sentences.\n",
    "\n",
    "You may suggest that we could simply remove these [stop words](#stop-word) in our preprocessing part, but sometimes we do not want to do it and in fact, in this section I will show you a better technique to tackle this problem -- TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add0b302",
   "metadata": {
    "papermill": {
     "duration": 0.018377,
     "end_time": "2023-08-11T09:38:28.035810",
     "exception": false,
     "start_time": "2023-08-11T09:38:28.017433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "❗ *In information retrieval, tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to **reflect how important a word is to a document in a collection or corpus**. It is often **used as a weighting factor** in searches of information retrieval, text mining, and user modeling.*\n",
    "\n",
    "When talking about TF-IDF, we have to introduce some math and statistical concept to better illustrate the concept. But don't get scared, the maths in TF-IDF is relatively easy so stay with me.\n",
    "\n",
    "Let's break TF-IDF into two parts, term frequency (TF) and inverse document frequency (IDF).\n",
    "\n",
    "## Term Frequency (TF)\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/RhnP5xc.png\" style=\"height:120px;\"> </div>\n",
    "\n",
    "This look a little overwhelming, to put the mathematical equation in words,\n",
    "\n",
    "`Term Frequency(TF) = number of times the word appeared / total no of words in a document`\n",
    "\n",
    "### ✍ Example: \n",
    "\n",
    "The best way to learn is always through examples, so let's see an example with this sentence\n",
    "\n",
    ">> **\"Natural Language Processing is fun Machine Learning is fun\"**\n",
    "\n",
    "❓ Question: Find the term frequency of the word *'is'*\n",
    "\n",
    "✅ Total number of word in the document: 9 <br>\n",
    "✅ number of times *is* appeared in the text: 2 <br>\n",
    "🟰 TF(is): 2/9 \n",
    "\n",
    "Similarly, TF(Machine) is what, yes your right, 1/9.\n",
    "\n",
    "🌟 If you think deeper, the range of possible value of TF is 0 to 1, when TF is closer to 1, it means that the word appear more frequently in the text.\n",
    "\n",
    "Now let's move on to Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7002d",
   "metadata": {
    "papermill": {
     "duration": 0.017637,
     "end_time": "2023-08-11T09:38:28.071725",
     "exception": false,
     "start_time": "2023-08-11T09:38:28.054088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inverse Document Frequency (IDF)\n",
    "\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/AvZkDVx.png\" style=\"height:150px;\"> </div>\n",
    "\n",
    "<br>\n",
    "\n",
    "To put it in words,\n",
    "\n",
    "`Inverse Document Frequency(IDF) = log(Total number of documents (rows) / Number of documents (rows) containing the word)`\n",
    "\n",
    "### ✍ Example: \n",
    "\n",
    "Let's see IDF in action with an example, imagine we have 2 documents (rows) \n",
    "\n",
    ">> **1. \"Natural Language Processing is fun Machine Learning is fun\"** <br>\n",
    ">> **2. \"This is the best thing I ever had\"**\n",
    "\n",
    "❓ Question: Find the inverse document frequency of the word *'is'*\n",
    "\n",
    "✅ Total number of documents: 2 <br>\n",
    "✅ Number of documents containing the word *is*: 2 <br>\n",
    "🟰 IDF(is): log(2/2) = 0\n",
    "\n",
    "Similarly, IDF(Machine) will be log(2/1) which is around 0.7 (note: we usually like to use the natural logrithm log base of e in machine learning and statistics)\n",
    "\n",
    "🌟 If you think deeper, the range of value of IDF is 0 to infinity, but really, if you notice from the example above, what we are doing is to give a smaller weighting to words that appear more frequently across all the documents. Now you see why this is the perfect replacement to simply removing all the stop words, we are achieving similar outcome in a more statistical and robust approach!\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/SBwxNWb.png\" style=\"height:300px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e71d4",
   "metadata": {
    "papermill": {
     "duration": 0.017674,
     "end_time": "2023-08-11T09:38:28.107466",
     "exception": false,
     "start_time": "2023-08-11T09:38:28.089792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/7RDG6tk.png\" style=\"height:200px;\"> </div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Now that you understand the concept of TF and IDF, TF-IDF is relatively simple idea. It is basically \n",
    "\n",
    "`Term frequency (TF) x Inverse document frequency (IDF)`\n",
    "\n",
    "Let's try to understand this intuitively. \n",
    "\n",
    "🌟 TF-IDF = TF x IDF\n",
    "> As TF increase, TF-IDF increase ➡ **If a word appear in a document more frequently, we give the word a higher weight and consider the word as 'more informative and important'** \n",
    "<br>\n",
    "<br>\n",
    "> As IDF decrease, TF-IDF decrease ➡ **If a word appear ACROSS documents more frequently, we give the word a lower weight and consider the word as a sort of 'stop word' and less informative to each individual document**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9644c3c",
   "metadata": {
    "papermill": {
     "duration": 0.017575,
     "end_time": "2023-08-11T09:38:28.142869",
     "exception": false,
     "start_time": "2023-08-11T09:38:28.125294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's see TF-IDF in action with sklearn. I will be using the same corpus as the corpus we used in Bag of Words section to allow us to see the difference and effect in using TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8154e9d6",
   "metadata": {
    "papermill": {
     "duration": 0.017611,
     "end_time": "2023-08-11T09:38:28.178374",
     "exception": false,
     "start_time": "2023-08-11T09:38:28.160763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 🧑‍💻 Let's code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c98b22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:28.216445Z",
     "iopub.status.busy": "2023-08-11T09:38:28.215691Z",
     "iopub.status.idle": "2023-08-11T09:38:29.448091Z",
     "shell.execute_reply": "2023-08-11T09:38:29.446953Z"
    },
    "papermill": {
     "duration": 1.254278,
     "end_time": "2023-08-11T09:38:29.450535",
     "exception": false,
     "start_time": "2023-08-11T09:38:28.196257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>TF-IDF</h3><div id=\"TF-IDF\" style=\"height:200px; overflow:auto;\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>announce</th>\n",
       "      <th>apple</th>\n",
       "      <th>away</th>\n",
       "      <th>be</th>\n",
       "      <th>day</th>\n",
       "      <th>doctor</th>\n",
       "      <th>fun</th>\n",
       "      <th>hey</th>\n",
       "      <th>how</th>\n",
       "      <th>keep</th>\n",
       "      <th>language</th>\n",
       "      <th>liverpool</th>\n",
       "      <th>natural</th>\n",
       "      <th>of</th>\n",
       "      <th>processing</th>\n",
       "      <th>signing</th>\n",
       "      <th>szoboszlai</th>\n",
       "      <th>the</th>\n",
       "      <th>there</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hey there how be the weather today</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407265</td>\n",
       "      <td>0.407265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259952</td>\n",
       "      <td>0.407265</td>\n",
       "      <td>0.407265</td>\n",
       "      <td>0.407265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liverpool announce the signing of szoboszlai</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.274487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an apple a day keep the doctor away</th>\n",
       "      <td>0.395056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.395056</td>\n",
       "      <td>0.395056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.395056</td>\n",
       "      <td>0.395056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.395056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natural language processing be fun</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import the same dataset from the Bag of Word section\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\"Hey there, how is the weather today.\",\n",
    "          \"Liverpool announced the signing of Szoboszlai\",\n",
    "          \"An apple a day keeps the doctor away.\",\n",
    "          \"Natural Language Processing is fun!\"]\n",
    "\n",
    "text_df = pd.DataFrame({\"Text\": corpus})\n",
    "# ----------------------------------------------------\n",
    "# preprocess function copy and paste from Bag of Word section, but we DO NOT remove stop word\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocessing(text):\n",
    "    doc = nlp(text.lower()) # lowercase\n",
    "    preprocessed_text = []\n",
    "    for token in doc:\n",
    "        if token.is_punct: # if the token is stop word/punctuation we do not add it to the list\n",
    "            continue\n",
    "        preprocessed_text.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(preprocessed_text)\n",
    "\n",
    "text_df[\"Text\"] = text_df[\"Text\"].apply(preprocessing)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(text_df[\"Text\"])\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out() # get all the unique tokens \n",
    "\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=feature_names, index=text_df[\"Text\"]) # create a pandas dataframe\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# just a function that better display large pandas dataframe\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def create_scrollable_table(df, table_id, title):\n",
    "    html = f'<h3>{title}</h3>'\n",
    "    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n",
    "    html += df.to_html()\n",
    "    html += '</div>'\n",
    "    return html\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "html_df = create_scrollable_table(tfidf_df, \n",
    "                            'TF-IDF', \n",
    "                            'TF-IDF')\n",
    "display(HTML(html_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3885f2c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:29.489361Z",
     "iopub.status.busy": "2023-08-11T09:38:29.488612Z",
     "iopub.status.idle": "2023-08-11T09:38:30.707491Z",
     "shell.execute_reply": "2023-08-11T09:38:30.706407Z"
    },
    "papermill": {
     "duration": 1.241907,
     "end_time": "2023-08-11T09:38:30.710620",
     "exception": false,
     "start_time": "2023-08-11T09:38:29.468713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Bag-of-Words</h3><div id=\"Bag-of-Words\" style=\"height:200px; overflow:auto;\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>announce</th>\n",
       "      <th>apple</th>\n",
       "      <th>away</th>\n",
       "      <th>day</th>\n",
       "      <th>doctor</th>\n",
       "      <th>fun</th>\n",
       "      <th>hey</th>\n",
       "      <th>keep</th>\n",
       "      <th>language</th>\n",
       "      <th>liverpool</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "      <th>signing</th>\n",
       "      <th>szoboszlai</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hey weather today</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liverpool announce signing szoboszlai</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple day keep doctor away</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natural language processing fun</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy of what we did in Bag of Word Section\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\"Hey there, how is the weather today.\",\n",
    "          \"Liverpool announced the signing of Szoboszlai\",\n",
    "          \"An apple a day keeps the doctor away.\",\n",
    "          \"Natural Language Processing is fun!\"]\n",
    "\n",
    "text_df = pd.DataFrame({\"Text\": corpus})\n",
    "# ----------------------------------------------------\n",
    "# preprocess function copy and paste from Bag of Word section\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocessing(text):\n",
    "    doc = nlp(text.lower()) # lowercase\n",
    "    preprocessed_text = []\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_stop: # if the token is stop word/punctuation we do not add it to the list\n",
    "            continue\n",
    "        preprocessed_text.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(preprocessed_text)\n",
    "\n",
    "text_df[\"Text\"] = text_df[\"Text\"].apply(preprocessing)\n",
    "# ----------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(text_df['Text'])\n",
    "\n",
    "feature_names = cv.get_feature_names_out() # get all the unique tokens \n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=feature_names, index=text_df['Text']) # create a pandas dataframe\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# just a function that better display large pandas dataframe\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def create_scrollable_table(df, table_id, title):\n",
    "    html = f'<h3>{title}</h3>'\n",
    "    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n",
    "    html += df.to_html()\n",
    "    html += '</div>'\n",
    "    return html\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "html_df = create_scrollable_table(df, \n",
    "                            'Bag-of-Words', \n",
    "                            'Bag-of-Words')\n",
    "display(HTML(html_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff58f71",
   "metadata": {
    "papermill": {
     "duration": 0.02054,
     "end_time": "2023-08-11T09:38:30.750912",
     "exception": false,
     "start_time": "2023-08-11T09:38:30.730372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now you see the purpose and power of TF-IDF, it's purpose is more clear when you work on larger dataset. But you can see that the weighting on words like 'the' is relatively lower compare to other words for the result in TF-IDF. While for bag of words, it has the same weight for all the words. (Noted that if the word appear twice in a row for bag of word, the weight will be 2, it appears that all weight is 1 right now since all word only appeared once in each df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a255ea",
   "metadata": {
    "papermill": {
     "duration": 0.018351,
     "end_time": "2023-08-11T09:38:30.787828",
     "exception": false,
     "start_time": "2023-08-11T09:38:30.769477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"word-embeddings\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">🌟 Word Embeddings 🌟</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf1d62",
   "metadata": {
    "papermill": {
     "duration": 0.018222,
     "end_time": "2023-08-11T09:38:30.824830",
     "exception": false,
     "start_time": "2023-08-11T09:38:30.806608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/FFdoj7s.png\" style=\"height:400px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc4a260",
   "metadata": {
    "papermill": {
     "duration": 0.01837,
     "end_time": "2023-08-11T09:38:30.862675",
     "exception": false,
     "start_time": "2023-08-11T09:38:30.844305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "❗ *Word embedding or word vector is an approach with which we represent documents and words. It is defined as a **numeric vector input that allows words with similar meanings to have the same representation**. It can approximate meaning and represent a word in a lower dimensional space.*\n",
    "\n",
    "Probably THE MOST IMPORTANT section throughout the notebook and in feature engineering process in NLP. Word embedding is a very important concept in NLP which is basically a way to 'vectorize' words into word vectors (if you don't know what's a vector, imagine it as a one dimensional array).\n",
    "\n",
    "But the concept of word embedding is not simply to assign a random vector to a word, but assign the vector to each word in a way so that words of similar meaning will have a similar vector and have a closer distance in the vector space.\n",
    "\n",
    "In fact, techniques we have mentioned just now like [TF-IDF](#TF-IDF) is in fact a form of word embedding, but this section and the following sections will focus on more advance and complex word embedding strategies and methods particularly involving deep learning. \n",
    "<br>\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/jsSEAHK.png\" style=\"height:300px;\"> </div>\n",
    "\n",
    "<br>\n",
    "\n",
    "But before we dive into the concepts and theory of word embeddings its related concepts and techniques like cosine similarity, I want to take another approach and show you how a word vector look like using spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed50a28",
   "metadata": {
    "papermill": {
     "duration": 0.018226,
     "end_time": "2023-08-11T09:38:30.899596",
     "exception": false,
     "start_time": "2023-08-11T09:38:30.881370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 🧑‍💻 Let's code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "908d4ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:30.939091Z",
     "iopub.status.busy": "2023-08-11T09:38:30.938359Z",
     "iopub.status.idle": "2023-08-11T09:38:36.257264Z",
     "shell.execute_reply": "2023-08-11T09:38:36.255512Z"
    },
    "papermill": {
     "duration": 5.342026,
     "end_time": "2023-08-11T09:38:36.260280",
     "exception": false,
     "start_time": "2023-08-11T09:38:30.918254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the word Natural have a vector in spacy? True\n",
      "Does the word Language have a vector in spacy? True\n",
      "Does the word Processing have a vector in spacy? True\n",
      "Does the word is have a vector in spacy? True\n",
      "Does the word fun have a vector in spacy? True\n",
      "Does the word asdasdasd have a vector in spacy? False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "doc = nlp(\"Natural Language Processing is fun asdasdasd\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"Does the word {token} have a vector in spacy? {token.has_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a01a5980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:36.300937Z",
     "iopub.status.busy": "2023-08-11T09:38:36.300521Z",
     "iopub.status.idle": "2023-08-11T09:38:36.312042Z",
     "shell.execute_reply": "2023-08-11T09:38:36.310908Z"
    },
    "papermill": {
     "duration": 0.034936,
     "end_time": "2023-08-11T09:38:36.314491",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.279555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6209e+00, -4.2642e+00,  2.4627e+00, -8.7363e-01,  7.3510e-01,\n",
       "       -1.2308e+00,  1.0982e+00, -3.9754e-01, -1.7352e+00,  1.0502e+00,\n",
       "        1.4811e+00,  2.1259e+00, -3.6473e+00, -1.3301e+00,  1.6452e+00,\n",
       "        1.6438e+00,  2.3485e+00,  1.6963e+00, -2.4390e+00, -4.2472e+00,\n",
       "       -2.7240e+00,  3.0848e+00, -2.1869e+00,  4.6633e+00,  2.4792e+00,\n",
       "       -1.3560e+00,  2.1133e+00, -3.9983e+00, -8.5734e-01, -4.3072e-01,\n",
       "       -6.2560e-01, -2.8925e+00, -2.0539e-01,  2.5628e+00, -5.8583e+00,\n",
       "       -5.8432e+00,  1.2105e+00,  1.2293e+00, -4.8879e+00,  2.1147e-01,\n",
       "        1.3969e+00, -1.9619e+00, -1.4299e+00,  8.8560e-01, -2.2521e+00,\n",
       "        5.8442e+00,  4.8863e+00,  5.6117e-01,  3.0167e+00, -5.3299e+00,\n",
       "        1.2969e+00,  1.2631e-01, -1.2806e+00,  3.0387e+00, -2.1331e-01,\n",
       "       -7.0597e-01, -3.4147e+00,  5.3751e-01, -2.2919e+00, -1.0904e+00,\n",
       "       -1.0717e+00, -3.8249e+00,  1.4455e+00,  2.0177e+00,  2.0078e+00,\n",
       "       -1.0829e+00, -3.1302e+00, -3.1663e-01,  5.8296e+00, -4.2118e+00,\n",
       "       -1.9953e+00, -3.5881e-02, -2.9315e+00, -1.6552e+00, -1.3832e+00,\n",
       "       -1.9949e+00, -4.6480e+00,  9.7992e-01,  5.9605e-03,  6.0209e+00,\n",
       "       -2.7420e-01,  3.3124e+00, -3.6728e+00, -1.9955e+00, -3.7864e+00,\n",
       "        1.9517e+00, -4.0983e+00, -7.5781e-02, -1.2098e+00,  8.2342e-01,\n",
       "        2.4037e+00,  3.2379e-01,  2.1618e+00,  5.1678e-01, -1.1527e+00,\n",
       "       -1.0616e+00, -1.9576e+00,  2.6197e+00,  2.5630e+00,  1.7385e-01,\n",
       "        8.8340e-01, -2.3144e+00,  5.4746e-01,  3.4025e+00, -2.1647e+00,\n",
       "        2.1994e+00, -1.1526e-01, -3.0178e+00,  1.3999e+00, -1.8079e+00,\n",
       "        2.8649e+00, -2.8151e-01, -4.1587e+00, -3.1073e+00, -1.5021e+00,\n",
       "        1.1237e+00,  1.1528e+00,  6.9207e-01, -8.3222e+00, -2.3675e+00,\n",
       "        1.3900e+00, -1.1711e+00,  2.3686e+00, -5.4060e+00, -2.4328e+00,\n",
       "       -4.1308e+00, -2.8748e+00, -3.0093e+00,  4.5271e+00, -1.7126e+00,\n",
       "       -2.3935e+00,  2.8302e+00,  3.3230e+00,  2.0673e+00, -3.7364e+00,\n",
       "       -5.3977e-01, -1.3410e+00,  1.8624e-02, -2.6109e+00,  2.0205e+00,\n",
       "        3.0294e+00, -1.0207e+00, -4.1563e+00,  2.6466e+00,  9.9714e-01,\n",
       "       -2.8366e-01, -3.5185e-01,  3.3260e+00,  9.7550e-01,  1.3008e+00,\n",
       "        3.1295e+00,  1.3201e+00, -1.3294e+00, -5.0171e+00, -1.6367e+00,\n",
       "        1.2302e+00,  2.0592e+00, -4.0621e+00,  7.0472e-01, -2.1723e-01,\n",
       "        1.2581e+00, -8.1675e-01,  4.6146e+00,  2.9505e+00,  1.9339e+00,\n",
       "       -2.1187e+00, -8.9446e-01, -5.6175e-01,  5.1397e+00,  2.8060e-01,\n",
       "       -3.4728e+00, -2.8275e+00,  3.0944e+00,  3.5677e-01, -1.1877e+00,\n",
       "       -2.3724e+00,  2.5417e+00,  8.0606e-01, -8.3459e-01,  8.1858e-01,\n",
       "       -1.0555e+00, -8.2218e-01,  1.1344e+00, -3.0724e+00, -3.8271e-01,\n",
       "       -1.5318e+00,  2.9766e-01,  1.4450e+00, -6.2743e-01,  2.7314e+00,\n",
       "        8.5800e-01, -3.5714e+00,  3.4116e-01, -2.1262e+00,  7.2500e-01,\n",
       "        2.8645e+00, -3.5599e+00,  1.5925e+00,  1.9793e-04, -1.6348e+00,\n",
       "        1.0988e+00, -3.0017e+00, -1.2319e+00,  6.0720e-01,  5.6213e+00,\n",
       "       -4.3551e+00, -6.3383e+00,  1.5395e+00,  2.1339e+00, -4.2565e+00,\n",
       "        2.0756e+00,  1.3922e+00, -2.2268e-04,  1.0594e+00, -4.9425e+00,\n",
       "        1.0606e+00,  3.8130e+00, -3.1420e+00, -1.2584e+00,  1.8955e-01,\n",
       "        2.2325e+00,  1.9351e+00, -5.4118e+00, -1.7292e+00,  1.0900e-02,\n",
       "       -3.0009e-01,  7.9210e-01,  2.9058e+00,  1.9106e+00,  1.5769e+00,\n",
       "        1.7735e+00,  3.4219e+00,  5.0043e-02,  2.6129e+00,  2.2253e+00,\n",
       "       -1.1729e+00,  1.5727e-01,  2.6524e+00,  1.3345e+00, -1.8003e+00,\n",
       "       -8.1750e-01,  1.0562e+00,  5.6902e+00,  2.2176e+00, -2.5116e+00,\n",
       "        5.4022e+00, -9.4124e-01,  3.1226e+00, -9.3300e-01, -9.9275e-01,\n",
       "        9.8880e-01,  5.5531e-01, -4.9224e-01, -3.3339e+00, -1.1779e+00,\n",
       "       -4.8862e+00, -3.8684e-01,  2.7388e+00, -9.0740e-01, -2.2699e+00,\n",
       "       -1.0681e+00, -3.9095e+00,  2.6228e+00,  2.2522e+00, -5.5166e-01,\n",
       "       -2.3282e+00, -1.9741e+00,  7.4659e-01, -3.7854e+00, -2.0206e+00,\n",
       "       -9.9948e-02,  5.6917e+00,  5.0359e+00, -1.0585e+00,  8.1430e-01,\n",
       "       -9.2595e-01, -2.9764e-01, -2.0574e+00, -1.4594e+00, -1.8023e-01,\n",
       "        1.1155e+00, -5.7191e-01,  9.0030e-01, -6.6178e+00, -2.7962e+00,\n",
       "       -5.3210e+00,  5.7338e-01, -4.1418e-01,  1.4151e+00,  2.7762e+00,\n",
       "       -6.6438e-01, -8.0946e-01,  1.2626e+00, -3.7339e-02, -2.0134e+00,\n",
       "       -1.5832e+00,  1.8674e+00, -7.2049e+00, -4.1533e+00, -8.7940e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector of the word \"Natural\"\n",
    "doc[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7646027d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:36.354558Z",
     "iopub.status.busy": "2023-08-11T09:38:36.354135Z",
     "iopub.status.idle": "2023-08-11T09:38:36.361471Z",
     "shell.execute_reply": "2023-08-11T09:38:36.360210Z"
    },
    "papermill": {
     "duration": 0.030295,
     "end_time": "2023-08-11T09:38:36.363917",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.333622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a83b4",
   "metadata": {
    "papermill": {
     "duration": 0.018902,
     "end_time": "2023-08-11T09:38:36.401922",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.383020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In fact, it is very common for word vector to have a length of 300.\n",
    "\n",
    "Now you might be confused, where does the vectors come from? For now, all you need to know is that the spaCy pipeline provide you with the word vectors of the tokens, which is, they have build models and algorithms to train and got the word vectors and allow users like us to use these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbe41cad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:36.441917Z",
     "iopub.status.busy": "2023-08-11T09:38:36.441538Z",
     "iopub.status.idle": "2023-08-11T09:38:36.459020Z",
     "shell.execute_reply": "2023-08-11T09:38:36.457453Z"
    },
    "papermill": {
     "duration": 0.040447,
     "end_time": "2023-08-11T09:38:36.461396",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.420949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between good and good: 1.0\n",
      "Similarity between good and happy: 0.5769590735435486\n",
      "Similarity between good and sad: 0.30881333351135254\n",
      "Similarity between good and bad: 0.7391888499259949\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"good happy sad bad\")\n",
    "\n",
    "good = doc[0]\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"Similarity between good and {token}: {token.similarity(good)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5f639",
   "metadata": {
    "papermill": {
     "duration": 0.018992,
     "end_time": "2023-08-11T09:38:36.499605",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.480613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "🌟 It is important to note that when we say that two word embeddings are similar, we mean that **the two vectors representing the embeddings are close to each other in some sense, such as geometric distance or cosine similarity**. We are **not only refering to how similar is their meaning**, but depending on the training method, it can be the likelihood of two words to appear in the same context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24576e7",
   "metadata": {
    "papermill": {
     "duration": 0.019864,
     "end_time": "2023-08-11T09:38:36.538742",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.518878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"cosine-similarity\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">🌟 Cosine Similarity 🌟</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a15d3",
   "metadata": {
    "papermill": {
     "duration": 0.019411,
     "end_time": "2023-08-11T09:38:36.577596",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.558185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/GhmO2iq.jpg\" style=\"\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ac7f4",
   "metadata": {
    "papermill": {
     "duration": 0.018887,
     "end_time": "2023-08-11T09:38:36.616271",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.597384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We have mentioned a lot about cosine similarity in our previous section. But what exactly is cosine similarity? \n",
    "\n",
    "❗ *Cosine similarity **measures the similarity between two vectors of an inner product space**. It is **measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction**. It is often used to measure document similarity in text analysis.*\n",
    "\n",
    "### The content of this section is heavily inspired by this video by \n",
    "\n",
    "### StatQuest: https://www.youtube.com/watch?v=e9U0QAFbfLI \n",
    "\n",
    "### It is highly recommended that you watch this 10 minutes video for awesome animations and a more detailed explanations. Below will be an extraction of content from the video.\n",
    "\n",
    "One of the main feature of cosine similarity is that it is a metric that can be used regardless of how large and high dimension the dataset is, that is, we can still compute the angle between two words no matter how many words and dimensionality there are, unlike other metrics such as the Euclidean distance approach (calculate distance btween two points) which may not work well when the dataset and dimension is huge. \n",
    "\n",
    "Before we dive into the maths, let's see a very simple example to let us have an intuitive understanding of how cosine similarity work with the following example from the StatQuest video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564af5b",
   "metadata": {
    "papermill": {
     "duration": 0.019002,
     "end_time": "2023-08-11T09:38:36.654531",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.635529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ✍ Example (Image and example taken from the StatQuest video)\n",
    "\n",
    "In particular, let us explore how do we find the cosine similarity between the phrase \"Hello\" and \"Hello World\". \n",
    "\n",
    "**1. The first step we would do is to create a countvectorizer similar to the idea of [bag of words](#bag-of-words).**\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/OhZfQkx.png\" style=\"height: 150px;\"> </div>\n",
    "\n",
    "<br>\n",
    "\n",
    "**2. Now we plot it on a 2D graph, with the value of axis being each unique token**\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/ijzhyc2.png\" style=\"height: 300px;\"> </div>\n",
    "\n",
    "**3. Calculate the cosine similarity, in this case, it is simply the cosine of the angle between the lines of the two point to the origin**\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/FWGI09l.png\" style=\"height: 300px;\"> </div>\n",
    "\n",
    "I hope you grasp the general idea behind cosine similarity with these awesome animations. In fact, this following example illustrate the aforementioned point that 'it is a metric that can be used regardless of how large and high dimension the dataset is' which tackle the flaws of metrics such as Eucliden distance approach.\n",
    "\n",
    "Let's see this example which demonstrate the cosine similarity of the phrase \"Hello Hello Hello\" and \"Hello World\".\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/5DoUown.png\" style=\"height: 300px;\"> </div>\n",
    "\n",
    "<br>\n",
    "\n",
    "This demonstrates that cosine similarity *is determined entirely by the angle between the lines and not by the lengths of the line*.\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/5DoUown.png\" style=\"height: 300px;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32aa6b2",
   "metadata": {
    "papermill": {
     "duration": 0.019396,
     "end_time": "2023-08-11T09:38:36.693460",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.674064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ❗ The greater the value of cosine similarity, the more similar two vectors are to each other\n",
    "\n",
    "(in general, value of cosine similarity can be between -1 and 1, but for word embedding, it is always positive for some reason: https://vaibhavgarg1982.medium.com/why-are-cosine-similarities-of-text-embeddings-almost-always-positive-6bd31eaee4d5#:~:text=Going%20by%20the%20definition%20of,the%20smallest%20score%20was%200.4522.)\n",
    "\n",
    "The above examples clearly explained the basic concept of cosine similarity. But it will not work when there are more than 2 unique tokens, ie. when there is a higher dimension. To deal with this problem, we need to utilize the mathematical equation of cosine similarity.\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/IxqVJiK.png\" style=\"height: 200px;\"> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c6384",
   "metadata": {
    "papermill": {
     "duration": 0.019276,
     "end_time": "2023-08-11T09:38:36.732543",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.713267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> cosine similarity = dot product of AxB / magnitude of A x magniture of B\n",
    "\n",
    "Let's see the formula in action with an example with these two phrases\n",
    "\n",
    "> **1. natural language processing is fun** <br>\n",
    "> **2. machine learning is fun**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b5f4610",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:36.773218Z",
     "iopub.status.busy": "2023-08-11T09:38:36.772778Z",
     "iopub.status.idle": "2023-08-11T09:38:36.790237Z",
     "shell.execute_reply": "2023-08-11T09:38:36.789031Z"
    },
    "papermill": {
     "duration": 0.040874,
     "end_time": "2023-08-11T09:38:36.792741",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.751867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>natural language processing is fun</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine learning is fun</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    fun  is  language  learning  machine  \\\n",
       "Text                                                                       \n",
       "natural language processing is fun    1   1         1         0        0   \n",
       "machine learning is fun               1   1         0         1        1   \n",
       "\n",
       "                                    natural  processing  \n",
       "Text                                                     \n",
       "natural language processing is fun        1           1  \n",
       "machine learning is fun                   0           0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement count vectorizer (for implementation details, view Bag of Word section)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\"natural language processing is fun\",\n",
    "          \"machine learning is fun\"]\n",
    "\n",
    "text_df = pd.DataFrame({\"Text\": corpus})\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(text_df['Text'])\n",
    "\n",
    "feature_names = cv.get_feature_names_out() # get all the unique tokens \n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=feature_names, index=text_df['Text']) # create a pandas dataframe\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f267287e",
   "metadata": {
    "papermill": {
     "duration": 0.019278,
     "end_time": "2023-08-11T09:38:36.831665",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.812387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's see how the maths work, let's first focus on the **numerator**\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/I6XXRjc.png\" style=\"height: 100px;\"> </div>\n",
    "\n",
    "If you know your linear algebra, you will know that this is simply the dot product of vector A and vector B, ie. the dot product of \\[1,1,1,0,0,1,1\\] and \\[1,1,0,1,1,0,0\\], which if we expand it, it will become\n",
    "\n",
    "> (1x1) + (1x1) + (1x0) + (0x1) + (0x1) + (1x0) + (1x0)\n",
    "\n",
    "Now you seeing where it goes right? The first (1x1) corresponds to the word 'fun' where this one in (*1*x1) is the corresponding value of vector A and the one in (1x*1*) corresponding value of vector B for the word 'fun'. We then do summation all the unique token to get the final result.\n",
    "\n",
    "Now let us look at the **denominator**\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/EGPoj5Z.png\" style=\"height: 100px;\"> </div>\n",
    "\n",
    "\n",
    "Now that you have an idea of what the notations refers to, this should be very easy, in this example, this will simply be\n",
    "\n",
    "> $\\sqrt{(1^2 + 1^2 + 1^2 + 0^2 + 0^2 + 1^2 + 1^2)}$ x $\\sqrt{(1^2 + 1^2 + 0^2 + 1^2 + 1^2 + 0^2 + 0^2)}$\n",
    "\n",
    "Where the first part of the square root is the summation of each value in vector A and similar for vector B\n",
    "\n",
    "\n",
    "So the overall equation will be \n",
    "\n",
    "$\\frac{(1*1) + (1*1) + (1*0) + (0*1) + (0*1) + (1*0) + (1*0)}{\\sqrt{(1^2 + 1^2 + 1^2 + 0^2 + 0^2 + 1^2 + 1^2)}  *  \\sqrt{(1^2 + 1^2 + 0^2 + 1^2 + 1^2 + 0^2 + 0^2)}}$\n",
    "\n",
    "And if you do the maths the answer will be roughly equal to 0.4472\n",
    "\n",
    "In fact we can easily verify it using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f7be33",
   "metadata": {
    "papermill": {
     "duration": 0.019448,
     "end_time": "2023-08-11T09:38:36.871109",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.851661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 🧑‍💻 Let's code \n",
    "\n",
    "Using cosine_similarity in Python is a relatively simple task, you simply need to import it from the sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fa8cfe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T09:38:36.912505Z",
     "iopub.status.busy": "2023-08-11T09:38:36.911756Z",
     "iopub.status.idle": "2023-08-11T09:38:37.026478Z",
     "shell.execute_reply": "2023-08-11T09:38:37.025381Z"
    },
    "papermill": {
     "duration": 0.138166,
     "end_time": "2023-08-11T09:38:37.028804",
     "exception": false,
     "start_time": "2023-08-11T09:38:36.890638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4472136]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([1,1,1,0,0,1,1]) # natural language processing is fun\n",
    "B = np.array([1,1,0,1,1,0,0]) # machine learning is fun\n",
    "\n",
    "cosine_similarity([A], [B])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbaa05",
   "metadata": {
    "papermill": {
     "duration": 0.019408,
     "end_time": "2023-08-11T09:38:37.068266",
     "exception": false,
     "start_time": "2023-08-11T09:38:37.048858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"arithmetic-of-word-embeddings\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Arithmetic of Word Embeddings</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aac803",
   "metadata": {
    "papermill": {
     "duration": 0.019251,
     "end_time": "2023-08-11T09:38:37.107128",
     "exception": false,
     "start_time": "2023-08-11T09:38:37.087877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/6Z1kIR8.png\" style=\"height: 350px;\"> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ed5e3",
   "metadata": {
    "papermill": {
     "duration": 0.020003,
     "end_time": "2023-08-11T09:38:37.146864",
     "exception": false,
     "start_time": "2023-08-11T09:38:37.126861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before we move on with more fancy concepts and algorithms, I want to show you some awesome thing word vectors can do, which is the arithmetic of word embeddings.\n",
    "\n",
    "As we mentioned, word embeddings are just some vectors, so naturally we can do some kind of mathematical operations with them. The beauty of word embeddings lies exactly here, let's take an example from the image above.\n",
    "\n",
    "### ✍ Example: King 🤴 - Man 👱‍♂️ + Woman 👩 = Queen 👸\n",
    "\n",
    "> Vector of King: \\[0,0,1\\] <br>\n",
    "> Vector of Man: \\[0,0,0\\] <br>\n",
    "> Vector of Woman: \\[1,0,0\\] <br>\n",
    "> Vector of Queen: \\[1,0,1\\] <br>\n",
    "\n",
    "Now when we do the maths, we can indeed verify that King - Man + Woman = Queen. How amazing! How does this happen? (look at the emoji to have a more intuitive understanding!)\n",
    "\n",
    "### Did this question ever hit you while you are surfing the previous sections -\n",
    "### >> What do each value of a word vector mean? How do we determine how the vector should be like to achieve this amazing outcome?\n",
    "\n",
    "The above picture explain a bit. For each word vector, each column have a meaning, which is Feminity, Youth, Royalty. And according to these **semantics** (relating to meaning in language or logic), we attempt to assign value based on these meanings for each word. \n",
    "\n",
    "In reality, like the vector we saw in spaCy library, we actually do not know what each column means, not that the information is not disclosed, but that we actually have no clue what each column represents. All we know is that each column has its own **meaning** , it is this **meaning** that allow the magic we saw in word vectors like similarity and clusterings of similar words. How does this happen, you may ask. Well this is highly due to the method and algorithms we use in training these word embeddings which is what will be introduced in the following section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c187f",
   "metadata": {
    "papermill": {
     "duration": 0.020248,
     "end_time": "2023-08-11T09:38:37.187277",
     "exception": false,
     "start_time": "2023-08-11T09:38:37.167029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"summary-2\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">📝 Short Summary (Part 2.1: Word Embeddings) 📝</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a69d1bb",
   "metadata": {
    "papermill": {
     "duration": 0.019482,
     "end_time": "2023-08-11T09:38:37.226466",
     "exception": false,
     "start_time": "2023-08-11T09:38:37.206984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's review what we have learnt in part 2: Feature Engineering. \n",
    "\n",
    "### 1. [Bag of Words](#bag-of-words)\n",
    "\n",
    "A way of word embedding, in which we create a count vectorizer to count the word appearance\n",
    "\n",
    "### 2. [Bag of n-grams](#bag-of-n-grams)\n",
    "\n",
    "A way of word embedding similar to bag of words, but instead of just use a single word as column, we use a range of consecutive words as features\n",
    "\n",
    "### 3. [TF-IDF](#tf-idf)\n",
    "\n",
    "A way of word embedding in which we use some sort of formulas to assign weight to each word in each corpus, effectively replacing the need to remove stop words\n",
    "\n",
    "### 4. [Word Embeddings](#word-embeddings)\n",
    "\n",
    "A way to represent text in which we attempt to numerize/vectorize words in a meaningful way in which words that are more similar will be closer together in vector space\n",
    "\n",
    "### 5. [Cosine Similarity](#cosine-similarity)\n",
    "\n",
    "A popular metrics to quantify similarity between two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae1dd6",
   "metadata": {
    "papermill": {
     "duration": 0.019311,
     "end_time": "2023-08-11T09:38:37.265503",
     "exception": false,
     "start_time": "2023-08-11T09:38:37.246192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Continue on: **🟢 NLP Beginner Series Part 2.2:  Embedding Models** \n",
    " https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-2-2-embedding-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d71eb2",
   "metadata": {
    "papermill": {
     "duration": 0.019457,
     "end_time": "2023-08-11T09:38:37.304513",
     "exception": false,
     "start_time": "2023-08-11T09:38:37.285056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"hey-there\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">👋 Hey there 👋</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373f4f6",
   "metadata": {
    "papermill": {
     "duration": 0.019974,
     "end_time": "2023-08-11T09:38:37.344121",
     "exception": false,
     "start_time": "2023-08-11T09:38:37.324147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 🌟 About the Author\n",
    "\n",
    "Hey there, I am an undergraduate majoring in quantitative finance at the Univeristy of Hong Kong. I am a highly motivated person with a huge passion in mathematics and computer science related topics. \n",
    "\n",
    "When I said I am a beginner in the start of the notebook, I am not lying! In fact, I am a self learner in the field of machine learning and natural language processing, and as the moment I am writing this, I have only started my machine learning journey for a couple of months. \n",
    "\n",
    "Although I do not have a lot of experience and knowlegde in the field, I am more than happy to connect and learn/work together in some projects and stuff like that.\n",
    "\n",
    "Do you know Kaggle got discord server? https://discord.gg/kaggle (crxxom) <br>\n",
    "My Linkedin: https://www.linkedin.com/in/jadon-ng-848a48263/\n",
    "\n",
    "### 🌟 Upvote the notebook!\n",
    "\n",
    "If you find the notebook useful, it will be great if you can show some support by upvoting the notebook, it means a lot to me! \n",
    "\n",
    "Also, if you want to make suggestions/corrections to the content of this notebook, don't hesitate to comment your thoughts, I will reply you as soon as possible.\n",
    "\n",
    "One more thing, if you want to share some of your resources/notebooks, comment down your links in the comment section and let us learn! \n",
    "\n",
    "\n",
    "### 🌟 Will there be updates?\n",
    "\n",
    "As in for now, I will only be updating the notebook if there are some incorrect information or if I discover some useful resources to share. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.752317,
   "end_time": "2023-08-11T09:38:40.019010",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-11T09:37:57.266693",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
