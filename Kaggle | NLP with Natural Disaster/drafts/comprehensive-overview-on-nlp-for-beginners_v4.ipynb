{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">‚ùó Important ‚ùó</span></div>\n\nHey there, after reviewing and finishing this notebook, I realised that the content in this notebook may be too long. That's why I have created a NLP tutorial series to split the content in this notebook into several parts. The content in this notebook and in the series is almost identical, feel free to choose to use this notebook or to learn with the series if it suits your learning need. (Tips: You can download this notebook as a note for future referencing and learn with the tutorial series)\n\nHere are the links to the whole series:\n     \n**‚úÖ Comprehensive Overview on NLP for Beginners ü•≥ (collection of all series)** <br>\n https://www.kaggle.com/code/crxxom/comprehensive-overview-on-nlp-for-beginners\n \n    \n**üî¥ NLP Beginner Series Part 1: NLP Preprocessing** <br>\nhttps://www.kaggle.com/code/crxxom/nlp-beginner-series-part-1-nlp-preprocessing\n\n**üü° NLP Beginner Series Part 2.1: Word Embeddings** <br>\nhttps://www.kaggle.com/code/crxxom/nlp-beginner-series-part-2-1-word-embeddings\n\n**üü¢ NLP Beginner Series Part 2.2: Embedding Models** <br>\nhttps://www.kaggle.com/code/crxxom/nlp-beginner-series-part-2-2-embedding-models\n\n**üü£ NLP Beginner Series Part 3: Case Study** <br>\n https://www.kaggle.com/code/crxxom/nlp-beginner-series-part-3-case-study","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üî•  Overview of the Notebook  üî•</div>\n","metadata":{}},{"cell_type":"markdown","source":"## >> Important concepts and techniques you need to know to get you started with NLP explained in a **‚û° beginner friendly way ‚¨Ö**\n\n## >> **ONLY** pre-requisite you need: Python, Pandas and Numpy\n\n## >> üïë Take your Time! (~ 2-3 hours)\n\nThe estimated time to complete this notebook can be between 2-3 hours if you are a beginner. It can take even longer if read the additional resources and do other research while surfing through the notebook. There is no need to rush the process! In fact, it is recommended to split the workload to a few days for a more comfortable and effective learning. But I can guarantee you your time will be well spent if your an absolute beginner!\n\n## >> Is this notebook for me ü§î\n\nThis notebook **covers the absolute fundamentals of NLP to let you have an overview with what exactly is NLP and how it work.** \n\n### üåü Skim through the notebook\n\nQuickly scroll through the whole notebook to see if the style suit your learning needs!\n\n### üåü This notebook is beginner friendly\nThis notebook is very much a **beginner friendly notebook**, the only pre-requisite you will need is Python and some common data science frameworks such as Pandas and Numpy. All the concepts covered in this notebook will be clearly explained but may not be deeply explained, you will be encouraged to do additional research on your own if you would like to understand more about a particular topic. The goal of this notebook is to give you an overview to NLP and introduce you with key vocabularies and concepts you will need to know in NLP.\n\n\n### üåü Seperate sections to suit your needs\n**Each section of the notebook is seperated**, ie. you do NOT need to run the code on section 1 to be able to run the code in section 2, libraries and framework will be introduced and imported in each individual section, dataset are not interrelatble across sections. If you are only interested in any particular section, **you can skip through the sections to run the code in that particular section only**. In fact, I try to make each codeblock as seperated as possible so that you do not need to run codeblock A to be able to run B.\n\n\n### üåü Images to help you learn\nThis notebook contains **a lot of images** to demonstrate the concepts and principles as I find it to be the best way to understand complicated concepts. These images are mostly images from google and screenshots from youtube tutorials, if any parties find it inappropriate to use their images/screenshots, please let me know I will delete is as soon as possible.\n\n### üåü I am a beginner also\nThis notebook is **written from the perspective of a beginner** to the world of machine learning. With that being said, there may be some concepts that isn't covered clearly/deeply, but on the other hand, I feel like it will be easier to catch up if you are a beginner as well, as I will not be throwing some complicated jargons and overload you too much since I am a beginner as well. Additionally I have also included a lot of resources if you want to have a deeper insight for each topics.\n\n\n### üåü Notebook designed in a way to not overwhelm you\n**Take your time!** Small and lengthy words are hard and dull to read, that's why this notebook trys to **minimize the amount of text and use bold text, images and emojis** so that you can navigate the notebook faster. The notebook also attempts to **minimize the amount of code** as much as possible, to save you from being too overwhelmed. \n\n**Try to run the codeblocks youself**, running and changing codes in the codeblocks can give you a more immersive learning experience. \n\nIf you appreciate my work or would like to express your views/make suggestions on some of my code and content, feel free to comment and correct me, I will make sure to keep updating and make improvements to this notebook! I hope you will find this notebook useful and an enjoyable journey.\n\n## üîé Concepts and techniques you can take away with you from this notebook\n\n#### üü¢ [Overview of NLP Pipeline](#nlp-pipeline)\n\n#### üü¢ [Tokenization and spaCy pipeline](#tokenization)\n\n#### üü¢ [Stop Word](#stop-word)\n\n#### üü¢ [Stemming and Lemmatization](#stemming-and-lemmatization)\n\n#### üü¢ [Out of Vocabulary (OOV)](#out-of-vocabulary)\n\n#### üü¢ [POS (Part of Speech) tagging](#part-of-speech-tagging)\n\n#### üü¢ [Name Entity Recognition (NER)](#named-entity-recognition)\n\n#### üü¢ [Bag of Words (BOW)](#bag-of-words)\n\n#### üü¢ [Bag of n-grams](#bag-of-n-grams)\n\n#### üü¢ [Term Frequency - Inverse Document Frequency (TF-IDF)](#tf-idf)\n\n#### üü¢ [Word Embeddings](#word-embeddings)\n\n#### üü¢ [Cosine Similarity](#cosine-similarity)\n\n#### üü¢ [Arithmetic of Word Embeddings](#arithmetic-of-word-embeddings)\n\n#### üü¢ [Continuous Bag of Words Model](#cbow)\n\n#### üü¢ [SkipGram Model](#skipgram)\n \n#### üü¢ [Word2Vec](#word2vec)\n\n#### üü¢ [fastText](#fasttext)\n\n#### üü¢ [GloVe](#glove)\n\n#### üü¢ [Hugging Face](#huggingface)\n\n#### üü¢ [Bidirectional Encoder Representations from Transformers (BERT)](#bert)\n\n#### üü¢ [Case Study: NLP with Disaster Tweet](#case-study)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/YmCSFRs.png\" style=\"height:500px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/buymiQ8.png\" style=\"height:500px;\"> </div>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-07T07:39:21.603523Z","iopub.execute_input":"2023-08-07T07:39:21.604021Z","iopub.status.idle":"2023-08-07T07:39:21.612542Z","shell.execute_reply.started":"2023-08-07T07:39:21.603985Z","shell.execute_reply":"2023-08-07T07:39:21.610742Z"}}},{"cell_type":"markdown","source":"# <a id=\"nlp-pipeline\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Overview of NLP Pipeline</span></div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/Jm6yryM.png\" style=\"height:250px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"### 1Ô∏è‚É£ Data Collection\n* Collection of raw data to train the model\n* Typical NLP data examples are text data such as reviews from amazon, tweets on twitter or news articles from google news\n \n### 2Ô∏è‚É£ Text Cleaning\n* Common text cleaning tasks are removing special characters, removing HTML tags, removing extra lines and space etc.\n* Often involve the use of **regular expression** (it's basically a technique to input a sequence to extract a pattern in a sentence, it is not a machine learning thing but more like a python thing)\n\n### 3Ô∏è‚É£ Preprocessing\n* The point of preprocessing is to transform our dataset in a way that allow our model to better grab the key information and value of our input data\n* A lot of techniques and concepts will be covered in this section including,\n* **Tokenization**\n* **Stop word**\n* **Stemming and Lemmatization**\n* **POS tagging**\n* **Name Entity Recognition (NER)**\n\n\n### 4Ô∏è‚É£ Feature Engineering\n* In NLP, feature engineering process mostly involve **vectorizing** words (converting strings into meaningful numbers), techniques and concept covered in this section including,\n* **Bag of Word (BOW)**\n* **Bag of n-grams**\n* **Term Frequency - Inverse Document Frequency (TF-IDF)**\n* **Word embedding**\n* **Cosine Similarity**\n* **CBOW and SkipGram**\n* **Word2Vec**\n* **GloVe**\n* **fastText**\n* **Bidirectional Encoder Representations from Transformers (BERT)**\n\n### 5Ô∏è‚É£ Modelling\n* Examples of models commonly used for NLP tasks are Naive Bayes and deep learning algorithms like LSTM\n\n### 6Ô∏è‚É£-8Ô∏è‚É£ Evaluation, Deployment and Monitoring and model updating","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"tokenization\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üåü Tokenization with spaCy üåü</span></div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/ia7NSZy.png\" style=\"height:300px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *Tokenization is used in natural language processing to **split paragraphs and sentences into smaller units** that can be more easily assigned meaning. The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words).*\n\nIn NLP, you will most commonly come across with the term 'token'. A lot of time a token is the same as a word, especially in english language, but for different languages, it may be different due to the difference in grammar and structure.\n\nIn this section, we will be exploring the power of tokenization with **spaCy**, a very popular NLP framework. We will be looking at what is tokenization and some useful methods and attributes built in spaCy libaray.","metadata":{}},{"cell_type":"code","source":"# if you never download spacy before:\n# pip install spacy","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filter out the warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-08-09T13:49:30.493628Z","iopub.execute_input":"2023-08-09T13:49:30.494305Z","iopub.status.idle":"2023-08-09T13:49:30.499819Z","shell.execute_reply.started":"2023-08-09T13:49:30.494262Z","shell.execute_reply":"2023-08-09T13:49:30.498660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we start talking about tokenization, I would like to briefy introduce you with the pipelines that spacy offer. spaCy offer users with trained pipelines that you can use by using the line `spacy.load(\"en_core_web_sm\")` where en stands for english, and sm stands for small. \n\nThere are different pipelines that spacy offer other than \"en_core_web_sm\" (12 MB), such as \"en_core_web_lg\" (560MB), \"zh_core_web_md\", (zh -> chinese, md -> medium, 74MB) and more. You can see **all the available pipeline on https://spacy.io/usage/models** and click into each 'pipeline' for more information. \n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/eAqN9F9.png\" style=\"height:400px;\"> </div>\n\n<br>\n\n**You need to download the pipeline before using it by typing `python -m spacy download en_core_web_sm` in your terminal**, which is what we will be using in this section.\n\n\nFor more information: \n\nspaCy pipeline: https://spacy.io/usage/processing-pipelines\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/sEZ1yu7.png\" style=\"height:200px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n","metadata":{}},{"cell_type":"code","source":"import spacy\n\nnlp_sm = spacy.load(\"en_core_web_sm\")\n\nnlp_blank = spacy.blank(\"en\")\n\nprint(\"Components in en_core_web_sm pipeline: \", nlp_sm.pipe_names)\nprint(\"Components in blank pipeline: \", nlp_blank.pipe_names)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:30:47.752715Z","iopub.execute_input":"2023-08-07T09:30:47.753255Z","iopub.status.idle":"2023-08-07T09:30:49.301363Z","shell.execute_reply.started":"2023-08-07T09:30:47.753212Z","shell.execute_reply":"2023-08-07T09:30:49.300115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can add elements manually to your nlp pipeline in your own need.","metadata":{}},{"cell_type":"code","source":"nlp_sm.add_pipe('sentencizer')\nprint(\"Components in sm pipeline: \", nlp_sm.pipe_names)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:30:57.579443Z","iopub.execute_input":"2023-08-07T09:30:57.579951Z","iopub.status.idle":"2023-08-07T09:30:57.603681Z","shell.execute_reply.started":"2023-08-07T09:30:57.579915Z","shell.execute_reply":"2023-08-07T09:30:57.602292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each spaCy pipeline contains a tokenizer, even for spacy.blank() pipeline. ","metadata":{}},{"cell_type":"code","source":"doc1 = nlp_sm(\"Natural Language Processing\")\ntoken1_0 = doc1[0]\nprint(\"Type of doc1[0] for nlp_sm: \", type(token1_0))\ndoc2 = nlp_blank(\"Natural Language Processing\")\ntoken2_0 = doc2[0]\nprint(\"Type of doc2[0] for nlp_blank: \", type(token2_0))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:34:08.519370Z","iopub.execute_input":"2023-08-07T09:34:08.519873Z","iopub.status.idle":"2023-08-07T09:34:08.539285Z","shell.execute_reply.started":"2023-08-07T09:34:08.519839Z","shell.execute_reply":"2023-08-07T09:34:08.537685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenization is often the very first step of preprocessing, it is important to break down a sentence to individual 'token' so that each token has like its own meaning. In fact, in some cases, you may want to assign token manually especially when it comes to slang.","metadata":{}},{"cell_type":"code","source":"# Before manually assigning token\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(\"yessir\")\n\nfor token in doc:\n    print(token)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:41:20.164527Z","iopub.execute_input":"2023-08-07T09:41:20.165102Z","iopub.status.idle":"2023-08-07T09:41:22.004109Z","shell.execute_reply.started":"2023-08-07T09:41:20.165061Z","shell.execute_reply":"2023-08-07T09:41:22.002454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After manually assigning token\n\nfrom spacy.symbols import ORTH\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nnlp.tokenizer.add_special_case(\"yessir\", [\n    {ORTH: \"yes\"},\n    {ORTH: \"sir\"},\n])\n\ndoc = nlp(\"yessir\")\n\nfor token in doc:\n    print(token)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:41:51.410499Z","iopub.execute_input":"2023-08-07T09:41:51.410978Z","iopub.status.idle":"2023-08-07T09:41:52.662827Z","shell.execute_reply.started":"2023-08-07T09:41:51.410926Z","shell.execute_reply":"2023-08-07T09:41:52.661376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenization is essential on its own, but spaCy offers a lot of **built-in attributes and methods** you can use to achieve cool things. Here are some few examples.\n\nFor more information: https://spacy.io/api/token","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(\"The total is $50, pay me through email at abc@gmail.com\")\n\nfor token in doc:\n    print(f\"{token} [alpha?: {token.is_alpha}] [currency?: {token.is_currency}] [punctuation? {token.is_punct}] [email? {token.like_email}]\")","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:49:54.792480Z","iopub.execute_input":"2023-08-07T09:49:54.792942Z","iopub.status.idle":"2023-08-07T09:49:56.077658Z","shell.execute_reply.started":"2023-08-07T09:49:54.792905Z","shell.execute_reply":"2023-08-07T09:49:56.076455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"stop-word\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Stop Word</span></div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img src=\"https://i.imgur.com/owv8iQX.jpg\" style=\"height:300px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *Stop words are a set of commonly used words in a language. Examples of stop words in English are ‚Äúa,‚Äù ‚Äúthe,‚Äù ‚Äúis,‚Äù ‚Äúare,‚Äù etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information.*\n\nThere are several reasons why we want to remove stop words, with the most important one being to  **reduce the number of irrelevant words** in text analysis process so that words that contribute more to the context of the text will be given a relatively heavier weight. Other reasons would be to reduce the size of input to **enhance training efficiency**.\n\nOf course, we do not always want to remove these stop words for various reasons such as loss of information etc.\n\n\nFor more information: <br>\nWhy shouldn't we remove stop word:  <br>\nhttps://opensourceconnections.com/blog/2023/01/24/10-reasons-why-you-shouldnt-remove-stop-words/#:~:text=Removing%20stop%20words%20may%20now,retrieved%20in%20the%20first%20step.","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n","metadata":{}},{"cell_type":"code","source":"import spacy\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nprint(\"Number of stop words in spacy:\", len(STOP_WORDS))\n\nprint(\"Some example of stop words:\", list(STOP_WORDS)[0:5])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T10:47:26.655877Z","iopub.execute_input":"2023-08-07T10:47:26.656355Z","iopub.status.idle":"2023-08-07T10:47:26.664291Z","shell.execute_reply.started":"2023-08-07T10:47:26.656321Z","shell.execute_reply":"2023-08-07T10:47:26.662846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(\"Apple and Google lead initiative for an industry specification to address unwanted tracking\")\n\nfiltered_stop_words = []\n\nfor token in doc:\n    if not token.is_stop: # we use token attribute .is_stop\n        filtered_stop_words.append(token.text)\nprint(\"Original: \", doc)\nprint(\"After filtering: \", \" \".join(filtered_stop_words))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T10:49:26.646492Z","iopub.execute_input":"2023-08-07T10:49:26.646905Z","iopub.status.idle":"2023-08-07T10:49:27.922300Z","shell.execute_reply.started":"2023-08-07T10:49:26.646875Z","shell.execute_reply":"2023-08-07T10:49:27.920800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"stemming-and-lemmatization\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üåü Stemming and Lemmatization üåü</span></div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/WbJvTWD.png\" style=\"height:300px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.*\n\nStemming and Lemmatization are very important and fundamental concept in NLP. They both aim to achieve the same goal - **convert words to their 'base form/root meaning'**. In fact, we usually only use either one of stemming and lemmatization, but not both in our NLP task. \n\nThe difference between Stemming and Lemmatization lies in their working principle. \n\nIn stemming, it attempts to achieve the goal by setting a set of rules to alter the words to change them to their base form. Let's see a raw example.","metadata":{}},{"cell_type":"code","source":"input_words = ['playing', 'happy', 'studies', 'plays', 'play']\n\ndef imaginary_stemming_function(token):\n    if token[-3:] == 'ies' or token[-3:] == 'ing':\n        token = token[:-3]\n    if token[-1] == 's':\n        token = token[:-1]\n    return token\n\nfor token in input_words:\n    print(token, '->', imaginary_stemming_function(token))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T11:06:12.777252Z","iopub.execute_input":"2023-08-07T11:06:12.777747Z","iopub.status.idle":"2023-08-07T11:06:12.787234Z","shell.execute_reply.started":"2023-08-07T11:06:12.777711Z","shell.execute_reply":"2023-08-07T11:06:12.785736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope this show a bit of how stemming work. As you can see, stemming **does not have linguistic knowledge**, so although it may be faster compare to lemmatization, there are several problems associated with stemming such as stemming cannot covert 'gave' into 'give'. \n\nIn fact, spaCy does not support stemming, so for illustration purpose, we will be using another popular NLP framework - **NLTK** to illustrate the concept of stemming.","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n","metadata":{}},{"cell_type":"code","source":"# If you never install nltk before\n# pip install nltk","metadata":{"execution":{"iopub.status.busy":"2023-08-07T12:32:13.229097Z","iopub.execute_input":"2023-08-07T12:32:13.229514Z","iopub.status.idle":"2023-08-07T12:32:28.449709Z","shell.execute_reply.started":"2023-08-07T12:32:13.229479Z","shell.execute_reply":"2023-08-07T12:32:28.448375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n\nraw_words = ['give', 'gave', 'giving', 'swim', 'swims', 'swimming']\n\nfor word in raw_words:\n    print(word, \"->\", stemmer.stem(word))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T12:33:49.514391Z","iopub.execute_input":"2023-08-07T12:33:49.514857Z","iopub.status.idle":"2023-08-07T12:33:49.522172Z","shell.execute_reply.started":"2023-08-07T12:33:49.514824Z","shell.execute_reply":"2023-08-07T12:33:49.521290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lemmatization** on the other hand **have linguistic knowledge**, it basically works like a dictionary where each word is somehow linked by some algorithm/model to the base word. Let's illustrate the idea of lemmatization with spaCy.","metadata":{}},{"cell_type":"code","source":"import spacy \n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(\"give gave giving swim swims swimming\")\n\nfor token in doc:\n    print(token, \"->\", token.lemma_, \"/\", token.lemma)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T12:39:17.322337Z","iopub.execute_input":"2023-08-07T12:39:17.322765Z","iopub.status.idle":"2023-08-07T12:39:18.545801Z","shell.execute_reply.started":"2023-08-07T12:39:17.322730Z","shell.execute_reply":"2023-08-07T12:39:18.544466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also add the .lemma_ yourself for special characters such as slang.","metadata":{}},{"cell_type":"code","source":"# Before adding attribute_ruler\n\ndoc = nlp(\"nah no nope\")\n\nfor token in doc:\n    print(token, \"->\", token.lemma_)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T12:41:54.669269Z","iopub.execute_input":"2023-08-07T12:41:54.669707Z","iopub.status.idle":"2023-08-07T12:41:54.685593Z","shell.execute_reply.started":"2023-08-07T12:41:54.669672Z","shell.execute_reply":"2023-08-07T12:41:54.684781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After adding attribute_ruler\n\nar = nlp.get_pipe(\"attribute_ruler\")\n\nar.add([[{\"TEXT\": \"nah\"}],[{\"TEXT\": \"nope\",}]], {\"LEMMA\": \"no\"})\n\ndoc = nlp(\"nah no nope\")\n\nfor token in doc:\n    print(token, \"->\", token.lemma_)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T12:42:21.838038Z","iopub.execute_input":"2023-08-07T12:42:21.838432Z","iopub.status.idle":"2023-08-07T12:42:21.856600Z","shell.execute_reply.started":"2023-08-07T12:42:21.838401Z","shell.execute_reply":"2023-08-07T12:42:21.854750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, the ultimate question, **stemming or lemmatization**. It really depends on how you take your tradeoffs, but in most of the cases, lemmatization is recommended for obvious reasons.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/v0pQ56g.png\" style=\"height:400px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"out-of-vocabulary\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Out of Vocabulary (OOV)</span></div>\n","metadata":{}},{"cell_type":"markdown","source":"The concept of OOV is actually being illustrated in the above examples. OOV is basically what it suggests, words that are out of our domain/model, they can be slangs like 'lol', 'nah' etc. Or if you are training your own model, it can be all the words that didn't appear in your training process. This is an issue that you will face later on when discussing on topics such as Bag of Words and Word Embedding. ","metadata":{}},{"cell_type":"code","source":"import spacy\n\n# Noted that we are using lg model, because sm model itself does not contain a lot of vocab,\n# so most of the words when using sm model will be OOV\nnlp = spacy.load('en_core_web_lg')\n\ndoc = nlp('nah no nope lol lmao yesir swim swimming yes')\n\nfor token in doc:\n    print(token, 'is OOV', token.is_oov)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T12:48:42.746473Z","iopub.execute_input":"2023-08-07T12:48:42.746864Z","iopub.status.idle":"2023-08-07T12:48:48.107069Z","shell.execute_reply.started":"2023-08-07T12:48:42.746833Z","shell.execute_reply":"2023-08-07T12:48:48.105739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result may be surprising, the large model even contain words like 'lol' and 'lmao' but 'yesir' demonstrate the idea of OOV.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"part-of-speech-tagging\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Part of Speech (PoS) Tagging</span></div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/A4jz5TR.png\" style=\"height:300px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"Part of Speech is probably something you learnt back in elementary school, it is very important part of a language. You might wonder, why do we bother to do part of speech tagging? Here is an example:\n\nTogether, they **plant** (verb) a **plant** (noun).\n\nNow you may start to appreciate the importance of part of speech tagging in NLP. A lot of times, especially in English Language, the same word will have very different meaning depending on the PoS, by doing PoS tagging, we aim to allow our model to have a **better understanding of the grammar and to be able to distinguish words that have multiple meaning**. ","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n\n\nLet's see PoS tagging in action with spaCy and NLTK.","metadata":{}},{"cell_type":"code","source":"import spacy\n\nnlp = spacy.load('en_core_web_sm')\n\ndoc = nlp(\"Visualizing Part-of-Speech Tags with NLTK and Spacy\")\n\nfor token in doc:\n    print(f\"{token}: {token.pos_} ({spacy.explain(token.pos_)}) -> {token.tag_} ({spacy.explain(token.tag_)})\")","metadata":{"execution":{"iopub.status.busy":"2023-08-07T13:42:06.517057Z","iopub.execute_input":"2023-08-07T13:42:06.517535Z","iopub.status.idle":"2023-08-07T13:42:07.708283Z","shell.execute_reply.started":"2023-08-07T13:42:06.517498Z","shell.execute_reply":"2023-08-07T13:42:07.707157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization with displacy\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Visualizing Part-of-Speech Tags with NLTK and Spacy\")\ndisplacy.render(doc, style = \"dep\", options={'distance': 100})","metadata":{"execution":{"iopub.status.busy":"2023-08-07T13:33:03.885368Z","iopub.execute_input":"2023-08-07T13:33:03.885843Z","iopub.status.idle":"2023-08-07T13:33:05.128649Z","shell.execute_reply.started":"2023-08-07T13:33:03.885806Z","shell.execute_reply":"2023-08-07T13:33:05.127385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PoS Tagging with NLTK\nimport nltk\nfrom nltk.tokenize import word_tokenize\n \ntext = word_tokenize(\"Visualizing Part-of-Speech Tags with NLTK and Spacy\")\nnltk.pos_tag(text)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T13:56:37.761551Z","iopub.execute_input":"2023-08-07T13:56:37.761997Z","iopub.status.idle":"2023-08-07T13:56:37.933764Z","shell.execute_reply.started":"2023-08-07T13:56:37.761962Z","shell.execute_reply":"2023-08-07T13:56:37.932628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Manually assign tagging with NLTK\n# Tuseful when you want to assign a default or fallback tag to all tokens in a text, \n# regardless of their actual part-of-speech.\n\nfrom nltk.tag import DefaultTagger\n \ntagging = DefaultTagger('NN')\n\ntagging.tag(['Apple', 'Orange'])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T13:57:58.565961Z","iopub.execute_input":"2023-08-07T13:57:58.566437Z","iopub.status.idle":"2023-08-07T13:57:58.574303Z","shell.execute_reply.started":"2023-08-07T13:57:58.566398Z","shell.execute_reply":"2023-08-07T13:57:58.573219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# untagging with NLTK\n# useful when you want to remove the PoS tags from the tagged tokens \n# and work with only the plain word-tokens.\nfrom nltk.tag import untag\ntext = word_tokenize(\"Visualizing Part-of-Speech Tags with NLTK and Spacy\")\ntagged_token = nltk.pos_tag(text)\nuntag(tagged_token)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:01:39.160477Z","iopub.execute_input":"2023-08-07T14:01:39.161383Z","iopub.status.idle":"2023-08-07T14:01:39.171897Z","shell.execute_reply.started":"2023-08-07T14:01:39.161330Z","shell.execute_reply":"2023-08-07T14:01:39.170782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"named-entity-recognition\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üåü Named Entity Recognition (NER) üåü </span></div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/edHXTu9.png\" style=\"height:300px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *Named Entity Recognition (NER) is a task of Natural Language Processing (NLP) that involves **identifying and classifying named entities in a text** into predefined categories such as person names, organizations, locations, and others.*\n\nNER is particularly useful if you would like to locate specific information from a corpus (text), particularly to **find out 'where', 'what', 'who', 'when'** from a paragraph. \n\nNER is being used for wide range of purposes, from recommendation system to search engines and customer support services, it is very useful if you want a quick summary of over a large amount of text. \n\nThere are 4 method of NER, including \n1. **dictionary-based** (a large dictionary containing vocabularies eg. {'Apple', 'Tesla'...} to match each token from the text)\n2. **rule-based** (detect named entities by patterns using techniques such as regular expression)\n3. **machine learning** (train ML models with text that are being labelled)\n4. **deep learning based** (train model to detect the entities by understanding the semantic and syntactic relationship between various words)","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n\nLet's see NER in action with spaCy.","metadata":{}},{"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_lg\")\ndoc = nlp(\"Liverpool confirm the signing of Alexis Mac Allister on cut-price ¬£35m deal\")\nfor ent in doc.ents:\n    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:28:37.011496Z","iopub.execute_input":"2023-08-07T14:28:37.012008Z","iopub.status.idle":"2023-08-07T14:28:40.174167Z","shell.execute_reply.started":"2023-08-07T14:28:37.011966Z","shell.execute_reply":"2023-08-07T14:28:40.172969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is the extracted article, let us see the power of NER with spacy visualization tool displacy.","metadata":{}},{"cell_type":"code","source":"from spacy import displacy\n\ndoc = nlp(\"\"\"\nLiverpool have ticked the first box on their summer rebuilding programme by completing the signing of Alexis Mac Allister.\n\nThe Argentine World Cup winner, who passed a medical in Merseyside on Tuesday, has agreed terms on a deal that will run to June 2028 and an idea of the importance that will be placed on his arrival is the fact he will be given Liverpool's vacant No 10 shirt.\n\nIt is a significant number in Liverpool's history, as it was worn by John Barnes and, more recently, Sadio Mane but it is also significant for Mac Allister to, given the symbolism of the number in Argentina with Diego Maradona and Lionel Messi.\n\nLiverpool have been tracking Mac Allister since before last winter's World Cup. The club have offered no guidance on how much they have paid for the 24-year-old nor are Brighton but Mail Sport understands the fee is in the region of ¬£35million.\n\nIt had initially been expected that it would take more than ¬£60million for Liverpool to secure Mac Allister's release, so this investment ‚Äì on face value ‚Äì makes huge economic sense: it is up to the player to see if the figure become a bargain.\n\"\"\")\n\n\ndisplacy.render(doc, style='ent')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:30:02.522599Z","iopub.execute_input":"2023-08-07T14:30:02.523031Z","iopub.status.idle":"2023-08-07T14:30:02.599208Z","shell.execute_reply.started":"2023-08-07T14:30:02.522997Z","shell.execute_reply":"2023-08-07T14:30:02.598061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, NER allow you to have a quick understanding of large amount of text by labelling named entities. In fact, named entities usually carry most of the information from a text as compared to stop words like 'is', 'the', 'and'. But of course, the use of NER is not limited to just a quick summary as mentioned above.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"summary-1\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üìù Short Summary (Part 1: Preprocessing) üìù</span></div>","metadata":{}},{"cell_type":"markdown","source":"Let's review what we have learnt so far. \n\n### 1. Tokenization\n\nProcess of converting sentences into 'tokens' to assign meanings\n\n### 2. spaCy pipeline\n\nIntroduced spaCy trained pipeline and architecture\n\n### 3. Stop word\n\nSet of commonly used words, sometimes we would like to remove them to reduce noise\n\n\n### 4. Stemming and Lemmatization\n\nProcess of breaking down tokens into their root/base form\n\n### 5. Out of Vocabulary (OOV)\n\nVocabularies that is not covered/trained by the NLP model\n\n### 6. Part of Speech (PoS) Tagging\n\nAssigning part of speech to tokens to allow model to classify words of multiple meaning\n\n### 7. Named Entity Recognition (NER)\n\nTechnique to identify named entity in a corpus (text)","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"practice-1\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">‚úç Do Some Practice and Take a Break ‚úç</span></div>\n\nBefore we move to the next section (feature engineering), it is important that you feel comfortable with the concepts we have mentioned so far. Preprocessing is a prior step to feature engineering, these concepts will be brough up in later sections when necessary. \n\nThe best way to achieve this is through some practice, try to preprocess the competition dataset with the techniques that you have just learnt! \n\nOnce you are ready, let's move on to the next section - Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"bag-of-words\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Bag of Words (BOW)</span></div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/iiIOKHx.png\" style=\"height:200px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *The bag-of-words model is a simplifying representation used in natural language processing and information retrieval. In this model, **a text is represented as the bag of its words**, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.*\n\n### Computer doesn't understandand string/words,\nthat's why we need certain algorithm and method to convert words into numbers that computer understand. One of these techniques is Bag of Word (BOW). \n\nThe idea behind Bag of Word is fairly simple. Basically we would like to **get all unique tokens in all the data you feed in as a column**. For each of the sentence, we assign a counter for each word/column, just like what the above image illustrate. ","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n\nThere are certain limitations associated with BOW, but first let us see BOW in action with sklearn, a popular machine learning framework in Python.","metadata":{}},{"cell_type":"code","source":"# Prepare a very small dataset, you can change the text to test the difference!\nimport pandas as pd\n\ncorpus = [\"Hey there, how's the weather today.\",\n          \"Liverpool announced the signing of Szoboszlai\",\n          \"An apple a day keeps the doctor away.\",\n          \"Natural Language Processing is fun!\"]\n\ntext_df = pd.DataFrame({\"Text\": corpus})\n\ntext_df","metadata":{"execution":{"iopub.status.busy":"2023-08-08T04:59:04.005519Z","iopub.execute_input":"2023-08-08T04:59:04.005961Z","iopub.status.idle":"2023-08-08T04:59:04.027138Z","shell.execute_reply.started":"2023-08-08T04:59:04.005925Z","shell.execute_reply":"2023-08-08T04:59:04.025294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text preprocessing, we will be \n# 1. removing stop words\n# 2. removing punctuations\n# 3. changing word to lower case\n# 4. lemmatization\n\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocessing(text):\n    doc = nlp(text.lower()) # lowercase\n    preprocessed_text = []\n    for token in doc:\n        if token.is_stop or token.is_punct: # if the token is stop word/punctuation we do not add it to the list\n            continue\n        preprocessed_text.append(token.lemma_)\n    \n    return \" \".join(preprocessed_text)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T05:06:17.668804Z","iopub.execute_input":"2023-08-08T05:06:17.669900Z","iopub.status.idle":"2023-08-08T05:06:19.051404Z","shell.execute_reply.started":"2023-08-08T05:06:17.669843Z","shell.execute_reply":"2023-08-08T05:06:19.050202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform our dataset through applying our preprocessing function to the text_df\n\ntext_df[\"Text\"] = text_df[\"Text\"].apply(preprocessing)\ntext_df","metadata":{"execution":{"iopub.status.busy":"2023-08-08T05:06:20.510474Z","iopub.execute_input":"2023-08-08T05:06:20.511657Z","iopub.status.idle":"2023-08-08T05:06:20.560470Z","shell.execute_reply.started":"2023-08-08T05:06:20.511615Z","shell.execute_reply":"2023-08-08T05:06:20.559098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's do our BOW \n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer()\ncv.fit(text_df['Text'])\ncv.vocabulary_ # Each unique token will be assigned as a dictionary with some kind of index","metadata":{"execution":{"iopub.status.busy":"2023-08-08T05:10:10.182210Z","iopub.execute_input":"2023-08-08T05:10:10.182677Z","iopub.status.idle":"2023-08-08T05:10:10.202559Z","shell.execute_reply.started":"2023-08-08T05:10:10.182628Z","shell.execute_reply":"2023-08-08T05:10:10.201271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv.get_feature_names_out() # we can also convert the vocabs to an array of the right order","metadata":{"execution":{"iopub.status.busy":"2023-08-08T05:25:26.612843Z","iopub.execute_input":"2023-08-08T05:25:26.613633Z","iopub.status.idle":"2023-08-08T05:25:26.621643Z","shell.execute_reply.started":"2023-08-08T05:25:26.613594Z","shell.execute_reply":"2023-08-08T05:25:26.619780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To get the vector of a particular corpus, we use the .transform function and turn the result to an array\n\ncv.transform(['hey weather today']).toarray()\n\n# You can see from the results, there are three position that shows '1'\n# which is array[6],array[13],array[14] -> matching the position of the word in cv.vocabulary_","metadata":{"execution":{"iopub.status.busy":"2023-08-08T05:11:35.065081Z","iopub.execute_input":"2023-08-08T05:11:35.065522Z","iopub.status.idle":"2023-08-08T05:11:35.078411Z","shell.execute_reply.started":"2023-08-08T05:11:35.065486Z","shell.execute_reply":"2023-08-08T05:11:35.076150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's visualize the result of countvectorizer\n\ncv = CountVectorizer()\nX = cv.fit_transform(text_df['Text'])\n\nfeature_names = cv.get_feature_names_out() # get all the unique tokens \n\ndf = pd.DataFrame(X.toarray(), columns=feature_names, index=text_df['Text']) # create a pandas dataframe\n\ndf","metadata":{"execution":{"iopub.status.busy":"2023-08-08T05:25:59.786459Z","iopub.execute_input":"2023-08-08T05:25:59.786878Z","iopub.status.idle":"2023-08-08T05:25:59.812178Z","shell.execute_reply.started":"2023-08-08T05:25:59.786846Z","shell.execute_reply":"2023-08-08T05:25:59.810804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I hope you grasp the general idea of how BOW work. Indeed is it pretty simply process and you can use this result dataframe to train your machine learning models for your NLP task. But in reality, there are several huge flaws in using BOW to vectorize the corpus. For instance, it faces the problem of\n\n### 1. **Sparsity** increase as the number of unique token (size of input) increases\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/EdPwBdK.png\" style=\"height:200px;\"> </div>\n<br>\n\nIf you notice, in the example demonstrate above, there are a lot of zeros as the value of each corpus, **the percentage of useful information of each row will decrease as the size of input increase**. Why? Image rather than 4 corpus, we have a million sentences, just image how many zeros will there be in each row!\n\n### 2. **Dimension** increase as the number of unique token (size of input) increases\n\nApart from the problem of sparsity, dimensionality is also a primary concern of BOW. Just like the example mentioned, if we have a million sentences, there will be millions of features/columns in the dataset which make our training process very inefficient.\n\n### 3. BOW cannot handle **Out of Vocabulary (OOV)** problem\n\nImagine we use our example dataframe to train our model and now we want to classifer the sentence \"Machine Learning is awesome\". Our model simply cannot give us an accurate prediction because none of these tokens are in our feature!","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"bag-of-n-grams\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Bag of n-grams</span></div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/QLE08hJ.png\" style=\"height:300px;\"> </div>\n\n","metadata":{}},{"cell_type":"markdown","source":"Just at looking at the imagine above, you may already have a good guess of what bag of n-grams do if you had go through the BOW section. Indeed, bag of n-grams simple increase the number of features/columns by altering between how to split the tokens.","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n\nLet's see bag of n-grams in action with sklearn.","metadata":{}},{"cell_type":"code","source":"# We will be all the dataset from the previous section, so these are just copy and paste\n\n# Dataset from BOW section\nimport pandas as pd\n\ncorpus = [\"Hey there, how's the weather today.\",\n          \"Liverpool announced the signing of Szoboszlai\",\n          \"An apple a day keeps the doctor away.\",\n          \"Natural Language Processing is fun!\"]\n\ntext_df = pd.DataFrame({\"Text\": corpus})\n\n# Preprocess function from BOW section\n\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocessing(text):\n    doc = nlp(text.lower()) # lowercase\n    preprocessed_text = []\n    for token in doc:\n        if token.is_stop or token.is_punct: # if the token is stop word/punctuation we do not add it to the list\n            continue\n        preprocessed_text.append(token.lemma_)\n    \n    return \" \".join(preprocessed_text)\n\ntext_df[\"Text\"] = text_df[\"Text\"].apply(preprocessing)\ntext_df","metadata":{"execution":{"iopub.status.busy":"2023-08-08T05:44:58.479905Z","iopub.execute_input":"2023-08-08T05:44:58.480351Z","iopub.status.idle":"2023-08-08T05:44:59.865950Z","shell.execute_reply.started":"2023-08-08T05:44:58.480314Z","shell.execute_reply":"2023-08-08T05:44:59.864676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\ncv = CountVectorizer(ngram_range=(1,2)) \n# to declare bag of n_grams, we simply change the parameter\n# in this example we will be using bag of one word and bag of two word\n# noted that the parameter is in form of range, so (1,3) will be bag of 1,2,3 words\n\n\n# Same code from previous section\nX = cv.fit_transform(text_df['Text'])\n\nfeature_names = cv.get_feature_names_out() # get all the unique tokens \n\ndf = pd.DataFrame(X.toarray(), columns=feature_names, index=text_df['Text']) # create a pandas dataframe\n\n\n# ----------------------------------------------------------\n# just a function that better display large pandas dataframe\nfrom IPython.display import display, HTML\n\ndef create_scrollable_table(df, table_id, title):\n    html = f'<h3>{title}</h3>'\n    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n    html += df.to_html()\n    html += '</div>'\n    return html\n# ----------------------------------------------------------\n\nhtml_df = create_scrollable_table(df, \n                            'Bag of n-grams', \n                            'Bag of n-grams')\ndisplay(HTML(html_df))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T05:48:59.751562Z","iopub.execute_input":"2023-08-08T05:48:59.752003Z","iopub.status.idle":"2023-08-08T05:48:59.776815Z","shell.execute_reply.started":"2023-08-08T05:48:59.751971Z","shell.execute_reply":"2023-08-08T05:48:59.775225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The difference between Bag of n-grams and Bag of Words is that Bag of n-grams contains more information than BOW because **bag of n-grams capture more context around each word than BOW**. For example, instead of just capturing 'hey', 'weather' and 'today', bag of n-gram captures 'hey', 'hey weather', 'weather', 'today', 'weather today'.\n\nBut also at the same time, the flaws of sparsity and dimensionality as mentioned in the BOW section is also being demonstrated here. While bag of n-grams also cannot deal with Out of Vocabulary (OOV) same as bag of words.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"tf-idf\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üåü Term Frequency - Inverse Document Frequency (TF-IDF)üåü </span></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/4WcIN8I.png\" style=\"height:300px;\"> </div>\n","metadata":{}},{"cell_type":"markdown","source":"When you are navigating through the sections of [Bag of Words](#bag-of-words) and [Bag of n-grams](#bag-of-n-grams), have you notice the fact that they both assign a weight of 1 to each appearance of word in the sentence? This might not make a big difference for our tiny dataset from our examples, but for larger dataset, it may be a problem because imagine words like 'is' will have a value of 100 and important words like 'Tesla' that contains the most information will have a value of 10. When we feed this dataset to our model to train, this will in turn **give words like 'is' a relatively high weighting and reduce the accuracy of the model** as you can imagine words like 'is' will have a very high value also for other sentences.\n\nYou may suggest that we could simply remove these [stop words](#stop-word) in our preprocessing part, but sometimes we do not want to do it and in fact, in this section I will show you a better technique to tackle this problem -- TF-IDF.","metadata":{}},{"cell_type":"markdown","source":"‚ùó *In information retrieval, tf‚Äìidf, short for term frequency‚Äìinverse document frequency, is a numerical statistic that is intended to **reflect how important a word is to a document in a collection or corpus**. It is often **used as a weighting factor** in searches of information retrieval, text mining, and user modeling.*\n\nWhen talking about TF-IDF, we have to introduce some math and statistical concept to better illustrate the concept. But don't get scared, the maths in TF-IDF is relatively easy so stay with me.\n\nLet's break TF-IDF into two parts, term frequency (TF) and inverse document frequency (IDF).\n\n## Term Frequency (TF)\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/RhnP5xc.png\" style=\"height:120px;\"> </div>\n\nThis look a little overwhelming, to put the mathematical equation in words,\n\n`Term Frequency(TF) = number of times the word appeared / total no of words in a document`\n\n### ‚úç Example: \n\nThe best way to learn is always through examples, so let's see an example with this sentence\n\n>> **\"Natural Language Processing is fun Machine Learning is fun\"**\n\n‚ùì Question: Find the term frequency of the word *'is'*\n\n‚úÖ Total number of word in the document: 9 <br>\n‚úÖ number of times *is* appeared in the text: 2 <br>\nüü∞ TF(is): 2/9 \n\nSimilarly, TF(Machine) is what, yes your right, 1/9.\n\nüåü If you think deeper, the range of possible value of TF is 0 to 1, when TF is closer to 1, it means that the word appear more frequently in the text.\n\nNow let's move on to Inverse Document Frequency (IDF)\n\n## Inverse Document Frequency (IDF)\n\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/AvZkDVx.png\" style=\"height:150px;\"> </div>\n<br>\n\nTo put it in words,\n\n`Inverse Document Frequency(IDF) = log(Total number of documents (rows) / Number of documents (rows) containing the word)`\n\n### ‚úç Example: \n\nLet's see IDF in action with an example, imagine we have 2 documents (rows) \n\n>> **1. \"Natural Language Processing is fun Machine Learning is fun\"** <br>\n>> **2. \"This is the best thing I ever had\"**\n\n‚ùì Question: Find the inverse document frequency of the word *'is'*\n\n‚úÖ Total number of documents: 2 <br>\n‚úÖ Number of documents containing the word *is*: 2 <br>\nüü∞ IDF(is): log(2/2) = 0\n\nSimilarly, IDF(Machine) will be log(2/1) which is around 0.7 (note: we usually like to use the natural logrithm log base of e in machine learning and statistics)\n\nüåü If you think deeper, the range of value of IDF is 0 to infinity, but really, if you notice from the example above, what we are doing is to give a smaller weighting to words that appear more frequently across all the documents. Now you see why this is the perfect replacement to simply removing all the stop words, we are achieving similar outcome in a more statistical and robust approach!\n\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/SBwxNWb.png\" style=\"height:300px;\"> </div>\n\n## TF-IDF\n\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/7RDG6tk.png\" style=\"height:200px;\"> </div>\n\n<br>\n\nNow that you understand the concept of TF and IDF, TF-IDF is relatively simple idea. It is basically \n\n`Term frequency (TF) x Inverse document frequency (IDF)`\n\nLet's try to understand this intuitively. \n\nüåü TF-IDF = TF x IDF\n> As TF increase, TF-IDF increase ‚û° **If a word appear in a document more frequently, we give the word a higher weight and consider the word as 'more informative and important'** \n<br>\n<br>\n> As IDF decrease, TF-IDF decrease ‚û° **If a word appear ACROSS documents more frequently, we give the word a lower weight and consider the word as a sort of 'stop word' and less informative to each individual document**","metadata":{}},{"cell_type":"markdown","source":"Now let's see TF-IDF in action with sklearn. I will be using the same corpus as the corpus we used in Bag of Words section to allow us to see the difference and effect in using TF-IDF.","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code ","metadata":{}},{"cell_type":"code","source":"# import the same dataset from the Bag of Word section\nimport pandas as pd\n\ncorpus = [\"Hey there, how is the weather today.\",\n          \"Liverpool announced the signing of Szoboszlai\",\n          \"An apple a day keeps the doctor away.\",\n          \"Natural Language Processing is fun!\"]\n\ntext_df = pd.DataFrame({\"Text\": corpus})\n# ----------------------------------------------------\n# preprocess function copy and paste from Bag of Word section, but we DO NOT remove stop word\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocessing(text):\n    doc = nlp(text.lower()) # lowercase\n    preprocessed_text = []\n    for token in doc:\n        if token.is_punct: # if the token is stop word/punctuation we do not add it to the list\n            continue\n        preprocessed_text.append(token.lemma_)\n    \n    return \" \".join(preprocessed_text)\n\ntext_df[\"Text\"] = text_df[\"Text\"].apply(preprocessing)\n# ----------------------------------------------------\n\n# TF-IDF vectorizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer()\n\nX = tfidf_vectorizer.fit_transform(text_df[\"Text\"])\n\nfeature_names = tfidf_vectorizer.get_feature_names_out() # get all the unique tokens \n\ntfidf_df = pd.DataFrame(X.toarray(), columns=feature_names, index=text_df[\"Text\"]) # create a pandas dataframe\n\n# ----------------------------------------------------------\n# just a function that better display large pandas dataframe\nfrom IPython.display import display, HTML\n\ndef create_scrollable_table(df, table_id, title):\n    html = f'<h3>{title}</h3>'\n    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n    html += df.to_html()\n    html += '</div>'\n    return html\n# ----------------------------------------------------------\n\nhtml_df = create_scrollable_table(tfidf_df, \n                            'TF-IDF', \n                            'TF-IDF')\ndisplay(HTML(html_df))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T08:49:16.987907Z","iopub.execute_input":"2023-08-08T08:49:16.988359Z","iopub.status.idle":"2023-08-08T08:49:18.442775Z","shell.execute_reply.started":"2023-08-08T08:49:16.988322Z","shell.execute_reply":"2023-08-08T08:49:18.441182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copy of what we did in Bag of Word Section\n\nimport pandas as pd\n\ncorpus = [\"Hey there, how is the weather today.\",\n          \"Liverpool announced the signing of Szoboszlai\",\n          \"An apple a day keeps the doctor away.\",\n          \"Natural Language Processing is fun!\"]\n\ntext_df = pd.DataFrame({\"Text\": corpus})\n# ----------------------------------------------------\n# preprocess function copy and paste from Bag of Word section\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocessing(text):\n    doc = nlp(text.lower()) # lowercase\n    preprocessed_text = []\n    for token in doc:\n        if token.is_punct or token.is_stop: # if the token is stop word/punctuation we do not add it to the list\n            continue\n        preprocessed_text.append(token.lemma_)\n    \n    return \" \".join(preprocessed_text)\n\ntext_df[\"Text\"] = text_df[\"Text\"].apply(preprocessing)\n# ----------------------------------------------------\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer()\nX = cv.fit_transform(text_df['Text'])\n\nfeature_names = cv.get_feature_names_out() # get all the unique tokens \n\ndf = pd.DataFrame(X.toarray(), columns=feature_names, index=text_df['Text']) # create a pandas dataframe\n\n\n# ----------------------------------------------------------\n# just a function that better display large pandas dataframe\nfrom IPython.display import display, HTML\n\ndef create_scrollable_table(df, table_id, title):\n    html = f'<h3>{title}</h3>'\n    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n    html += df.to_html()\n    html += '</div>'\n    return html\n# ----------------------------------------------------------\n\nhtml_df = create_scrollable_table(df, \n                            'Bag-of-Words', \n                            'Bag-of-Words')\ndisplay(HTML(html_df))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T09:12:32.099587Z","iopub.execute_input":"2023-08-08T09:12:32.100168Z","iopub.status.idle":"2023-08-08T09:12:33.597269Z","shell.execute_reply.started":"2023-08-08T09:12:32.100126Z","shell.execute_reply":"2023-08-08T09:12:33.595467Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you see the purpose and power of TF-IDF, it's purpose is more clear when you work on larger dataset. But you can see that the weighting on words like 'the' is relatively lower compare to other words for the result in TF-IDF. While for bag of words, it has the same weight for all the words. (Noted that if the word appear twice in a row for bag of word, the weight will be 2, it appears that all weight is 1 right now since all word only appeared once in each df)","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"word-embeddings\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üåü Word Embeddings üåü</span></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/FFdoj7s.png\" style=\"height:400px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *Word embedding or word vector is an approach with which we represent documents and words. It is defined as a **numeric vector input that allows words with similar meanings to have the same representation**. It can approximate meaning and represent a word in a lower dimensional space.*\n\nProbably THE MOST IMPORTANT section throughout the notebook and in feature engineering process in NLP. Word embedding is a very important concept in NLP which is basically a way to 'vectorize' words into word vectors (if you don't know what's a vector, imagine it as a one dimensional array).\n\nBut the concept of word embedding is not simply to assign a random vector to a word, but assign the vector to each word in a way so that words of similar meaning will have a similar vector and have a closer distance in the vector space.\n\nIn fact, techniques we have mentioned just now like [TF-IDF](#TF-IDF) is in fact a form of word embedding, but this section and the following sections will focus on more advance and complex word embedding strategies and methods particularly involving deep learning. \n<br>\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/jsSEAHK.png\" style=\"height:300px;\"> </div>\n\n<br>\n\nBut before we dive into the concepts and theory of word embeddings its related concepts and techniques like cosine similarity, I want to take another approach and show you how a word vector look like using spaCy.","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code ","metadata":{}},{"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_lg\")\n\ndoc = nlp(\"Natural Language Processing is fun asdasdasd\")\n\nfor token in doc:\n    print(f\"Does the word {token} have a vector in spacy? {token.has_vector}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-08T09:29:03.195667Z","iopub.execute_input":"2023-08-08T09:29:03.196184Z","iopub.status.idle":"2023-08-08T09:29:10.082748Z","shell.execute_reply.started":"2023-08-08T09:29:03.196143Z","shell.execute_reply":"2023-08-08T09:29:10.081783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vector of the word \"Natural\"\ndoc[0].vector","metadata":{"execution":{"iopub.status.busy":"2023-08-08T09:30:03.753705Z","iopub.execute_input":"2023-08-08T09:30:03.754170Z","iopub.status.idle":"2023-08-08T09:30:03.768803Z","shell.execute_reply.started":"2023-08-08T09:30:03.754134Z","shell.execute_reply":"2023-08-08T09:30:03.766293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc[0].vector.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-08T09:30:27.348014Z","iopub.execute_input":"2023-08-08T09:30:27.348445Z","iopub.status.idle":"2023-08-08T09:30:27.357332Z","shell.execute_reply.started":"2023-08-08T09:30:27.348411Z","shell.execute_reply":"2023-08-08T09:30:27.355821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In fact, it is very common for word vector to have a length of 300.\n\nNow you might be confused, where does the vectors come from? For now, all you need to know is that the spaCy pipeline provide you with the word vectors of the tokens, which is, they have build models and algorithms to train and got the word vectors and allow users like us to use these vectors.","metadata":{}},{"cell_type":"code","source":"doc = nlp(\"good happy sad bad\")\n\ngood = doc[0]\n\nfor token in doc:\n    print(f\"Similarity between good and {token}: {token.similarity(good)}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-08T09:34:16.035122Z","iopub.execute_input":"2023-08-08T09:34:16.035629Z","iopub.status.idle":"2023-08-08T09:34:16.061749Z","shell.execute_reply.started":"2023-08-08T09:34:16.035592Z","shell.execute_reply":"2023-08-08T09:34:16.059698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üåü It is important to note that when we say that two word embeddings are similar, we mean that **the two vectors representing the embeddings are close to each other in some sense, such as geometric distance or cosine similarity**. We are **not only refering to how similar is their meaning**, but depending on the training method, it can be the likelihood of two words to appear in the same context.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"cosine-similarity\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üåü Cosine Similarity üåü</span></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/GhmO2iq.jpg\" style=\"\"> </div>","metadata":{}},{"cell_type":"markdown","source":"We have mentioned a lot about cosine similarity in our previous section. But what exactly is cosine similarity? \n\n‚ùó *Cosine similarity **measures the similarity between two vectors of an inner product space**. It is **measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction**. It is often used to measure document similarity in text analysis.*\n\n### The content of this section is heavily inspired by this video by \n\n### StatQuest: https://www.youtube.com/watch?v=e9U0QAFbfLI \n\n### It is highly recommended that you watch this 10 minutes video for awesome animations and a more detailed explanations. Below will be an extraction of content from the video.\n\nOne of the main feature of cosine similarity is that it is a metric that can be used regardless of how large and high dimension the dataset is, that is, we can still compute the angle between two words no matter how many words and dimensionality there are, unlike other metrics such as the Euclidean distance approach (calculate distance btween two points) which may not work well when the dataset and dimension is huge. \n\nBefore we dive into the maths, let's see a very simple example to let us have an intuitive understanding of how cosine similarity work with the following example from the StatQuest video.","metadata":{}},{"cell_type":"markdown","source":"### ‚úç Example (Image and example taken from the StatQuest video)\n\nIn particular, let us explore how do we find the cosine similarity between the phrase \"Hello\" and \"Hello World\". \n\n**1. The first step we would do is to create a countvectorizer similar to the idea of [bag of words](#bag-of-words).**\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/OhZfQkx.png\" style=\"height: 150px;\"> </div>\n\n<br>\n\n**2. Now we plot it on a 2D graph, with the value of axis being each unique token**\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/ijzhyc2.png\" style=\"height: 300px;\"> </div>\n\n**3. Calculate the cosine similarity, in this case, it is simply the cosine of the angle between the lines of the two point to the origin**\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/FWGI09l.png\" style=\"height: 300px;\"> </div>\n\nI hope you grasp the general idea behind cosine similarity with these awesome animations. In fact, this following example illustrate the aforementioned point that 'it is a metric that can be used regardless of how large and high dimension the dataset is' which tackle the flaws of metrics such as Eucliden distance approach.\n\nLet's see this example which demonstrate the cosine similarity of the phrase \"Hello Hello Hello\" and \"Hello World\".\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/5DoUown.png\" style=\"height: 300px;\"> </div>\n\n<br>\n\nThis demonstrates that cosine similarity *is determined entirely by the angle between the lines and not by the lengths of the line*.\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/5DoUown.png\" style=\"height: 300px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"### ‚ùó The greater the value of cosine similarity, the more similar two vectors are to each other\n\n(in general, value of cosine similarity can be between -1 and 1, but for word embedding, it is always positive for some reason: https://vaibhavgarg1982.medium.com/why-are-cosine-similarities-of-text-embeddings-almost-always-positive-6bd31eaee4d5#:~:text=Going%20by%20the%20definition%20of,the%20smallest%20score%20was%200.4522.)\n\nThe above examples clearly explained the basic concept of cosine similarity. But it will not work when there are more than 2 unique tokens, ie. when there is a higher dimension. To deal with this problem, we need to utilize the mathematical equation of cosine similarity.\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/IxqVJiK.png\" style=\"height: 200px;\"> </div>\n","metadata":{}},{"cell_type":"markdown","source":"> cosine similarity = dot product of AxB / magnitude of A x magniture of B\n\nLet's see the formula in action with an example with these two phrases\n\n> **1. natural language processing is fun** <br>\n> **2. machine learning is fun**","metadata":{}},{"cell_type":"code","source":"# Implement count vectorizer (for implementation details, view Bag of Word section)\n\nimport pandas as pd\n\ncorpus = [\"natural language processing is fun\",\n          \"machine learning is fun\"]\n\ntext_df = pd.DataFrame({\"Text\": corpus})\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer()\nX = cv.fit_transform(text_df['Text'])\n\nfeature_names = cv.get_feature_names_out() # get all the unique tokens \n\ndf = pd.DataFrame(X.toarray(), columns=feature_names, index=text_df['Text']) # create a pandas dataframe\n\ndf","metadata":{"execution":{"iopub.status.busy":"2023-08-08T13:24:17.000004Z","iopub.execute_input":"2023-08-08T13:24:17.002315Z","iopub.status.idle":"2023-08-08T13:24:17.033233Z","shell.execute_reply.started":"2023-08-08T13:24:17.002261Z","shell.execute_reply":"2023-08-08T13:24:17.032020Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see how the maths work, let's first focus on the **numerator**\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/I6XXRjc.png\" style=\"height: 100px;\"> </div>\n\nIf you know your linear algebra, you will know that this is simply the dot product of vector A and vector B, ie. the dot product of \\[1,1,1,0,0,1,1\\] and \\[1,1,0,1,1,0,0\\], which if we expand it, it will become\n\n> (1x1) + (1x1) + (1x0) + (0x1) + (0x1) + (1x0) + (1x0)\n\nNow you seeing where it goes right? The first (1x1) corresponds to the word 'fun' where this one in (*1*x1) is the corresponding value of vector A and the one in (1x*1*) corresponding value of vector B for the word 'fun'. We then do summation all the unique token to get the final result.\n\nNow let us look at the **denominator**\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/EGPoj5Z.png\" style=\"height: 100px;\"> </div>\n\n\nNow that you have an idea of what the notations refers to, this should be very easy, in this example, this will simply be\n\n> $\\sqrt{(1^2 + 1^2 + 1^2 + 0^2 + 0^2 + 1^2 + 1^2)}$ x $\\sqrt{(1^2 + 1^2 + 0^2 + 1^2 + 1^2 + 0^2 + 0^2)}$\n\nWhere the first part of the square root is the summation of each value in vector A and similar for vector B\n\n\nSo the overall equation will be \n\n$\\frac{(1*1) + (1*1) + (1*0) + (0*1) + (0*1) + (1*0) + (1*0)}{\\sqrt{(1^2 + 1^2 + 1^2 + 0^2 + 0^2 + 1^2 + 1^2)}  *  \\sqrt{(1^2 + 1^2 + 0^2 + 1^2 + 1^2 + 0^2 + 0^2)}}$\n\nAnd if you do the maths the answer will be roughly equal to 0.4472\n\nIn fact we can easily verify it using sklearn","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n\nUsing cosine_similarity in Python is a relatively simple task, you simply need to import it from the sklearn library","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nA = np.array([1,1,1,0,0,1,1]) # natural language processing is fun\nB = np.array([1,1,0,1,1,0,0]) # machine learning is fun\n\ncosine_similarity([A], [B])","metadata":{"execution":{"iopub.status.busy":"2023-08-08T13:44:44.537022Z","iopub.execute_input":"2023-08-08T13:44:44.537453Z","iopub.status.idle":"2023-08-08T13:44:44.547351Z","shell.execute_reply.started":"2023-08-08T13:44:44.537419Z","shell.execute_reply":"2023-08-08T13:44:44.546397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"arithmetic-of-word-embeddings\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Arithmetic of Word Embeddings</span></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/6Z1kIR8.png\" style=\"height: 350px;\"> </div>\n","metadata":{}},{"cell_type":"markdown","source":"Before we move on with more fancy concepts and algorithms, I want to show you some awesome thing word vectors can do, which is the arithmetic of word embeddings.\n\nAs we mentioned, word embeddings are just some vectors, so naturally we can do some kind of mathematical operations with them. The beauty of word embeddings lies exactly here, let's take an example from the image above.\n\n### ‚úç Example: King ü§¥ - Man üë±‚Äç‚ôÇÔ∏è + Woman üë© = Queen üë∏\n\n> Vector of King: \\[0,0,1\\] <br>\n> Vector of Man: \\[0,0,0\\] <br>\n> Vector of Woman: \\[1,0,0\\] <br>\n> Vector of Queen: \\[1,0,1\\] <br>\n\nNow when we do the maths, we can indeed verify that King - Man + Woman = Queen. How amazing! How does this happen? (look at the emoji to have a more intuitive understanding!)\n\n### Did this question ever hit you while you are surfing the previous sections -\n### >> What do each value of a word vector mean? How do we determine how the vector should be like to achieve this amazing outcome?\n\nThe above picture explain a bit. For each word vector, each column have a meaning, which is Feminity, Youth, Royalty. And according to these **semantics** (relating to meaning in language or logic), we attempt to assign value based on these meanings for each word. \n\nIn reality, like the vector we saw in spaCy library, we actually do not know what each column means, not that the information is not disclosed, but that we actually have no clue what each column represents. All we know is that each column has its own **meaning** , it is this **meaning** that allow the magic we saw in word vectors like similarity and clusterings of similar words. How does this happen, you may ask. Well this is highly due to the method and algorithms we use in training these word embeddings which is what will be introduced in the following section.\n","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"cbow\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Continuous Bag of Words Model(CBOW)</span></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/J3eyyY3.png\" style=\"height: 450px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"### üõë Hold up - this section may not be beginner friendly\nThis part and the next section will involve the concept of neural network, which can be tough for beginners. I will try to explain the concepts in the most beginner friendly way possible in the perspective of a beginner, which is, I will not be explaining the maths and calculus involved in backpropagation and the details inside the neural network. In fact, I do recommend you to have a basic understanding of how neural network work (though not necessary) before moving on. Or, if you are not interested in the concept behind how the magic happens, you can comfortably continue from [Word2Vec](#word2vec) if you do not really care about the theory.\n\nHere are some resources you can use to have a good understanding of neural network\n\n### üìë Resources:\n\nConcept behind neural network with StatQuest:\n\n> https://www.youtube.com/watch?v=CqOfi41LfDw\n\n\nUnderstanding neural network in Medium:\n\n> https://towardsdatascience.com/understanding-neural-networks-19020b758230\n> https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf\n","metadata":{}},{"cell_type":"markdown","source":"‚ùó *CBOW (Continuous Bag of Words) model **predicts a target word based on the context of the surrounding words in a sentence or text**. It is trained using a **feedforward neural network** where the **input is a set of context words, and the output is the target word**.*\n\n\n### üö® Important\n\nBefore we move on to how the neural network work, it is a very important concept that you understand that the word embeddings are a **SIDE PRODUCT** of our neural network. In general, the underlying principle of CBOW is that we \n\n#### **create a fake problem** ‚û° **solve it using neural network** ‚û° **get the word vectors as side products**\n\nIn CBOW, the 'fake problem' is that we try to use neural network to predict a word. In particular, we try to predict the target word from a set of context words. Let's take an example based on the image shown.\n\n### ‚úç Example\n\n> **I like to eat apple**\n\n>> **Input**: I, like, eat, apple <br>\n>> **Target output**: to\n\nSo basically in our neural network, there will be four inputs which are 'I', 'like', 'eat', 'apple' (in terms of vector) and then they will be passed through the hidden layer and predict the word 'to' (in terms of vector). The neural network will not predict the right outcome base off the first try, so the neural network will take some kind of metrics (loss) to calculate the error and use a technique call backpropagation to keep repeating the process to adjust the weight and bias to keep getting a closer step to the right output. In the end, the weight and bias is the 'side product' which is the word embedding we want to obtain. \n\nConfusing right? Don't worry, here is an example taken from this video from the channel codebasics: https://www.youtube.com/watch?v=hQwFeIupNP0 \n\nIn the example, we try to predict the word 'king' from the phrase 'king ordered his minister'\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/LnIQP0r.png\" style=\"height: 300px;\"> </div>\n\n<br>\n\nNow we pass the input to the neural network,\n\n<br>\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/yucngTz.png\" style=\"height: 400px;\"> </div>\n\n<br>\n\nThe concept is that we will keep training the model again and again by adjusting the weights (image below) and bias connecting different layers and in the end, the weights obtained will be our side product, the word embeddings of the word 'king'\n\n<br>\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/NmJkzsm.png\" style=\"height: 400px;\"> </div>\n\n<br>\n\nI hope I have explained the concept clearly, if you still don't understand clearly, check out the video I attached above, codebasics definitely did a better job in explaining the idea than I did. Or if you are really interested and want to clear your concepts entirely, check out the resources I attached below for a deeper and clearer understanding.\n\n\n### üåü Small wrap up\n\nIf you are confused and overwhelmed by my explaining above, don't worry, the whole idea behind CBOW is actually very simple. It is just some **fancy model and algorithm that give us the word embeddings as side product** which is what we care about. \n\nOne thing to note that I did not include any coding part in this section since like I mentioned in the beginning, this is a beginner friendly notebook that aims to give you an overview on the topic and I do not want to future complicate this by introducing more complex frameworks and stuff like that. If you are interested in the coding behind CBOW, here are some resources that you may find it to be useful.\n\n### üìë Resources:\n\nüåü Deeper understanding of the neural network behind CBOW \n\nhttps://www.youtube.com/watch?v=viZrOnJclY0\n\nCBOW with PyTorch: \n\nhttps://srijithr.gitlab.io/post/word2vec/\n\nhttps://www.youtube.com/watch?v=Rqh4SRcZuDA\n\nWord Embeddings using keras embedding layer:\n\nhttps://www.youtube.com/watch?v=Fuw0wv3X-0o","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"skipgram\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">SkipGram Model</span></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/IcQ3LkP.png\" style=\"height: 300px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *The skip-gram model is a method for learning word embeddings, which are continuous, dense, and low-dimensional representations of words in a vocabulary. It is trained using large amounts of unstructured text data and can capture the context and semantic similarity between words.*\n\nIf you understand the concept of CBOW, the idea of SkipGram is very similar, it basically just reverse the order of input and target output. Instead of using surrounding context to predict a target word, **SkipGram uses a word to predict the surrounding context.**\n\nIn the same way, the word embeddings are the side product from the task of predicting surrounding context through neural network just like CBOW. ","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"word2vec\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">Word2Vec</span></div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a **model can detect synonymous words or suggest additional words for a partial sentence**. Word2vec is not a singular algorithm, rather, it is **a family of model architectures and optimizations** that can be used to learn word embeddings from large datasets.*\n\nIn fact, the algorithm behind Word2Vec is exactly [CBOW](#cbow) and [SkipGram](#skipgram). If you still do not understand the magic behind these algorithm, don't worry! It does not affect what we are going to do next, which is to try out some popular word2vec model, particularly we will be using **Gensim** - a free open-source Python library for representing documents as semantic vectors. \n\n\nBefore we move on, make you sure you a good understanding of [word embeddings](#word-embeddings) as it is an important concept you should know before moving on.\n\n### üö® Important\n\nIn this section, we will be using the standard 'word2vec-google-news-300' model which is almost 2GB worth of memory. I have uploaded the model as an input in this kaggle notebook to save you from needing to download 2GB of data on your computer. \n\nBut I have also included a guideline below to help you to download the gensim word2vec-google-news-300 model on your local computer if you would like to do so.\n\nLearn more about gensim model: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n\nAll available models you can download with gensim: \nhttps://github.com/RaRe-Technologies/gensim-data","metadata":{}},{"cell_type":"markdown","source":"### üîé Downloading gensim word2vec-google-news-300 model locally \n\n#### (optional, alternatively you can scroll down to continue the tutorial using the predownloaded model)","metadata":{}},{"cell_type":"code","source":"# !pip install gensim \n# if you never download gensim before","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:01:09.929175Z","iopub.execute_input":"2023-08-09T03:01:09.929522Z","iopub.status.idle":"2023-08-09T03:01:09.955513Z","shell.execute_reply.started":"2023-08-09T03:01:09.929494Z","shell.execute_reply":"2023-08-09T03:01:09.954756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have commented the following part out to save the saving time, uncomment them if you want to download it yourself.","metadata":{}},{"cell_type":"code","source":"# import gensim.downloader as api\n# word2vec_model = api.load('word2vec-google-news-300') \n\n\n# # the model will be downloaded if you never download it before\n# # approximately 2GB of data \n# # it WILL take up memory in your computer if you download it for the first time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Once you have ran api.load('word2vec-google-news-300') once, it will download the model on your local computer. Unless you delete it or change it's name, the next time you want to use the model, you can simply do the following\n\nNote: To find the directory (folder) in your local computer, gensim typical store it in the following locations.\n\n> Windows: C:\\Users\\YourUsername\\gensim-data <br>\n> Linux: /home/YourUsername/gensim-data <br>\n> macOS: /Users/YourUsername/gensim-data <br>\n ","metadata":{}},{"cell_type":"code","source":"# from gensim.downloader import load\n\n# word2vec_model = load(\"word2vec-google-news-300\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:18:21.768418Z","iopub.execute_input":"2023-08-09T03:18:21.769098Z","iopub.status.idle":"2023-08-09T03:19:06.475694Z","shell.execute_reply.started":"2023-08-09T03:18:21.769066Z","shell.execute_reply":"2023-08-09T03:19:06.474227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ü•≥ Using the pre-downloaded model in the notebook\n\n(if you download the code instead of copy and edit, you should follow the above intructions to download the model locally)","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec-google-news-300/word2vec-google-news-300/word2vec-google-news-300/GoogleNews-vectors-negative300.bin', binary=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:39:55.555376Z","iopub.execute_input":"2023-08-09T03:39:55.556085Z","iopub.status.idle":"2023-08-09T03:40:24.499737Z","shell.execute_reply.started":"2023-08-09T03:39:55.556035Z","shell.execute_reply":"2023-08-09T03:40:24.498360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code ","metadata":{}},{"cell_type":"code","source":"# retrieve vocabularies from the model\n# assumed you already imported the model from either one of the above methods\n\nfor index, word in enumerate(word2vec_model.index_to_key):\n    if index == 10:\n        break\n    print(f\"word #{index}/{len(word2vec_model.index_to_key)} is {word}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:40:37.121914Z","iopub.execute_input":"2023-08-09T03:40:37.122403Z","iopub.status.idle":"2023-08-09T03:40:37.130460Z","shell.execute_reply.started":"2023-08-09T03:40:37.122369Z","shell.execute_reply":"2023-08-09T03:40:37.128695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtaining vector from the model\n\nvec_king = word2vec_model['king']\nlen(vec_king) # you can see that the model also uses a vector with length of 300 (aka they used 300 nodes in the hidden layer) ","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:49:27.154099Z","iopub.execute_input":"2023-08-09T03:49:27.154426Z","iopub.status.idle":"2023-08-09T03:49:27.161300Z","shell.execute_reply.started":"2023-08-09T03:49:27.154403Z","shell.execute_reply":"2023-08-09T03:49:27.159949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's try some of the arithmetic of word embedding (view the section: Arithmetic of Word Embeddings for more info)\n\nvec_king = word2vec_model['king']\nvec_man = word2vec_model['man']\nvec_woman = word2vec_model['woman']\nvec_queen = word2vec_model['queen']\n\narithmetic_queen = vec_king - vec_man + vec_woman\n\n# we will be using cosine_similarity as metrics\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_similarity([arithmetic_queen], [vec_woman])","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:54:47.983693Z","iopub.execute_input":"2023-08-09T03:54:47.984116Z","iopub.status.idle":"2023-08-09T03:54:48.334515Z","shell.execute_reply.started":"2023-08-09T03:54:47.984083Z","shell.execute_reply":"2023-08-09T03:54:48.333285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wv build in approach for king + woman - man\n# where positive means + king + woman\n# negative means - man\n\nword2vec_model.most_similar(positive=['king', 'woman'], negative=['man'])","metadata":{"execution":{"iopub.status.busy":"2023-08-09T04:01:05.335784Z","iopub.execute_input":"2023-08-09T04:01:05.336236Z","iopub.status.idle":"2023-08-09T04:01:06.612844Z","shell.execute_reply.started":"2023-08-09T04:01:05.336198Z","shell.execute_reply":"2023-08-09T04:01:06.612100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result of the two approach show a pretty wide difference, I am guessing it is because we use cosine similarity as metrics in our manual approach while wv.most_similar uses different metrics, I am not entirely sure also, if anyone know the reason behind, please let me know.","metadata":{}},{"cell_type":"code","source":"# Note that postive and negative are optional parameters\nword2vec_model.most_similar(['king'])","metadata":{"execution":{"iopub.status.busy":"2023-08-09T04:21:03.810495Z","iopub.execute_input":"2023-08-09T04:21:03.810934Z","iopub.status.idle":"2023-08-09T04:21:04.003434Z","shell.execute_reply.started":"2023-08-09T04:21:03.810899Z","shell.execute_reply":"2023-08-09T04:21:04.002214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs = [\n    ('king', 'queen'), # both have some kind of authority\n    ('man', 'woman'),  # both are genders  \n    ('king', 'man'),   # both are in some way 'male'\n    ('woman', 'queen'), # both are in some way 'female'\n    ('king', 'woman'), # doesn't seem like they have a lot in common\n    ('queen', 'man'), # doesn't seem like they have a lot in common\n]\nfor w1, w2 in pairs:\n    print(f\"{w1} <> {w2}: they are {word2vec_model.similarity(w1, w2)} similar\") \n\n# use wv.similarity(word1, word2) to get the similarity of two word\n# the higher the number the more similar they are\n\n# Look at the result below, recall that when we do wv.most_similar(['king'])\n# we get this result ('queen', 0.6510956883430481) which same as what we observe below!","metadata":{"execution":{"iopub.status.busy":"2023-08-09T04:18:11.440181Z","iopub.execute_input":"2023-08-09T04:18:11.440596Z","iopub.status.idle":"2023-08-09T04:18:11.447823Z","shell.execute_reply.started":"2023-08-09T04:18:11.440554Z","shell.execute_reply":"2023-08-09T04:18:11.447036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wv.doesnt_match to see which item in the list does not match with the rest of the items\n\n# in this example, other than 'car' the rest is some form of food\nprint(word2vec_model.doesnt_match(['apple', 'banana', 'orange', 'car', 'cheese', 'juice']))","metadata":{"execution":{"iopub.status.busy":"2023-08-09T04:19:47.796088Z","iopub.execute_input":"2023-08-09T04:19:47.796509Z","iopub.status.idle":"2023-08-09T04:19:47.804522Z","shell.execute_reply.started":"2023-08-09T04:19:47.796482Z","shell.execute_reply":"2023-08-09T04:19:47.802915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sometimes the word you pass in doesn't have a vector\n# for more info, review the section: Out of Vocabulary (OOV)\n\ntry:\n    vec_cameroon = word2vec_model['asdasd']\nexcept KeyError:\n    print(\"The word 'asdasd' does not appear in this model\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T04:23:04.851253Z","iopub.execute_input":"2023-08-09T04:23:04.851653Z","iopub.status.idle":"2023-08-09T04:23:04.858730Z","shell.execute_reply.started":"2023-08-09T04:23:04.851626Z","shell.execute_reply":"2023-08-09T04:23:04.857063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"fasttext\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">fastText</span></div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *fastText is a **library for learning of word embeddings and text classification** created by Facebook's AI Research lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages.*\n\nBasically, fastText is similar to Word2Vec as in they both are open source models that you can use for word embeddings and use them for NLP related tasks.\n\nfastText offers a lot of models in a lot of different languages you can install locally on your computer. In this notebook, we will be using the fastText English model, please note that the model is even larger than the gensim word2vec-google-news-300 model, it take up 7.24GB of space in your local computer. Below are two methods you can use to use the model in your kaggle notebook, which is to download the model locally, or import it from kaggle directly. Alternatively, you can just read through the code without installing the model if you feel like it.","metadata":{}},{"cell_type":"markdown","source":"### 1Ô∏è‚É£ Downloading fasttext-english-300 locally to your computer\n\n1. Visit this link https://fasttext.cc/docs/en/crawl-vectors.html\n\n2. Scroll down until you see the model list and choose your desire language to download (download the bin file)\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/RnDawj1.png\" style=\"height: 300px;\"> </div>\n\n\n3. Put the downloaded model in a directory and unzip the file, in the end you should get a bin file similar to this\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/iDzt8Yx.png\" style=\"height: 100px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"### 2Ô∏è‚É£ Import it from Kaggle\n\n1. Visit this site https://www.kaggle.com/datasets/bzhbzh35/fasttext-english-300, which is the same model as the one you download it locally from the official website (speaking this as in 9/8/2023)\n\n2. On the left side of your Kaggle notebook workspace, you should see a place where it says +Add Data\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/xEoW87W.png\" style=\"height: 100px;\"> </div>\n\n3. Since you just visited the site above, the first result should be the 'FastText English 300' dataset, add it into your workspace and you are good to go\n","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n\nNow let's move on to the coding part","metadata":{}},{"cell_type":"code","source":"## if you have not install fasttext\n\n# pip install fasttext ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext\n\n#if you install it locally, the PATH should be the PATH of your local computer that lead to the model\nPATH = '/kaggle/input/fasttext-en-model/cc.en.300.bin' \nft_model = fasttext.load_model(PATH)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:54:01.771685Z","iopub.execute_input":"2023-08-09T05:54:01.772547Z","iopub.status.idle":"2023-08-09T05:54:16.391024Z","shell.execute_reply.started":"2023-08-09T05:54:01.772505Z","shell.execute_reply":"2023-08-09T05:54:16.389628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to get word vector in fasttext\nlen(ft_model.get_word_vector('king')) # again we see the vector has length of 300","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:56:49.365200Z","iopub.execute_input":"2023-08-09T05:56:49.365675Z","iopub.status.idle":"2023-08-09T05:56:49.373370Z","shell.execute_reply.started":"2023-08-09T05:56:49.365641Z","shell.execute_reply":"2023-08-09T05:56:49.372277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I thought it would be interesting to compare the word vector of the same word for different model, let's see what we got, what do you expect the result to be?","metadata":{}},{"cell_type":"code","source":"#import the word2vec model (see section: Word2Vec for more info)\nfrom gensim.models import KeyedVectors\n\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec-google-news-300/word2vec-google-news-300/word2vec-google-news-300/GoogleNews-vectors-negative300.bin', binary=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:57:22.157947Z","iopub.execute_input":"2023-08-09T05:57:22.158393Z","iopub.status.idle":"2023-08-09T05:58:44.205840Z","shell.execute_reply.started":"2023-08-09T05:57:22.158359Z","shell.execute_reply":"2023-08-09T05:58:44.203463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\nprint(\"Cosine similarity of 'king':\", cosine_similarity([word2vec_model['king']], [ft_model.get_word_vector('king')]))\n\nprint((word2vec_model['king'] - ft_model.get_word_vector('king'))[:5]) # get the difference in vector for the first 5 values","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:00:29.432909Z","iopub.execute_input":"2023-08-09T06:00:29.433549Z","iopub.status.idle":"2023-08-09T06:00:29.443396Z","shell.execute_reply.started":"2023-08-09T06:00:29.433506Z","shell.execute_reply":"2023-08-09T06:00:29.441912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! Does the results surprise you? Indeed, the word embeddings from different model is very different from each other! What about their relative performance? ","metadata":{}},{"cell_type":"code","source":"ft_model.get_nearest_neighbors('king')","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:02:16.171359Z","iopub.execute_input":"2023-08-09T06:02:16.171905Z","iopub.status.idle":"2023-08-09T06:02:32.734254Z","shell.execute_reply.started":"2023-08-09T06:02:16.171865Z","shell.execute_reply":"2023-08-09T06:02:32.733281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we compare it with the result we got from word2vec gensim model (see more [word2vec](#word2vec)), the first few results does look quite similar, even thought their vector is completely different!\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/fkKBBiR.png\" style=\"height: 300px;\"> </div>","metadata":{}},{"cell_type":"code","source":"# .get_analogies function\n# takes in a word triplet like (\"berlin\", \"germany\", \"france\") and provide analogies\n# berlin is capital of germany -> so we would expect 'paris' as the answer as paris the capital of france\n\nft_model.get_analogies(\"berlin\", \"germany\", \"france\")\n\n# it's really similar idea to berlin - germany + france = paris","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:13:42.095635Z","iopub.execute_input":"2023-08-09T06:13:42.096080Z","iopub.status.idle":"2023-08-09T06:13:43.122223Z","shell.execute_reply.started":"2023-08-09T06:13:42.096015Z","shell.execute_reply":"2023-08-09T06:13:43.120882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But what is really the power of fastText is that you can easily train fastText model with your own data, ie. train the dataset on text you feed in so that it can give you a more accurate result on specific domain. In fact, fastText support the training of model for both supervised and unsupervised data. Let's take a look at this example below in which I will be using my own dataset for illustration (https://www.kaggle.com/datasets/crxxom/all-animes-in-mal)","metadata":{}},{"cell_type":"code","source":"# Before unsupervised training\n\nft_model.get_nearest_neighbors('Ichigo', k=20) #parameter for you to show how many neighbors to be displayed","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:40:36.875333Z","iopub.execute_input":"2023-08-09T06:40:36.875747Z","iopub.status.idle":"2023-08-09T06:40:37.874697Z","shell.execute_reply.started":"2023-08-09T06:40:36.875716Z","shell.execute_reply":"2023-08-09T06:40:37.873439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nanime_df = pd.read_csv('/kaggle/input/all-animes-in-mal/mal_anime.csv')\n\nanime_df.to_csv('anime_synopsis.txt', columns=['synopsis'], index=False, header=False)\n\nmodel_trained = fasttext.train_unsupervised(\"/kaggle/working/anime_synopsis.txt\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:34:38.170092Z","iopub.execute_input":"2023-08-09T06:34:38.171416Z","iopub.status.idle":"2023-08-09T06:35:48.100702Z","shell.execute_reply.started":"2023-08-09T06:34:38.171378Z","shell.execute_reply":"2023-08-09T06:35:48.099264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Noted that there are a lot of parameters that you can change for a better training performance, for more information, visit the official documentation at: https://fasttext.cc/docs/en/unsupervised-tutorial.html","metadata":{}},{"cell_type":"code","source":"# After unsupervised training\n\nmodel_trained.get_nearest_neighbors('Ichigo', k=20) ","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:40:44.091300Z","iopub.execute_input":"2023-08-09T06:40:44.091749Z","iopub.status.idle":"2023-08-09T06:40:44.104390Z","shell.execute_reply.started":"2023-08-09T06:40:44.091714Z","shell.execute_reply":"2023-08-09T06:40:44.103226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare the results before and after the supervised training, we can see that the results is very different after you train the model compare to before you train the model. That is, **fastText give you a huge flexibility to train your own custom data to give your model more accurate result based on the data you provide with it**. To be completely fair, the result before training seems to give a more accurate outcome, but that is due to my dataset passing is extremely small compare to the 7GB of data and the dataset I am feeding to the model is the synopsis which may give a different result.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"glove\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">GloVe</span></div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *GloVe (Global Vectors for Word Representation) is an **unsupervised learning algorithm for obtaining vector representations for words**. Training is performed on **aggregated global word-word co-occurrence statistics from a corpus**, and the resulting representations showcase interesting linear substructures of the word vector space.*\n\nSimilar to word2vec and fasttext, it is some kind of algorithm that provide us with word embeddings. But it is important to note the GloVe does not use [CBOW](#cbow) nor [SkipGram](#skipgram) to train, instead it uses another training methology call **global matrix factorization** which analyze the global co-occurrence statistics of words in a corpus to learn the word embeddings. Let's see a very simple example as demonstration.\n\n\n### ‚úç Example\n\nIn this example, we will be examining how GloVe work using these three sentences\n> **1. I like deep learning** <br>\n> **2. I like NLP** <br>\n> **3. I enjoy flying** <br>\n\nAnd we contruct the count matrix/co-occurance matrix \n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/MMKYOLp.png\" style=\"height: 300px;\"> </div>\n\n<br>\n\nBefore we move on to illustrate how each value is being assigned, we should first understand the concept of **context window**, which is a specified range of tokens surrounding a target token within a text or sequence. In fact, you probably have come across with this concept in the [bag of n-grams](#bag-of-n-grams) and [CBOW](#cbow) sections. \n\n","metadata":{}},{"cell_type":"markdown","source":"In the above example, we are using a context window of 1, take 'like' in the first sentence as an example\n\n> **I** *like* **deep** learning\n\nWhen we use a context window of 1, we look for one token in front of our current word and one token after the current word, in this case, it will be **I** and **deep**. In this case, we will +1 to the corresponding entries in our co-occurance matrix, for the pair ('I', 'like') the corresponding entries will be X\\[1,2\\] and X\\[2,1\\]. But why is the number in the image 2 instead of 1? This is due to the fact that we also add 1 to this pair in our second sentence\n\n> **I** *like* **NLP**\n\nThat explains how the co-occurance matrix is being contructed.\n\nNote: Usually we use a higher context window such as 5 or more, if a context window is 5, we will be adding words from 5 words before the current word and 5 words after the current word.\n\nThis also give you some intuitive understanding of how GloVe cluster words together using the appropriate vectors. **The more two word appear together within a context window, the more they will get grouped together in a vector space.**\n\nThere are a lot more behind the algorithm of GloVe in which I plan to make a seperate notebook to explain GloVe in details in the future.","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n\nI have installed glove6b100dtxt model from kaggle for you to use it directly in this notebook. If you wish to download locally or download another version of the model, visit: https://github.com/stanfordnlp/GloVe/tree/master for more details.","metadata":{}},{"cell_type":"code","source":"# Write the content into a dictionary for later use\nimport numpy as np\n\n# Basically what we are doing is to create a dictionary object with the \n# key being the word, value being the respective vector\nembeddings_index = {}\nPATH = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'\nwith open(PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], \"float32\")\n        embeddings_index[word] = vector\nf.close()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T12:30:27.357619Z","iopub.execute_input":"2023-08-09T12:30:27.358423Z","iopub.status.idle":"2023-08-09T12:30:39.450169Z","shell.execute_reply.started":"2023-08-09T12:30:27.358378Z","shell.execute_reply":"2023-08-09T12:30:39.448883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embeddings_index['king']) # we see that the glove6b100d model uses 100 as the size of vector instead of the common 300","metadata":{"execution":{"iopub.status.busy":"2023-08-09T12:31:05.257011Z","iopub.execute_input":"2023-08-09T12:31:05.257475Z","iopub.status.idle":"2023-08-09T12:31:05.265129Z","shell.execute_reply.started":"2023-08-09T12:31:05.257349Z","shell.execute_reply":"2023-08-09T12:31:05.264179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is a function I found from: https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db","metadata":{}},{"cell_type":"code","source":"# We use the euclidean distance metrics from this example\n\nfrom scipy import spatial\n\ndef find_closest_embeddings_ed(embedding, cutoff=25):\n    return sorted(embeddings_index.keys(), \n                  key=lambda word: spatial.distance.euclidean(embeddings_index[word], embedding))","metadata":{"execution":{"iopub.status.busy":"2023-08-09T12:38:33.906003Z","iopub.execute_input":"2023-08-09T12:38:33.906916Z","iopub.status.idle":"2023-08-09T12:38:33.911988Z","shell.execute_reply.started":"2023-08-09T12:38:33.906873Z","shell.execute_reply":"2023-08-09T12:38:33.910896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec_king = embeddings_index[\"king\"]\nvec_man = embeddings_index[\"man\"]\nvec_woman = embeddings_index[\"woman\"]\n\nfind_closest_embeddings_ed(vec_king - vec_man + vec_woman)[:5]","metadata":{"execution":{"iopub.status.busy":"2023-08-09T12:36:18.219438Z","iopub.execute_input":"2023-08-09T12:36:18.219924Z","iopub.status.idle":"2023-08-09T12:36:21.122275Z","shell.execute_reply.started":"2023-08-09T12:36:18.219888Z","shell.execute_reply":"2023-08-09T12:36:21.121149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is a function I modified using cosine similarity instead of euclidean distance","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef find_closest_embeddings_cs(embedding, cutoff=25):\n    return sorted(embeddings_index.keys(), \n                  key=lambda word: -cosine_similarity([embeddings_index[word]], [embedding])) \n\n# Noted that we take negative cosine_similarity since the larger the value, the closer the clustering should be","metadata":{"execution":{"iopub.status.busy":"2023-08-09T12:41:11.991839Z","iopub.execute_input":"2023-08-09T12:41:11.992925Z","iopub.status.idle":"2023-08-09T12:41:11.998384Z","shell.execute_reply.started":"2023-08-09T12:41:11.992866Z","shell.execute_reply":"2023-08-09T12:41:11.997062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# When I ran the two function respectively, the cosine_similarity function takes a \n# significantly longer time to generate the results\n\nvec_king = embeddings_index[\"king\"]\nvec_man = embeddings_index[\"man\"]\nvec_woman = embeddings_index[\"woman\"]\n\nfind_closest_embeddings_cs(vec_king - vec_man + vec_woman)[:5]","metadata":{"execution":{"iopub.status.busy":"2023-08-09T12:41:14.131272Z","iopub.execute_input":"2023-08-09T12:41:14.131630Z","iopub.status.idle":"2023-08-09T12:43:07.156168Z","shell.execute_reply.started":"2023-08-09T12:41:14.131586Z","shell.execute_reply":"2023-08-09T12:43:07.155048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that using two different metrics give us almost identical result.\n\nLet's try to visualize the word vectors using TSNE.\n\n**What is TSNE?**\n\n‚ùó *t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.*\n\nBasically it is a **dimensionality reduction** method using some statistical approach, similar to other dimensionality reduction methods like **PCA**.\n\nIn short, what we are trying to achieve through TSNE is to reduce the 100 dimensions (vector size) down to 2 dimension so that we can plot it in a graph.","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport pandas as pd\ntsne = TSNE(n_components=2, random_state=42, perplexity=3) # reduce it to 2 dimension\n\nwords = [\"king\", \"woman\", \"man\", \"queen\"]\nvectors = []\nfor word in words:\n    vectors.append(embeddings_index[word])\n\nY = tsne.fit_transform(np.array(vectors)) \n#tsne takes in a matrix as input, with the each row representing a vector\n\nY\n\n# As we can see from the result below, we have successfully reduce the dimension to 2\n# Note that there will be loss of information/variance when we attempts to reduce the dimension","metadata":{"execution":{"iopub.status.busy":"2023-08-09T12:54:21.124836Z","iopub.execute_input":"2023-08-09T12:54:21.125945Z","iopub.status.idle":"2023-08-09T12:54:21.420564Z","shell.execute_reply.started":"2023-08-09T12:54:21.125898Z","shell.execute_reply":"2023-08-09T12:54:21.419702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.scatter(Y[:, 0], Y[:, 1]) # x-coordinates and y-coordinates as input\n\nfor label, x, y in zip(words, Y[:, 0], Y[:, 1]):\n    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points') # add annotates\n\nplt.title('Visualization of word embeddings using GloVe model')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T12:58:01.698210Z","iopub.execute_input":"2023-08-09T12:58:01.699529Z","iopub.status.idle":"2023-08-09T12:58:01.941772Z","shell.execute_reply.started":"2023-08-09T12:58:01.699478Z","shell.execute_reply":"2023-08-09T12:58:01.940460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"huggingface\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">ü§ó Hugging Face ü§ó</span></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/CoXGZHh.png\" style=\"height: 300px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"Before we move on to the next part, I would like to introduce you with one of the best open souce community and platform for AI - Hugging Face.\n\n‚ùó *Hugging Face, Inc. is an American company that develops tools for building applications using machine learning. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets.*\n\nHugging Face offers you with an enormous amount of open source AI models and dataset that you can definately find useful in your NLP journey. In this short section, I will briefly introduce you how can you utilize these awesome resources for your projects. \n\n\n**Official Hugging Face website: https://huggingface.co/**","metadata":{}},{"cell_type":"markdown","source":"### 1Ô∏è‚É£ Importing Hugging Face datasets to train your model\n\nImporting Hugging Face datasets is an extremely easy task, first you find the dataset you want. Go to the Hugging Face website -> Click on 'Datasets' on the navigation bar and look for the dataset you want to import.\n\nYou can utilize the filtering functions on the left side of the screen to find dataset related to the machine learning task you are working on.\n\nIn this example, we will be using this dataset: https://huggingface.co/datasets/gamino/wiki_medical_terms\n\n\nOn the right side of the screen, you should see a button which says **Use in dataset library**\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/Msskk3C.png\" style=\"height: 400px;\"> </div>\n\n<br>\n\nClick on it and copy the code to access the dataset","metadata":{}},{"cell_type":"code","source":"# run this command if datasets is not installed in your computer\n# pip install datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"gamino/wiki_medical_terms\")\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-08-09T13:15:16.482166Z","iopub.execute_input":"2023-08-09T13:15:16.482593Z","iopub.status.idle":"2023-08-09T13:15:22.290882Z","shell.execute_reply.started":"2023-08-09T13:15:16.482554Z","shell.execute_reply":"2023-08-09T13:15:22.289784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It is very easy to navigate through the dataset, it's just like how you navigate through a \n# python dictionary\n\ndataset['train']","metadata":{"execution":{"iopub.status.busy":"2023-08-09T13:17:44.811846Z","iopub.execute_input":"2023-08-09T13:17:44.812252Z","iopub.status.idle":"2023-08-09T13:17:44.818657Z","shell.execute_reply.started":"2023-08-09T13:17:44.812220Z","shell.execute_reply":"2023-08-09T13:17:44.817658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You can directly use pd.DataFrame to convert the dataset to your familiar pandas dataframe format\n\nimport pandas as pd\n\npd.DataFrame(dataset['train']).head()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T13:19:09.830991Z","iopub.execute_input":"2023-08-09T13:19:09.832070Z","iopub.status.idle":"2023-08-09T13:19:10.473510Z","shell.execute_reply.started":"2023-08-09T13:19:09.832029Z","shell.execute_reply":"2023-08-09T13:19:10.472437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2Ô∏è‚É£ Using Hugging Face models in your project\n\n\nImporting Hugging Face models is also a very easy task, first find the model you want to use from the 'Models' tab on the navigation bar on Hugging Face.\n\nSimilar to how you do with dataset, you can copy the code by clicking the **Use in Transformers** button. We will be using the pipeline method for illustration in this notebook. \n\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/GKRYTGM.png\" style=\"height: 400px;\"> </div>\n\n<br>\n\nAlso, one thing to point is that a lot of models in Hugging Face also have a workspace in the right hand side for you to test the model, so you can mess around with it to see if this is the model you want to use!\n\nIn this example, we will be using the popular sentiment-analysis model: https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis","metadata":{}},{"cell_type":"code","source":"# install transformers if you never use it before\n# Noted that you may need to install either PyTorch or Tensorflow to use the transformer \n# if you never download either one before\n\n# pip install transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"text-classification\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T13:28:20.335056Z","iopub.execute_input":"2023-08-09T13:28:20.335709Z","iopub.status.idle":"2023-08-09T13:28:43.779163Z","shell.execute_reply.started":"2023-08-09T13:28:20.335671Z","shell.execute_reply":"2023-08-09T13:28:43.778157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As you can see the model gives us a pretty accurate result\n\npipe(\"Learning natural language processing is fun\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T13:29:34.661844Z","iopub.execute_input":"2023-08-09T13:29:34.662888Z","iopub.status.idle":"2023-08-09T13:29:34.837808Z","shell.execute_reply.started":"2023-08-09T13:29:34.662850Z","shell.execute_reply":"2023-08-09T13:29:34.836568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, HuggingFace really offer you with a lot of cool things you can do and can really optimize your machine learning journey a lot. All thanks to such a wonderful open source community üî•","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"bert\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üåü Bidirectional Encoder Representations from Transformers (BERT) üåü</span></div>\n\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/wEEfptI.png\" style=\"height: 500px;\"> </div>","metadata":{}},{"cell_type":"markdown","source":"‚ùó *BERT, which stands for Bidirectional Encoder Representations from Transformers, is **based on Transformers**, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection.*\n\nBERT is an extremely powerful model in the sense that **it can 'understand' contextual meaning** and assign each word with a unique **contextualized embedding** based on the context of the sentence.\n\nWhat do we mean by that?\n\nThroughout this notebook, we have introduced multiple word embedding techniques and models such as Word2Vec and GloVe. But have you ever come across a situation where **the same word have different meaning in different sentences**? If you ran the algorithm with those models, it will return the same embedding for the same word, regardless of their **contextual meaning**.\n\nFor example,\n\n> **'set is an important concept in mathematics'** <br>\n> **'peter just bought a set of lego'** <br>\n> **'This is a set of equipments'**\n\nIn the above example, we human know that the 'set' from the first sentence is very different from the 'set' in the second and third sentences. BERT is a model which can give us a different embedding for the same word based on the context, which is really powerful thing in NLP.\n\nApart from BERT being able to generate contextualized embedding for words in different context, **BERT can also assign embedding to a sentence instead of just assigning embeddings to tokens**, which is something really handy when dealing with tasks such as search engine etc.\n\nThe working principle behind BERT is pretty complicated, which involve the architecture of transformers and concepts of encoders. These concepts will not be explained in this notebook since I personally found it to be not very beginner friendly, and in fact, I myself do not have a deep understanding to the topic as well as in for now. With that being said, if anyone can share some good resources in helping us to understanding more about transformers and BERT model, it will be great if you can share with us through commenting on this notebook!\n\nI will put some resources here if you are interested in how BERT work: \n\n### üìë Resources:\n\nStatQuest: https://www.youtube.com/watch?v=zxQyTK8quyY <br>\nUmar Jamil: https://www.youtube.com/watch?v=bCz4OMemCcA <br>\nJay Alammar: http://jalammar.github.io/illustrated-bert/ <br>\nGoogle Youtube: https://www.youtube.com/watch?v=t45S_MwAcOw <br>\nGoogle BERT github: https://github.com/google-research/bert <br>\nAttention is All You Need (Paper on Transformer): https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n","metadata":{}},{"cell_type":"markdown","source":"### üßë‚Äçüíª Let's code \n\nIn this section, we will be touching the tips of BERT using tensorflow model.\n\n\nFull list of BERT models from tensorflow:  https://tfhub.dev/google/collections/bert/1\n","metadata":{}},{"cell_type":"code","source":"# pip install tensorflow_hub\n# pip install tensorflow_text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_hub as hub\nimport tensorflow_text as text","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:14:51.416896Z","iopub.execute_input":"2023-08-09T14:14:51.417550Z","iopub.status.idle":"2023-08-09T14:14:52.266488Z","shell.execute_reply.started":"2023-08-09T14:14:51.417511Z","shell.execute_reply":"2023-08-09T14:14:52.265097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# did you know the word 'set' has the most meaning? \ntext_input = ['set is an important concept in mathematics',\n              'peter just bought a set of lego',\n              'This is a set of equipments',]","metadata":{"execution":{"iopub.status.busy":"2023-08-09T15:01:17.187709Z","iopub.execute_input":"2023-08-09T15:01:17.188108Z","iopub.status.idle":"2023-08-09T15:01:17.193835Z","shell.execute_reply.started":"2023-08-09T15:01:17.188076Z","shell.execute_reply":"2023-08-09T15:01:17.192295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download the preprocessor and run it with tensorflow_hub\npreprocessor = hub.KerasLayer(\n    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\nencoder_inputs = preprocessor(text_input) # pass the text input into the preprocessor","metadata":{"execution":{"iopub.status.busy":"2023-08-09T15:01:18.772473Z","iopub.execute_input":"2023-08-09T15:01:18.772882Z","iopub.status.idle":"2023-08-09T15:01:20.979457Z","shell.execute_reply.started":"2023-08-09T15:01:18.772850Z","shell.execute_reply":"2023-08-09T15:01:20.978166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_inputs.keys()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:42:58.973939Z","iopub.execute_input":"2023-08-09T14:42:58.974781Z","iopub.status.idle":"2023-08-09T14:42:58.981252Z","shell.execute_reply.started":"2023-08-09T14:42:58.974742Z","shell.execute_reply":"2023-08-09T14:42:58.980340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_inputs['input_word_ids']","metadata":{"execution":{"iopub.status.busy":"2023-08-09T15:01:22.353318Z","iopub.execute_input":"2023-08-09T15:01:22.354438Z","iopub.status.idle":"2023-08-09T15:01:22.362778Z","shell.execute_reply.started":"2023-08-09T15:01:22.354396Z","shell.execute_reply":"2023-08-09T15:01:22.361632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"encoder_inputs\\['input_word_ids'\\] return us with tensor of shape (3, 128), what does that mean\n\n> 3 refers to the '3' sentences we have in text input\n\n> 128 is the array to represent the sentence when we use this particular model \n(that is, if the number of tokens from a sentence is longer than 128 (126 in fact due to masking), it will not work properly)\n>> - Notice how each sentence is 'masked' between '101' and '102' <br>\n>> - Also notice that the word 'set' has the same word_id up till this stage","metadata":{}},{"cell_type":"code","source":"# Download the encoder and run it with tensorflow_hub\nencoder = hub.KerasLayer(\n    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\",\n    trainable=True)\noutputs = encoder(encoder_inputs) # pass the encoder inputs to the encoder","metadata":{"execution":{"iopub.status.busy":"2023-08-09T15:01:26.248198Z","iopub.execute_input":"2023-08-09T15:01:26.248644Z","iopub.status.idle":"2023-08-09T15:01:40.553718Z","shell.execute_reply.started":"2023-08-09T15:01:26.248593Z","shell.execute_reply":"2023-08-09T15:01:40.552816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.keys()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:29:23.275010Z","iopub.execute_input":"2023-08-09T14:29:23.275534Z","iopub.status.idle":"2023-08-09T14:29:23.283359Z","shell.execute_reply.started":"2023-08-09T14:29:23.275497Z","shell.execute_reply":"2023-08-09T14:29:23.282016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the pooled_output is the embeddings for the whole sentence\n# ie. outputs['pooled_output'][0] is the embeddings for the whole sentence 'set is an important concept in mathematics'\n\noutputs['pooled_output']","metadata":{"execution":{"iopub.status.busy":"2023-08-09T15:01:46.310415Z","iopub.execute_input":"2023-08-09T15:01:46.311043Z","iopub.status.idle":"2023-08-09T15:01:46.318917Z","shell.execute_reply.started":"2023-08-09T15:01:46.311010Z","shell.execute_reply":"2023-08-09T15:01:46.317869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the BERT model can give us embedding for the whole sentence instead of just embedding for each individual word like other models we have reviewed so far. This can be some really power tool in handling various task such as a search engine.","metadata":{}},{"cell_type":"code","source":"# sequence_output gives us the embedding of each individual words\n\noutputs['sequence_output']","metadata":{"execution":{"iopub.status.busy":"2023-08-09T15:01:48.716618Z","iopub.execute_input":"2023-08-09T15:01:48.717037Z","iopub.status.idle":"2023-08-09T15:01:48.727139Z","shell.execute_reply.started":"2023-08-09T15:01:48.717003Z","shell.execute_reply":"2023-08-09T15:01:48.725838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the sequence_output gives us a tensor of shape(5, 128, 768), where \n\n> 3 is referring to our 3 sentences\n\n> 128 refers the **array of each sentence**\n>> If you recall from encoder_inputs\\['input_word_ids'\\], we see that the preprocessor gives us an array of 128 length for each sentence in which it will automatically add zeros to fill up the empty space aka padding.\n\n> 768 refers to the **contextualized embedding of each individual token**\n\n### Question: How do we get the embedding for the word 'set' from the first sentence ('set is an important concept in mathematics')?","metadata":{}},{"cell_type":"code","source":"first_sentence = outputs['sequence_output'][0]\nfirst_word_in_sentence = first_sentence[1] # recall that first_sentence[0] is the 101 mask\nfirst_word_in_sentence # we successfully get the embedding for the word 'set' from the first sentence","metadata":{"execution":{"iopub.status.busy":"2023-08-09T15:02:00.658832Z","iopub.execute_input":"2023-08-09T15:02:00.659236Z","iopub.status.idle":"2023-08-09T15:02:00.675097Z","shell.execute_reply.started":"2023-08-09T15:02:00.659205Z","shell.execute_reply":"2023-08-09T15:02:00.673954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_from_first_sentence = outputs['sequence_output'][0][1] # 'set is an important concept in mathematics'\nset_from_second_sentence = outputs['sequence_output'][1][5] # 'peter just bought a set of lego'\nset_from_third_sentence = outputs['sequence_output'][2][4] # 'This is a set of equipments'\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nprint(\"Cosine similarity of set from 1st & 2nd sentence\",cosine_similarity([set_from_first_sentence], [set_from_second_sentence]))\nprint(\"Cosine similarity of set from 2nd & 3rd sentence\",cosine_similarity([set_from_second_sentence], [set_from_third_sentence]))","metadata":{"execution":{"iopub.status.busy":"2023-08-09T15:03:21.551061Z","iopub.execute_input":"2023-08-09T15:03:21.552036Z","iopub.status.idle":"2023-08-09T15:03:21.564055Z","shell.execute_reply.started":"2023-08-09T15:03:21.551998Z","shell.execute_reply":"2023-08-09T15:03:21.562504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fascinating isn't it. Even though we have the same word (set) for all the three sentence, **BERT has the ability to understanding contextual meaning and assign different embeddings to the same word in different context**! In this case, we know that the 'set' from 'a set of lego' does indeed have a more similar meaning to the 'set' in 'a set of equipments' than to 'set is an important concept in mathematics'.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"summary-2\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üìù Short Summary (Part 2: Feature Engineering) üìù</span></div>","metadata":{}},{"cell_type":"markdown","source":"Let's review what we have learnt in part 2: Feature Engineering. \n\n### 1. [Bag of Words](#bag-of-words)\n\nA way of word embedding, in which we create a count vectorizer to count the word appearance\n\n### 2. [Bag of n-grams](#bag-of-n-grams)\n\nA way of word embedding similar to bag of words, but instead of just use a single word as column, we use a range of consecutive words as features\n\n### 3. [TF-IDF](#tf-idf)\n\nA way of word embedding in which we use some sort of formulas to assign weight to each word in each corpus, effectively replacing the need to remove stop words\n\n### 4. [Word Embeddings](#word-embeddings)\n\nA way to represent text in which we attempt to numerize/vectorize words in a meaningful way in which words that are more similar will be closer together in vector space\n\n### 5. [Cosine Similarity](#cosine-similarity)\n\nA popular metrics to quantify similarity between two vectors\n\n### 6. [CBOW](#cbow)\n\nA deep learning algorithm to generate word embeddings\n\n### 7. [SkipGram](#skipgram)\n\nA deep learning algorithm similar to CBOW to generate word embeddings\n\n### 8. [Word2Vec](#word2vec)\n\nModel that involve a family of algorithms including both CBOW and SkipGram to generate word embeddings\n\n### 9. [fastText](#fasttext)\n\nModel similar to Word2Vec that generate word embeddings\n\n### 10. [GloVe](#glove)\n\nAnother model similar to fastText and Word2Vec but using a different training methology of global matrix factorization to generate word embeddings\n\n### 11. [Hugging Face](#huggingface)\n\nOpen source AI community that offers a lot of awesome resources, including datasets and models for you to use\n\n### 12. [BERT](#bert)\n\nPowerful NLP model to generate contextualized embeddings for both words and sentences ","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"here-is-a-task-for-you\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">‚úç Here is a Task for You ‚úç</span></div>","metadata":{}},{"cell_type":"markdown","source":"Before we move on to the last part which is a case study, I would like you to use the techniques you have learnt so far and create a seperate notebook to try out the competition: https://www.kaggle.com/competitions/nlp-getting-started\n\nI know that I haven't touch anything about building machine learning model yet in this notebook, but this is a challenge for you. To do research and try come up with your own version of NLP model. \n\nHere is a lightweighted guideline to give you some directions:\n\nimport the data ‚û° understand and clean the data ‚û° preprocess the data ‚û° vectorize the data ‚û° train your model ‚û° make predictions ‚û° review your model and make improvements\n\nIn fact, this is exactly what we have learnt in our first section [overview of NLP pipeline](#nlp-pipeline)\n\nAfter you have completed your version, you can scroll down to see my version to complete the task, which is in no way the perfect model or a high performance model. But I will try to include as much concepts we have learnt so far in the process as a way of learning. If you want to see what the pros are doing, there are plenty of high performance notebooks out there that you can take notes on.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"case-study\"></a><div style=\"text-align:center; font-size:40px;\"><span style=\"color:#3A506B\">üö® Case Study: NLP with Disaster Tweet üö®</span></div>","metadata":{}},{"cell_type":"markdown","source":"## 1Ô∏è‚É£ Understanding the data","metadata":{}},{"cell_type":"code","source":"# Some fundamental libraries that you will need to use throughout the tutorial\n# for libraries for more specific tasks, I will be importing them seperately in each block\n# for learning purpose, so you know which library corresponds to which function\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt # not part of pre-requisite, but it is just some data visualization tools, which is not our the focus of the notebook\nimport seaborn as sns # another data visualization tool\n\n# import data\n\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T02:59:40.029870Z","iopub.execute_input":"2023-08-10T02:59:40.030350Z","iopub.status.idle":"2023-08-10T02:59:40.099616Z","shell.execute_reply.started":"2023-08-10T02:59:40.030315Z","shell.execute_reply":"2023-08-10T02:59:40.098488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/qiqMMnN.png\" style=\"height: 300px;\"> </div>","metadata":{}},{"cell_type":"code","source":"train_df.shape\n\n# We see that the training df consists of 7613 rows","metadata":{"execution":{"iopub.status.busy":"2023-08-10T02:22:16.155642Z","iopub.execute_input":"2023-08-10T02:22:16.156403Z","iopub.status.idle":"2023-08-10T02:22:16.163276Z","shell.execute_reply.started":"2023-08-10T02:22:16.156366Z","shell.execute_reply":"2023-08-10T02:22:16.162401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.target.value_counts()\n\n# We see that the target is slightly imbalanced\n# to deal with imbalance dataset, one can apply a lot of difference techniques such as \n# downsampling, in this tutorial, we will just use the entire dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-10T02:23:29.641275Z","iopub.execute_input":"2023-08-10T02:23:29.641711Z","iopub.status.idle":"2023-08-10T02:23:29.654705Z","shell.execute_reply.started":"2023-08-10T02:23:29.641678Z","shell.execute_reply":"2023-08-10T02:23:29.653856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for null value\n\ntrain_df.isna().sum()\n\n# In this notebook, we will just be using the text column and the target column for simplicity","metadata":{"execution":{"iopub.status.busy":"2023-08-10T02:27:12.656182Z","iopub.execute_input":"2023-08-10T02:27:12.657242Z","iopub.status.idle":"2023-08-10T02:27:12.676324Z","shell.execute_reply.started":"2023-08-10T02:27:12.657199Z","shell.execute_reply":"2023-08-10T02:27:12.674903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some cool method to give you an overview to some of the most frequently appearing words\n\n# pip install the corresponding libraries if you have not\n\nfrom PIL import Image\nfrom wordcloud import WordCloud\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200,  # number of word display in the wordcloud\n)\n\n# join text messages together\njoined_text = ''\nfor text in train_df.loc[train_df['target']== 0, 'text']: #target == 0 refer to all the fake tweets\n    joined_text = joined_text + ' ' + text + ' '\n\nwc.generate(joined_text) \nplt.figure(figsize=(18,10))\nplt.title('100 most frequently appeared words for fake tweets', \n          fontdict={'size': 30})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()\n\n# we can see some random words and even swear words as well","metadata":{"execution":{"iopub.status.busy":"2023-08-10T02:52:22.777319Z","iopub.execute_input":"2023-08-10T02:52:22.777785Z","iopub.status.idle":"2023-08-10T02:52:24.181234Z","shell.execute_reply.started":"2023-08-10T02:52:22.777738Z","shell.execute_reply":"2023-08-10T02:52:24.179951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom wordcloud import WordCloud\n\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n)\n\njoined_text = ''\nfor text in train_df.loc[train_df['target']== 1, 'text']:\n    joined_text = joined_text + ' ' + text + ' '\n    \nwc.generate(joined_text)\nplt.figure(figsize=(18,10))\nplt.title('100 most frequently appeared words for real tweets', \n          fontdict={'size': 30})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T02:52:47.677808Z","iopub.execute_input":"2023-08-10T02:52:47.678928Z","iopub.status.idle":"2023-08-10T02:52:48.728969Z","shell.execute_reply.started":"2023-08-10T02:52:47.678886Z","shell.execute_reply":"2023-08-10T02:52:48.727844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['tweet_len'] = train_df['text'].apply(lambda x: len(x.split(' ')))\n\nsns.histplot(train_df['tweet_len'])\n\nplt.title(\"Distribution of word counts in each tweet in train data\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T02:59:43.692120Z","iopub.execute_input":"2023-08-10T02:59:43.692528Z","iopub.status.idle":"2023-08-10T02:59:44.216578Z","shell.execute_reply.started":"2023-08-10T02:59:43.692494Z","shell.execute_reply":"2023-08-10T02:59:44.215385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2Ô∏è‚É£ Data Cleaning and Preprocessing","metadata":{}},{"cell_type":"code","source":"# As we mentioned, we will only be using the text column and target\n\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n\ntrain_df = train_df.drop(['id', 'keyword', 'location'],axis=1)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:47:40.866890Z","iopub.execute_input":"2023-08-10T04:47:40.867410Z","iopub.status.idle":"2023-08-10T04:47:40.906629Z","shell.execute_reply.started":"2023-08-10T04:47:40.867372Z","shell.execute_reply":"2023-08-10T04:47:40.905288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaning text data is always a very important step, we want to reduce the 'noise' in our dataset as much as possible to give a more accurate results. Cleaning tweets is a fairly long process and a good cleaning will require a lot of work and coding. If you want to learn more about how to pros are cleaning the text for this competition, here are some notebooks that did a very detailed cleaning using regular expression: \n\nhttps://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n\nhttps://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove\n","metadata":{}},{"cell_type":"code","source":"# code from https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove\n\nimport re # regular expression\nimport string\n\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  \n                           u\"\\U0001F300-\\U0001F5FF\"  \n                           u\"\\U0001F680-\\U0001F6FF\"  \n                           u\"\\U0001F1E0-\\U0001F1FF\"  \n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ntrain_df['text']= train_df['text'].apply(remove_URL)\ntrain_df['text']= train_df['text'].apply(remove_html)\ntrain_df['text']= train_df['text'].apply(remove_emoji)\ntrain_df['text']= train_df['text'].apply(remove_punct)\n\n\ntrain_df.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-08-10T04:47:42.734895Z","iopub.execute_input":"2023-08-10T04:47:42.738087Z","iopub.status.idle":"2023-08-10T04:47:42.924978Z","shell.execute_reply.started":"2023-08-10T04:47:42.738034Z","shell.execute_reply":"2023-08-10T04:47:42.923445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some extra functions I added\n\ndef remove_newline(text):\n    newline = re.compile(r'\\n')\n    return newline.sub(r'',text)\n\ndef remove_extra_space(text):\n            return re.sub(r'\\s+', ' ', text)\n\ntrain_df['text'] = train_df['text'].apply(remove_newline)\ntrain_df['text'] = train_df['text'].apply(lambda x: x.lower()) # apply lowercase\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:49:25.377751Z","iopub.execute_input":"2023-08-10T04:49:25.378249Z","iopub.status.idle":"2023-08-10T04:49:25.418525Z","shell.execute_reply.started":"2023-08-10T04:49:25.378213Z","shell.execute_reply":"2023-08-10T04:49:25.417515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ‚úç Test your knowledge\n\nTry to implement your own function of lemmatization using spaCy.","metadata":{}},{"cell_type":"code","source":"#  Your code here, scroll down to see my version of implementation\n# ---------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ---------------------------------------------","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying lemmatization to the text\n\nimport spacy \n\nnlp = spacy.load(\"en_core_web_lg\")\n\ndef lemmatization(text):\n    processed_text = []\n    doc = nlp(text)\n    for token in doc:\n        processed_text.append(token.lemma_)\n    \n    return ' '.join(processed_text)\n\ntrain_df['text'] = train_df['text'].apply(lemmatization)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now try to implement a function that remove all the stop words","metadata":{}},{"cell_type":"code","source":"#  Your code here, scroll down to see my version of implementation\n# ---------------------------------------------\n\nimport spacy \n\nnlp = spacy.load(\"en_core_web_lg\")\n\ndef remove_stop_words(text):\n    processed_text = []\n    doc = nlp(text)\n    for token in doc:\n        if token.is_stop:\n            continue\n        processed_text.append(token.text)\n        \n    return ' '.join(processed_text)\n\ntrain_df['text'] = train_df['text'].apply(remove_stop_words)\n\n# ---------------------------------------------","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In fact, you can implement the remove_stop_words function and lemmatization function in one single function. But I decided to split it into two section so that I can test out different results when comparing input without removing stop words and input with the removal of stop words as some sort of experimentation.","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"pipeline\"></a>Pipeline üü© ‚û° üîµ\n\nBefore we move on, I would like to introduce you to a very handy tool in machine learning - Pipeline. A pipeline is just like a more standard and professional way as a replacement of a function, it allows you to have some kind of abstraction so that it gives you huge convenience and help you a lot with your ML training process. Let's see it in action.\n\nIf you find it too overwhelming, you can skip this part and use normal functions instead.","metadata":{}},{"cell_type":"code","source":"# necessary libraries for pipline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# libraries used in the pipeline\nimport re \nimport string\nimport spacy \n\n\n# standard structure of an estimator (like a function)\nclass text_cleaning(BaseEstimator, TransformerMixin):  # change the class name, keep everything unchanged\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        # this part is what you write in your function\n        # what I will do is basically copy and paste what we have done just now\n        def remove_URL(text):\n            url = re.compile(r'https?://\\S+|www\\.\\S+')\n            return url.sub(r'',text)\n\n        def remove_html(text):\n            html=re.compile(r'<.*?>')\n            return html.sub(r'',text)\n\n        def remove_emoji(text):\n            emoji_pattern = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  \n                                   u\"\\U0001F300-\\U0001F5FF\"  \n                                   u\"\\U0001F680-\\U0001F6FF\"  \n                                   u\"\\U0001F1E0-\\U0001F1FF\"  \n                                   u\"\\U00002702-\\U000027B0\"\n                                   u\"\\U000024C2-\\U0001F251\"\n                                   \"]+\", flags=re.UNICODE)\n            return emoji_pattern.sub(r'', text)\n\n        def remove_punct(text):\n            table=str.maketrans('','',string.punctuation)\n            return text.translate(table)\n        \n        def remove_newline(text):\n            newline = re.compile(r'\\n')\n            return newline.sub(r'',text)\n        \n        def remove_extra_space(text):\n            return re.sub(r'\\s+', ' ', text)\n\n        \n        X['text'] = X['text'].apply(remove_URL) # noted that we use X as passed in from the argument instead of train_df\n        X['text'] = X['text'].apply(remove_html)\n        X['text'] = X['text'].apply(remove_emoji)\n        X['text'] = X['text'].apply(remove_punct)\n        X['text'] = X['text'].apply(remove_newline)\n        X['text'] = X['text'].apply(remove_extra_space)\n        X['text'] = X['text'].apply(lambda x: x.lower()) \n        \n        return X\n\nclass lemmatization(BaseEstimator, TransformerMixin):  \n    def fit(self, X, y=None):\n        return self\n    def transform(self, X): \n        nlp = spacy.load(\"en_core_web_lg\")\n        def lemma_func(text):\n            processed_text = []\n            doc = nlp(text)\n            for token in doc:\n                processed_text.append(token.lemma_)\n                \n            return ' '.join(processed_text)\n\n        X['text'] = X['text'].apply(lemma_func)\n        \n        return X\n    \nclass stop_word_removal(BaseEstimator, TransformerMixin):  \n    def fit(self, X, y=None):\n        return self\n    def transform(self, X): \n        nlp = spacy.load(\"en_core_web_lg\")\n        def remove_stop_words(text):\n            processed_text = []\n            doc = nlp(text)\n            for token in doc:\n                if token.is_stop:\n                    continue\n                processed_text.append(token.text)\n        \n            return ' '.join(processed_text)\n        \n        X['text'] = X['text'].apply(remove_stop_words)\n        \n        return X","metadata":{"execution":{"iopub.status.busy":"2023-08-10T12:32:11.789333Z","iopub.execute_input":"2023-08-10T12:32:11.790730Z","iopub.status.idle":"2023-08-10T12:32:23.872213Z","shell.execute_reply.started":"2023-08-10T12:32:11.790679Z","shell.execute_reply":"2023-08-10T12:32:23.870737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain_df = train_df.drop(['id', 'keyword', 'location'],axis=1)\n\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_df = test_df.drop(['id', 'keyword', 'location'],axis=1)\n\n# defining pipeline object, order matters by the way\npipeline_cleaning = Pipeline([\n        ('clean_text', text_cleaning()), # 'text_cleaning()' is our class name we defined just now, 'clean_text' is sort of like a description\n        ('removing_stop_words', stop_word_removal()), \n        ('lemma', lemmatization()), \n        \n]) \n\ncleaned_train_df = pipeline_cleaning.fit_transform(train_df)  # use fit_transform on the dataset you pass in\ncleaned_test_df = pipeline_cleaning.fit_transform(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T12:33:03.588536Z","iopub.execute_input":"2023-08-10T12:33:03.589134Z","iopub.status.idle":"2023-08-10T12:37:10.713435Z","shell.execute_reply.started":"2023-08-10T12:33:03.589085Z","shell.execute_reply":"2023-08-10T12:37:10.711259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now everytime we want to preprocess the data, we can pass it directly to the pipeline which abstract the whole process, basically you just pass in the dataframe and then it will return you with the cleaned dataframe.\n\nFor instance, if we want to preprocess the test_df now, all we need to do is to run the code\n\n`cleaned_test_df = pipeline_cleaning.fit_transform(test_df)`\n\nas shown above, which save us from a lot of trouble.\n\nIn fact, you can customize your pipeline to the way you like, for example, instead of just the large function of text_cleaning(), you can split it to individual small estimators like remove_punct(), remove_url() just like what you do with a function.","metadata":{}},{"cell_type":"markdown","source":"### 3Ô∏è‚É£ Comparing different word embedding methods\n\nIn this section, we will be using several models for word embeddings and test the result to have some kind of insight/experimentation with what kind of impact of using different models for word embeddings will have to our model accuracy.\n\nThis section assumed you already have cleaned_train_df and cleaned_test_df in your kernel. If not, run all the code in the section [Pipeline](#pipeline)","metadata":{}},{"cell_type":"markdown","source":"### <div style=\"text-align:center; font-size:40px;\">üü† Embedding Method 1: Bag of n-grams üü†</div>\n\n(To run the code below, all you need to do is to run the two code block in Pipeline section above)","metadata":{}},{"cell_type":"code","source":"# split the data using sklearn train_test_split to evaluate our model\n# basically the idea behind train_test_split is to use a percentage of our training dataset\n# to train our model, then use the remaining data to evaluate the performance of the model\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cleaned_train_df.text,\n    cleaned_train_df.target,\n    stratify=cleaned_train_df.target, #stratify allow the split to be split in a way that balance the train and test set by the target column\n    test_size=0.2, # 20% of data will be used as test data\n    random_state=42) # setting random_state allow the split to be the same everytime you run this","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:35:52.663463Z","iopub.execute_input":"2023-08-10T09:35:52.664135Z","iopub.status.idle":"2023-08-10T09:35:52.684712Z","shell.execute_reply.started":"2023-08-10T09:35:52.664091Z","shell.execute_reply":"2023-08-10T09:35:52.682532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simple model using MultinomialNB with no hyperparameter tuning\n# we use ngram_range=(1,2) to train the model, for more detail check out the section bag of n-grams\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nclf_bog_nb = Pipeline([\n    ('bag-of-2-grams', CountVectorizer(ngram_range=(1,2))),\n    ('multiNB', MultinomialNB())\n])\n\nclf_bog_nb.fit(X_train, y_train)\n\ny_pred = clf_bog_nb.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:35:56.283407Z","iopub.execute_input":"2023-08-10T09:35:56.285098Z","iopub.status.idle":"2023-08-10T09:35:56.697685Z","shell.execute_reply.started":"2023-08-10T09:35:56.285032Z","shell.execute_reply":"2023-08-10T09:35:56.696458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_estimator(\n                        clf_bog_nb,\n                        X_test,\n                        y_test,\n                        values_format='d',\n                        display_labels=['Fake news','Real news'])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:36:00.533075Z","iopub.execute_input":"2023-08-10T09:36:00.534708Z","iopub.status.idle":"2023-08-10T09:36:00.941647Z","shell.execute_reply.started":"2023-08-10T09:36:00.534646Z","shell.execute_reply":"2023-08-10T09:36:00.940495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### üîé Confusion Matrix \n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://i.imgur.com/cDhprM7.jpg\" style=\"height: 300px;\"> </div>\n\n<br>\n\nPlotting a confusion matrix allow us to have a clear and quick insight to our model performance.\n\nIt is very easy to interperate the result from a confusion matrix, basically the 'y-axis' represents the true label and 'x-axis' represents the predicted label.\n\nFor example, one can tell from the confusion matrix that\n\n> our model predicted a total of 925 fake news, in which 752 of them is correctly predicted and 173 of them is wrongly predicted\n\nIn machine learning, we usually represent this as sensitivity, in which,\n\n\n<div style=\"text-align:center; font-size:20px;\"> Sensitivity =  $\\frac{True Positives}{True Positives + False Negatives}$ </div>\n\n<br>\n\nIn our case, the sensitivity will be around 0.8129 (752/(752+173))\n\n\n<br>\n<br>\n\n> our model predicted a total of 559 fake news, in which 468 of them is correctly predicted and 91 of them is wrongly predicted\n\nIn machine learning, we usually represent this as specificity, in which,\n\n<div style=\"text-align:center; font-size:20px;\"> Specificity =  $\\frac{True Negatives}{True Negatives + False Positives}$ </div>\n\n<br>\n\nIn our case, the specificity will be around 0.8043 (481/(481+117))\n","metadata":{}},{"cell_type":"code","source":"# You can also run a classification report for more detailed info\n\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:36:16.298946Z","iopub.execute_input":"2023-08-10T09:36:16.300358Z","iopub.status.idle":"2023-08-10T09:36:16.322449Z","shell.execute_reply.started":"2023-08-10T09:36:16.300305Z","shell.execute_reply":"2023-08-10T09:36:16.320513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf_bog_rf = Pipeline([\n    ('bag-of-2-grams', CountVectorizer(ngram_range=(1,2))),\n    ('rf', RandomForestClassifier())\n])\n\nclf_bog_rf.fit(X_train, y_train)\n\ny_pred = clf_bog_rf.predict(X_test)\n\nConfusionMatrixDisplay.from_estimator(\n                        clf_bog_rf,\n                        X_test,\n                        y_test,\n                        values_format='d',\n                        display_labels=['Fake news','Real news'])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:36:19.390450Z","iopub.execute_input":"2023-08-10T09:36:19.391448Z","iopub.status.idle":"2023-08-10T09:36:54.261017Z","shell.execute_reply.started":"2023-08-10T09:36:19.391400Z","shell.execute_reply":"2023-08-10T09:36:54.259645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sensitivity of 0.7631, Specificity of 0.8581**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:36:57.853364Z","iopub.execute_input":"2023-08-10T09:36:57.854019Z","iopub.status.idle":"2023-08-10T09:36:57.875040Z","shell.execute_reply.started":"2023-08-10T09:36:57.853980Z","shell.execute_reply":"2023-08-10T09:36:57.873429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <div style=\"text-align:center; font-size:40px;\">üü° Embedding Method 2: TF-IDF üü° </div>\n\n(To run the code below, all you need to do is to run the two code block in Pipeline section above)","metadata":{}},{"cell_type":"code","source":"# split the data using sklearn train_test_split to evaluate our model\n# basically the idea behind train_test_split is to use a percentage of our training dataset\n# to train our model, then use the remaining data to evaluate the performance of the model\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cleaned_train_df.text,\n    cleaned_train_df.target,\n    stratify=cleaned_train_df.target, #stratify allow the split to be split in a way that balance the train and test set by the target column\n    test_size=0.2, # 20% of data will be used as test data\n    random_state=42) # setting random_state allow the split to be the same everytime you run this","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:37:05.964580Z","iopub.execute_input":"2023-08-10T09:37:05.966325Z","iopub.status.idle":"2023-08-10T09:37:05.983683Z","shell.execute_reply.started":"2023-08-10T09:37:05.966243Z","shell.execute_reply":"2023-08-10T09:37:05.981792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nclf_tfidf_nb = Pipeline([\n    ('tf-idf', TfidfVectorizer()),\n    ('multiNB', MultinomialNB())\n])\n\nclf_tfidf_nb.fit(X_train, y_train)\n\ny_pred = clf_tfidf_nb.predict(X_test)\n\nConfusionMatrixDisplay.from_estimator(\n                        clf_tfidf_nb,\n                        X_test,\n                        y_test,\n                        values_format='d',\n                        display_labels=['Fake news','Real news'])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:37:07.028195Z","iopub.execute_input":"2023-08-10T09:37:07.028617Z","iopub.status.idle":"2023-08-10T09:37:07.588580Z","shell.execute_reply.started":"2023-08-10T09:37:07.028583Z","shell.execute_reply":"2023-08-10T09:37:07.587120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sensitivity of 0.7955, Specificity of 0.8449|**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:37:10.628167Z","iopub.execute_input":"2023-08-10T09:37:10.628686Z","iopub.status.idle":"2023-08-10T09:37:10.649751Z","shell.execute_reply.started":"2023-08-10T09:37:10.628646Z","shell.execute_reply":"2023-08-10T09:37:10.647868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf_tdidf_rf = Pipeline([\n    ('tf-idf', TfidfVectorizer()),\n    ('rf', RandomForestClassifier())\n])\n\nclf_tdidf_rf.fit(X_train, y_train)\n\ny_pred = clf_tdidf_rf.predict(X_test)\n\nConfusionMatrixDisplay.from_estimator(\n                        clf_tdidf_rf,\n                        X_test,\n                        y_test,\n                        values_format='d',\n                        display_labels=['Fake news','Real news'])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:37:14.418750Z","iopub.execute_input":"2023-08-10T09:37:14.419299Z","iopub.status.idle":"2023-08-10T09:37:28.617511Z","shell.execute_reply.started":"2023-08-10T09:37:14.419260Z","shell.execute_reply":"2023-08-10T09:37:28.616069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sensitivity of 0.7915, Specificity of 0.8158**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:40:42.553690Z","iopub.execute_input":"2023-08-10T09:40:42.555169Z","iopub.status.idle":"2023-08-10T09:40:42.574714Z","shell.execute_reply.started":"2023-08-10T09:40:42.555110Z","shell.execute_reply":"2023-08-10T09:40:42.573123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <div style=\"text-align:center; font-size:40px;\">üü¢ Embedding Method 3: GloVe üü¢</div>\n\n(To run the code below, all you need to do is to run the two code block in Pipeline section above)","metadata":{}},{"cell_type":"markdown","source":"### ‚úç Test your knowledge\n\nImport the GloVe model to the notebook into a embedding dictionary object.","metadata":{}},{"cell_type":"code","source":"# Your code here, scroll down to see my version of implementation\n# -----------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# -----------------------------------------------","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write the content into a dictionary for later use\nimport numpy as np\n\n# Basically what we are doing is to create a dictionary object with the \n# key being the word, value being the respective vector\nembeddings_index = {}\nPATH = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'\nwith open(PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], \"float32\")\n        embeddings_index[word] = vector\nf.close()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T10:18:35.688571Z","iopub.execute_input":"2023-08-10T10:18:35.689206Z","iopub.status.idle":"2023-08-10T10:18:51.235858Z","shell.execute_reply.started":"2023-08-10T10:18:35.689167Z","shell.execute_reply":"2023-08-10T10:18:51.233996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I assume you know how to use the embedding_index dictionary, but now here is a problem. How do we connect the dots? How do we create a dataset suitable for training our machine learning model?\n \nTo solve this problem let us first think about what do we have. For instance, we can create word embeddings for all tokens in each sentences","metadata":{"execution":{"iopub.status.busy":"2023-08-10T08:01:56.968835Z","iopub.execute_input":"2023-08-10T08:01:56.969443Z","iopub.status.idle":"2023-08-10T08:01:57.806899Z","shell.execute_reply.started":"2023-08-10T08:01:56.969402Z","shell.execute_reply":"2023-08-10T08:01:57.805325Z"}}},{"cell_type":"code","source":"# split the data using sklearn train_test_split to evaluate our model\n# basically the idea behind train_test_split is to use a percentage of our training dataset\n# to train our model, then use the remaining data to evaluate the performance of the model\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cleaned_train_df.text,\n    cleaned_train_df.target,\n    stratify=cleaned_train_df.target, #stratify allow the split to be split in a way that balance the train and test set by the target column\n    test_size=0.2, # 20% of data will be used as test data\n    random_state=42) # setting random_state allow the split to be the same everytime you run this","metadata":{"execution":{"iopub.status.busy":"2023-08-10T08:57:39.788361Z","iopub.execute_input":"2023-08-10T08:57:39.792852Z","iopub.status.idle":"2023-08-10T08:57:39.840824Z","shell.execute_reply.started":"2023-08-10T08:57:39.792697Z","shell.execute_reply":"2023-08-10T08:57:39.839690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\n\n# Recall that we have our cleaned_train_df from 'pipeline' section\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\ntrain_sequence = tokenizer.texts_to_sequences(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:43:52.114454Z","iopub.execute_input":"2023-08-10T09:43:52.115140Z","iopub.status.idle":"2023-08-10T09:43:52.387029Z","shell.execute_reply.started":"2023-08-10T09:43:52.115104Z","shell.execute_reply":"2023-08-10T09:43:52.385328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_padded = pad_sequences(train_sequence,\n                             maxlen=50,\n                             truncating='post',\n                             padding='post')","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:46:00.204686Z","iopub.execute_input":"2023-08-10T09:46:00.205231Z","iopub.status.idle":"2023-08-10T09:46:00.252634Z","shell.execute_reply.started":"2023-08-10T09:46:00.205197Z","shell.execute_reply":"2023-08-10T09:46:00.251286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_padded # the solution of our problem, which is simply do a padding so that each sentence have the same length","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:46:04.353297Z","iopub.execute_input":"2023-08-10T09:46:04.353734Z","iopub.status.idle":"2023-08-10T09:46:04.362765Z","shell.execute_reply.started":"2023-08-10T09:46:04.353703Z","shell.execute_reply":"2023-08-10T09:46:04.361420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index\n\nlen(word_index) # dictionary mapping all the word <> vector","metadata":{"execution":{"iopub.status.busy":"2023-08-10T09:48:53.356599Z","iopub.execute_input":"2023-08-10T09:48:53.357566Z","iopub.status.idle":"2023-08-10T09:48:53.364667Z","shell.execute_reply.started":"2023-08-10T09:48:53.357518Z","shell.execute_reply":"2023-08-10T09:48:53.363647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we map the embedding index into a matrix\n\n# the structure of embedding matrix is like this\n# each row of the matrix correspond to the vector of the word in word_index\n# so basically, embedding_matrix[1] will be the GloVe word vector of the first word in word_index\n\nnum_word = len(word_index) + 1 # we +1 since the word_index dictionary start with 1 instead of 0\nembeddings_matrix = np.zeros((num_word,100))\n\nfor word, i in word_index.items():\n    emb_vec = embeddings_index.get(word, None) # if the word is OOV in GloVe, return None\n    if emb_vec is not None:\n        embeddings_matrix[i] = emb_vec","metadata":{"execution":{"iopub.status.busy":"2023-08-10T10:19:16.278268Z","iopub.execute_input":"2023-08-10T10:19:16.278730Z","iopub.status.idle":"2023-08-10T10:19:16.330499Z","shell.execute_reply.started":"2023-08-10T10:19:16.278701Z","shell.execute_reply":"2023-08-10T10:19:16.329069Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embeddings_matrix[1] correspond to the word 'not'\nembeddings_matrix[1] == embeddings_index['not'] # we verify that the embedding matrix is correctly set up","metadata":{"execution":{"iopub.status.busy":"2023-08-10T10:20:55.479732Z","iopub.execute_input":"2023-08-10T10:20:55.480436Z","iopub.status.idle":"2023-08-10T10:20:55.493866Z","shell.execute_reply.started":"2023-08-10T10:20:55.480385Z","shell.execute_reply":"2023-08-10T10:20:55.492015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_padded","metadata":{"execution":{"iopub.status.busy":"2023-08-10T10:32:17.532026Z","iopub.execute_input":"2023-08-10T10:32:17.532631Z","iopub.status.idle":"2023-08-10T10:32:17.542531Z","shell.execute_reply.started":"2023-08-10T10:32:17.532594Z","shell.execute_reply":"2023-08-10T10:32:17.540891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\nmodel=Sequential()\n\nembedding=Embedding(len(word_index)+1,\n                    100,\n                    embeddings_initializer=Constant(embeddings_matrix),\n                    input_length=50,\n                    trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T10:52:37.001104Z","iopub.execute_input":"2023-08-10T10:52:37.001546Z","iopub.status.idle":"2023-08-10T10:52:39.544895Z","shell.execute_reply.started":"2023-08-10T10:52:37.001512Z","shell.execute_reply":"2023-08-10T10:52:39.543229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T10:52:51.553069Z","iopub.execute_input":"2023-08-10T10:52:51.553471Z","iopub.status.idle":"2023-08-10T10:52:51.585033Z","shell.execute_reply.started":"2023-08-10T10:52:51.553440Z","shell.execute_reply":"2023-08-10T10:52:51.582762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <div style=\"text-align:center; font-size:40px;\">üîµ Embedding Method 4: BERT üîµ</div>\n\n(To run the code below, all you need to do is to run the two code block in Pipeline section above)","metadata":{}},{"cell_type":"code","source":"# split the data using sklearn train_test_split to evaluate our model\n# basically the idea behind train_test_split is to use a percentage of our training dataset\n# to train our model, then use the remaining data to evaluate the performance of the model\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cleaned_train_df.text,\n    cleaned_train_df.target,\n    stratify=cleaned_train_df.target, #stratify allow the split to be split in a way that balance the train and test set by the target column\n    test_size=0.2, # 20% of data will be used as test data\n    random_state=42) # setting random_state allow the split to be the same everytime you run this","metadata":{"execution":{"iopub.status.busy":"2023-08-10T12:37:41.937541Z","iopub.execute_input":"2023-08-10T12:37:41.938000Z","iopub.status.idle":"2023-08-10T12:37:42.038525Z","shell.execute_reply.started":"2023-08-10T12:37:41.937965Z","shell.execute_reply":"2023-08-10T12:37:42.036939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download the preprocessor and run it with tensorflow_hub\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\npreprocessor = hub.KerasLayer(\n    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\nencoder_inputs = preprocessor(X_train) # pass the text input into the preprocessor\n\n# Download the encoder and run it with tensorflow_hub\nencoder = hub.KerasLayer(\n    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\",\n    trainable=True)\noutputs = encoder(encoder_inputs) # pass the encoder inputs to the encoder","metadata":{"execution":{"iopub.status.busy":"2023-08-10T12:37:43.930390Z","iopub.execute_input":"2023-08-10T12:37:43.930933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be capitalizing the feature in BERT where it return us a sentence embedding","metadata":{}},{"cell_type":"code","source":"outputs['pooled_output']","metadata":{},"execution_count":null,"outputs":[]}]}